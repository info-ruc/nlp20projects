A fundamental step in the software design process is the selection of a refinement (implementation) for a data abstraction. This step traditionally involves investigating the expected performance of a system under different refinements of an abstraction and then selecting a single alternative which minimizes some performance cost metric. In this paper we reformulate this design step to allow different refinements of the same data abstraction within a computation. This reformulation reflects the fact that the implementation appropriate for a data abstraction is dependent on the behavior exhibited by the objects of the abstraction. Since this behavior can vary among the objects of a computation a single refinement is often inappropriate. Accordingly three frameworks are presented for understanding and representing variations in the behavior of objects and thus the potential for multiple implementations. The three frameworks are based upon 1) a static partitioning of objects into disjoint implementation classes 2) static partitioning of classes into implementation regions and 3) dynamic partitioning of classes into implementation regions. These frameworks and analytic tools useful in investigating expected performance under multiple implementations are described in detail.
From the Preface (See Front Matter for full Preface) Advances in the design and production of computer hardware have brought many more people into direct contact with computers. Similar advances in the design and production of computer software are required in order that this increased contact be as rewarding as possible. The Smalltalk80 system is a result of a decade of research into creating computer software that is appropriate for producing highly functional and interactive contact with personal computer systems. This book is the first detailed account of the Smalltalk80 system. It is divided into four major parts Part One  an overview of the concepts and syntax of the programming language. Part Two  an annotated and illustrated specification of the systems functionality. Part Three  an example of the design and implementation of a moderatesize application. Part Four  a specification of the Smalltalk80 virtual machine.
The trie data structure has many properties which make it especially attractive for representing large files of data. These properties include fast retrieval time quick unsuccessful search determination and finding the longest match to a given identifier. The main drawback is the space requirement. In this paper the concept of trie compaction is formalized. An exact algorithm for optimal trie compaction and three algorithms for approximate trie compaction are given and an analysis of the three algorithms is done. The analysis indicate that for actual tries reductions of around 70 percent in the space required by the uncompacted trie can be expected. The quality of the compaction is shown to be insensitive to the number of nodes while a more relevant parameter is the alphabet size of the key.
This work investigates one aspect of the performance of CODASYL database systems the data reference behavior. We introduce a model of database traversals at three levels the logical internal and physical levels. The mapping between the logical and internal levels is defined by the internal schema whereas the mapping between the internal and the physical levels depends on cluster properties of the database. Our model explains the physical reference behavior for a given sequence of DML statements at the logical level.Software has been implemented to monitor references in two selected CODASYL DBMS applications. In a series of experiments the physical reference behavior was observed for varying internal schemas and cluster properties of the database. The measurements were limited to retrieval transactions so that a variety of queries could be analyzed for the same wellknown state of the database. Also all databases were relatively small in order to allow fast reloading with varying internal schema parameters. In all cases the database transactions showed less locality of reference than do programs under virtual memory operating systems some databases showed no locality at all. No evidence of physical sequentiality was found. This suggests that standard page replacement strategies are not optimal for CODASYL database buffer management instead replacement decisions in a database buffer should be based on specific knowledge available from higher system layers.
This paper presents the design of a relational query processor. The query processor consists of only four processing PIPEs and a number of randomaccess memory modules. Each PIPE processes tuples of relations in a bitserial tupleparallel manner for each of the primitive database operations which comprise a complex relational query. The design of the query processor meets three major objectives the query processor must be manufacturable using existing and nearterm LSI (VLSI) technology it must support in a uniform manner both the numeric and nonnumeric processing requirements a highlevel user interface like SQL presents and it must support the queryprocessing strategy derived in the query optimizer to satisfy certain systemwide performance optimality criteria.
The problem of statistical database confidentiality in releasing microdata is addressed through the use of approximate dataswapping. Here a portion of the microdata is replaced with a database that has been selected with approximately the same statistics. The result guarantees the confidentiality of the original data while providing microdata with accurate statistics. Methods for achieving such transformations are considered and analyzed through simulation.
Several researchers (Beeri Bernstein Chiu Fagin Goodman Maier Mendelzon Ullman and Yannakakis) have introduced a special class of database schemes called acyclic or tree schemes. Beeri et al. have shown that an acyclic join dependency naturally defined by an acyclic database scheme has several desirable properties and that an acyclic join dependency is equivalent to a conflictfree set of multivalued dependencies. However since their results are confined to multivalued and join dependencies it is not clear whether we can handle functional dependencies independently of other dependencies.In the present paper we define an extension of a conflictfree set called an extended conflictfree set including multivalued dependencies and functional dependencies and show the following two properties of an extended conflictfree setThere are three equivalent definitions of an extended conflictfree set. One of them is defined as a set including an acyclic joint dependency and a set of functional dependencies such that the left and right sides of each functional dependency are included in one of the attribute sets that construct the acyclic join dependency.For a relation scheme with an extended conflictfree set there is a decomposition into third normal form with a lossless join and preservation of dependencies.
A very simple and efficient technique for the introduction of separate compilation facilities into compilers for blockstructured languages is presented. Using this technique programs may be compiled in parts while the compiletime checking advantages of compilation as a whole are retained. These features are simple for a programmer to understand and are easy to implement. Experience has shown this separate compilation mechanism to be a useful tool in the development of large programs in blockstructured languages.
One of the requirements for building an operating system in a highlevel operating system language such as Ada Concurrent Pascal or Modula is the construction of a language support system or kernel. This paper presents a model that generalizes the concept of a kernel and defines a kernel and the processes it supports to be at different levels of abstraction. A highlevel language mechanism the Execute statement is then proposed as the basis of the interface between a kernel and the processes it supports. Software capabilities control access between levels and the Execute statement controls processor context switching between levels. The mechanisms rely on data typing for reliability and protection. They encourage systems that are well protected and exhibit an explicit hierarchical structure. Software capabilities and the Execute statement are illustrated with a pilot implementation on the Prime 650. An experimental operating system that encompasses their use is discussed. Extensions are presented which manage interrupts timeslicing and preemption and hardware protection mechanisms.
A new and flexible solution to the problem of multiple users accessing a single resource such as communication bandwidth or composite object in memory is derived. The means of communication consists of sending and receiving messages in known locations (or equivalently mailboxes without queueing). Any particular user is able to deposit and hence destroy previous messages in a mailbox. It is assumed that exclusive access to a mailbox is supplied by an underlying system. The major results of this paper are 1) a simple treebased algorithm that guarantees 驴 no user or group of users can conspire to prevent access by some other user to the resource 驴 only one user accesses the resource at a time 驴 if there are N users an individual user is guaranteed access when requested to the resource in no more than N1 turns Knuths solution 6 can delay a user up to 2 (N1)1 turns 2) an extension of Dekkers algorithm (2 users) 2 that allows the relative rates of reservations for access to the resource to be proportional to a set of N integers. When a reservation is not being used by its owner it will be assigned to another contending request. The assignment is optimal for periodic requests.
The control of concurrent access to shared resources is an important feature of both centralized and distributed operating systems. In conventional systems exclusive access is the rule while concurrent access is the exception. Dataflow computer systems along with an applicative style of programming provide an execution environment in which this philosophy is reversed. In these latter systems it is necessary to reexamine the manner in which synchronization of access to shared resources is specified and implemented. A basic design for a dataflow resource manager is reviewed illustrating the clear separation between access mechanism and scheduling policy. The semantics of the access mechanism is based solely on the principle of data dependency. Specifications are presented for a general scheduler to further constrain or order accesses to the resource. Using open path expressions as a very highlevel specification language for synchronization it is shown how to automatically synthesize a scheduler as a distributed network of communicating modules.
A control abstraction called atomic action is a powerful general mechanism for ensuring consistent behavior of a system in spite of failures of individual computations running in the system and in spite of system crashes. However because of the allornothing property of atomic actions an important amount of work might be abandoned needlessly when an internal error is encountered. This paper discusses how implementation of resilient distributed systems can be supported using a combination of nested atomic actions and stable checkpoints. Nested atomic actions form a tree structure. When an internal atomic action terminates its results are not made permanent until the outermost atomic action commits but they survive local node failures. Each subtree of atomic actions is recoverable individually. A checkpoint is established in stable storage as part of a remote request so that results of such a request can be reclaimed if the requesting node fails in the meantime The paper shows how remote procedure call primitives with atmostonce semantics and recovery blocks can be built with these mechanisms.
A way of programming realtime systems is described which inverts the usual image of parallel processes instead of processes which are ordinarily running and which wait occasionally in order to synchronize with other cooperating processes intervention schedules are ordinarily waiting and run nonpreemptibly triggered by events which may be external (modeling hardware interrupts) or generated by other intervention schedules. In order for nonpreemptive scheduling to make sense the maximum period of time for which any event in an intervention schedule runs must be carefully controlled. This and other aspects of the model are considered and it is compared with more traditional models of parallel processes and with message passing models. Programming language features to support this programmming model are discussed. Strengths and limitations of the model are discussed.
The problem of analyzing an initialized loop and verifying that the program computes some particular function of its inputs is addressed. A heuristic technique for solving these problems is proposed that appears to work well in many commonly occurring cases. The use of the technique is illustrated with a number of applications. An attribute of initialized loops is identified that corresponds to the effort required to apply this method in a deterministic (i.e. guaranteed to succeed) manner. It is explained that in any case the success of the proposed heuristic relies on the loop exhibiting a reasonable form of behavior.
This paper discusses the current style of algebraic data type specifications. Some simple examples illustrate that whether or not two objects of the type being specified are equal can be implementation dependent even for very simple objects of the type. To remedy this it is proposed that specifications should be safe where safety is a stronger requirement than Guttags sufficient completeness. The paper also discusses when an operator should be part of a specification and when it should be introduced by extension and concludes with safe specifications of some common data types.
In this experiment seven software teams developed versions of the same smallsize (20004000 source instruction) application software product. Four teams used the Specifying approach. Three teams used the Prototyping approach. The main results of the experiment were the following. 1) Prototyping yielded products with roughly equivalent performance but with about 40 percent less code and 45 percent less effort. 2) The prototyped products rated somewhat lower on functionality and robustness but higher on ease of use and ease of learning. 3) Specifying produced more coherent designs and software that was easier to integrate. The paper presents the experimental data supporting these and a number of additional conclusions.
Programming languages have traditionally had more data types than database systems. The flexibility of abstract types could make a database system more useful in supporting application development. Abstract types allow users to think about and manipulate data in terms and structures that they are familiar with. This paper proposes that databases have a type system interface and describes a representation of a type system in terms of relations. The type system model supports a variety of programming language constructs such as userdefined parameterized data types and userdefined generic operations. The efficiency of the type system is compared to the access time of the database system.
Database management systems (DBMSs) today are usually built as subsystems on top of an operating system (OS). This design approach can lead to problems of unreliability and inefficient performance as well as forcing a duplication of functions between the DBMS and OS. A new design approach is proposed which eliminates much of this duplication by integrating the duplicated functions into independent subsystems used by both the DBMS and OS. Specifically an IO and file support subsystem and a security subsystem are defined. Both subsystems make use of a logical information model which models the stored information in secondary storage. The new database operating system organization and the logical information model are presented in detail. Design of the security subsystem is based on the access control model and is extended with Boolean predicates to produce an access control model capable of enforcing contentdependent security policies. The access matrix is implemented using a combination of access lists and capabilities. Authorization models and multiple user processes are discussed in relation to the new system organization. The outline of a formal specification and proof of correctness of the security subsystem is also discussed.
A simple and general definition of denialofservice in operating systems is presented. It is argued that no current protection mechanism nor model resolves this problem in any demonstrable way. The notion of interuser dependency is introduced and identified as the common cause for all problem instances. Decomposition of operating systems into hierarchies of services is assumed for the discovery of denialofservice instances.
An intuitive presentation of the trace method for the abstractspecification of software contains sample specifications syntacticand semantic definitions of consistency and totalness methods forproving specifications consistent and total and a comparison ofthe method with the algebraic approach to specification. Thisintuitive presentation is underpinned by a formal syntaxsemantics and derivation system for the method. Completeness andsoundness theorems establish the correctness of the derivationsystem with respect to the semantics the coextensiveness of thesyntactic definitions of consistency and totalness with theirsemantic counterparts and the correctness of the proof methodspresented. Areas for future research are discussed.
The informationbased study of the optimal solution of largelinear systems is initiated by studying the case of Krylovinformation. Among the algorithms that use Krylov information areminimal residual conjugate gradient Chebyshev and successiveapproximation algorithms. A sharp lower bound on the number ofmatrixvector multiplications required to compute anapproximation is obtained for any orthogonally invariantclass of matrices. Examples of such classes include many ofpractical interest such as symmetric matrices symmetric positivedefinite matrices and matrices with bounded condition number. Itis shown that the minimal residual algorithm is within at most onematrixvector multiplication of the lower bound. A similar resultis obtained for the generalized minimal residual algorithm. Thelower bound is computed for certain classes of orthogonallyinvariant matrices. How the lack of certam properties (symmetrypositive definiteness) increases the lower bound is shown. Aconjecture and a number of open problems are stated.
A mathematical theory for the study of data representation indatabases is introduced and developed. The theory focuses on threedata constructs (collection composition and classification).Formats with semantically rich yet tractable structure are builtrecursively using these constructs. Using formats we obtainseveral nontrivial results concerning notions of relativeinformation capacity and restructuring of data sets. As such theformat model provides a new approach for the formal study of theconstruction of user views and other data manipulations indatabases.
A mathematical model for communicating sequential processes isgiven and a number of its interesting and useful properties arestated and proved. The possibilities of nondetermimsm are fullytaken into account.
As users entrust more and more of their applications to computersystems the need for systems that are continuously operational (24hours per day) has become even greater. This paper presents asurvey and analysis of representative architectures and techniquesthat have been developed for constructing highly available systemsfor database applications. It then proposes a design of adistributed software subsystem that can serve as a unifiedframework for constructing database application systems that meetvarious requirements for high availability.
During the past ten years the field of multipleaccesscommunication has developed into a major area of both practical andtheoretical interest within the field of computer communications.The multipleaccess problem arises from the necessity of sharing asingle communication channel among a community of distributedusers. The distributed algorithm used by the stations to share thechannel is known as the multipleaccess protocol. In this paper weexamine the multipleaccess problem and various approaches to itsresolution.In this survey we first define the multipleaccess problem andthen present the underlying issues and difficulties in achievingmultipleaccess communication. A taxonomy for multipleaccessprotocols is then developed in order to characterize commonapproaches and to provide a framework within which these protocolscan be compared and contrasted. Different proposed protocols arethen described and discussed and aspects of their performance areexamined. The use of multipleaccess protocols for real time ortimeconstrained communication applications such as voicetransmission is examined next. Issues in timeconstrainedcommunication are identified and recent work in the design oftimeconstrained multipleaccess protocols is surveyed.
The rapidly evolving field of local network technology has produced a steady stream of local network products in recent years.The IEEE 802 standards that are now taking shape because of their complexity do little to narrow the range of alternative technicalapproaches and at the same time encourage more vendors into thefield. The purpose of this paper is to present a systematicorganized overview of the alternative architectures for and designapproaches to local networks.The key elements that determine the cost and performance of alocal network are its topology transmission medium and mediumaccess control protocol. Transmission media include twisted pairbaseband and broadband coaxial cable and optical fiber. Topologiesinclude bus tree and ring. Medium access control protocolsinclude CSMACD token bus token ring register insertion andslotted ring. Each of these areas is examined in detailcomparisons are drawn between competing technologies and thecurrent status of standards is reported.
Several new methods are presented for selecting n records at random without replacement from a file containing N records. Each algorithm selects the records for the sample in a sequential mannerin the same order the records appear in the file. The algorithms are online in that the records for the sample are selected iteratively with no preprocessing. The algorithms require a constant amount of space and are short and easy to implement. The main result of this paper is the design and analysis of Algorithm D which does the sampling in O(n) time on the average roughly n uniform random variates are generated and approximately n exponentiation operations (of the form ab for real numbers a and b) are performed during the sampling. This solves an open problem in the literature. CPU timings on a large mainframe computer indicate that Algorithm D is significantly faster than the sampling algorithms in use today.
SystemU is a universal relation database system under development at Standford University which uses the language C on UNIX. The system is intended to test the use of the universal view in which the entire database is seen as one relation. This paper describes the theory behind SystemU in particular the theory of maximal objects and the connection between a set of attributes. We also describe the implementation of the DDL (Data Description Language) and the DML (Data Manipulation Language) and discuss in detail how the DDL finds maximal objects and how the DML determines the connection between the attributes that appear in a query.
The query inference problem is to translate a sentence of a query language into an unambiguous representation of a query. A query is represented as an expression over a set of query trees. A metric is introduced for measuring the complexity of a query and also a proposal that a sentence be translated into the least complex query which satisfies the sentence. This method of query inference can be used to resolve ambiguous sentences and leads to easier formulation of sentences.
A modification of linear hashing is proposed for which the conventional use of overflow records is avoided. Furthermore an implementation of linear hashing is presented for which the amount of physical storage claimed is only fractionally more than the minimum required. This implementation uses a fixed amount of incore space. Simulation results are given which indicate that even for storage utilizations approaching 95 percent the average successful search cost for this method is close to one disk access.
In recent years the information processing requirements of business organizations have expanded tremendously. With this expansion the design of databases to efficiently manage and protect business information has become critical. We analyze the impacts of record segmentation (the assignment of data items to segments defining subfiles) an efficiencyoriented design technique and of backup and recovery strategies a data protection technique on the overall process of database design. A combined record segmentationbackup and recovery procedure is presented and an application of the procedure is discussed. Results in which problem characteristics are varied along three dimensions update frequencies available types of access paths and the predominant type of data retrieval that must be supported by the database are presented.
Using the nested loops method this paper addresses the problem of minimizing the number of page fetches necessary to evaluate a given query to a relational database. We first propose a data structure whereby the number of page fetches required for query evaluation is substantially reduced and then derive a formula for the expected number of page fetches. An optimal solution to our problem is the nesting order of relations in the evaluation program which minimizes the number of page fetches. Since the minimization of the formula is NPhard as shown in the Appendix we propose a heuristic algorithm which produces a good suboptimal solution in polynomial time. For the special case where the input query is a tree query we present an efficient algorithm for finding an optimal nesting order.
A protocol for transaction processing during partition failures is presented which guarantees mutual consistency between copies of dataitems after repair is completed. The protocol is optimistic in that transactions are processed without restrictions during failure conflicts are then detected at repair time using a precedence graph and are resolved by backing out transactions according to some backout strategy. The resulting database state then corresponds to a serial execution of some subset of transactions run during the failure. Results from simulation and probabilistic modeling show that the optimistic protocol is a reasonable alternative in many cases. Conditions under which the protocol performs well are noted and suggestions are made as to how performance can be improved. In particular a backout strategy is presented which takes into account individual transaction costs and attempts to minimize total backout cost. Although the problem of choosing transactions to minimize total backout cost is in general NPcomplete the backout strategy is efficient and produces very good results.
In my view the most interesting aspect of thisexperiment with prototypes is its implications for software engineeringeveryone talks about prototypes today but is anyone really producingany Or amongst those who do produce prototypes how many can resistthe temptation once the prototype is complete to treat it as thefinished product The experiences described here present a clear cutcase it would have been impossible to write the compiler immediately inits final formthe cost of perpetual modifications in response torevisions to ADA of a compiler written in a traditional languagewould have been intolerably highdesigning a prototype seems tohave been the only way out.Finally once a decision in favor of prototypes has been takeneither as a way of giving concrete expression to a set of specificationsor a way of protecting a particular design a choice of language inwhich to write the prototype has to be made. The advantages of SETL areclearly demonstrated in this article with particular stress laid on thefacilities it provides for using several semantic levels and thereforerefining a prototype in successive stages until the final productemerges or a program very close to the final product. Translation intoa language providing for a higher performance level or at any rate intoa more common language C in this case is then a simple task. My guessis that a lot of TSI readers are going to want more information on SETLto enable them to use it in similar circumstances in the future.To sum up this article is particularly interesting for threereasons it provides a closer understanding of ADA demonstrates thevalue of prototypes in software design and makes a plea in favor ofSETL which does indeed appear wholly satisfactory as a language in whichto write such prototypes.From the Commentary by E. GirardCommentary recommended by E. A. Feustel Sherborn MA
Performance in database systems is strongly influenced by buffer management and transaction recovery methods. This paper presents the principles of the database cache which replaces the traditional buffer. In comparison to buffer management cache management is more carefully coordinated with transaction management and integrates transaction recovery. High throughput ofsmall and mediumsized transactions is achieved by fast commit processing and low database traffic. Very fast handling of transaction failures and short restart time after system failure are guaranteed in such an environment. Very long retrieval and update transactions are also supported.
Various logging and recovery techniques for centralized transactionoriented database systems under performance aspects are described and discussed. The classification of functional principles that has been developed in a companion paper is used as a terminological basis. In the main sections a set of analytic models is introduced and evaluated in order to compare the performance characteristics of nine different recovery techniques with respect to four key parameters and a set of other parameters with less influence. Finally the results of model evaluation as well as the limitations of the models themselves are discussed.
First Page of the Article
First Page of the Article
First Page of the Article
First Page of the Article
First Page of the Article
First Page of the Article
First Page of the Article
First Page of the Article
First Page of the Article
First Page of the Article
First Page of the Article
First Page of the Article
First Page of the Article
First Page of the Article
First Page of the Article
First Page of the Article
First Page of the Article
First Page of the Article
First Page of the Article
This paper discusses the implementation of a database buffer manager as a component of a DBMS. The interface between calling components of higher system layers and the buffer manager is described the principal differences between virtual memory paging and database buffer management are outlined the notion of referencing versus addressing of database pages is introduced and the concept of fixing pages in the buffer to prevent uncontrolled replacement is explained.Three basic tasks have to be performed by the buffer manager buffer search allocation of frames to concurrent transactions and page replacement. For each of these tasks implementation alternatives are discussed and illustrated by examples from a performance evaluation project of a CODASYL DBMS.
An analysis of the distributions and relationships derived from the change data collected during development of a mediumscale software project produces some surprising insights into the factors influencing software development. Among these are the tradeoffs between modifying an existing module as opposed to creating a new one and the relationship between module size and error proneness.
Practical suggestions are presented for effectively managing software development in smallproject environments (i.e. no more than several million dollars per year). The suggestions are based on an approach to product development using a product assurance group that is independent from the development group. Within this checkandbalance managementdevelopmentproduct assurance structure a design review process is described that effects an orderly transition from customer needs statement to software code. The testing activity that follows this process is then explained. Finally the activities of a change control body (called a configuration control board) and supporting functions geared to maintaining delivered software are described. The suggested software management practices result from the experience of a small (approximately 100 employees) software engineering company that develops and maintains computer systems supporting realtime interactive commercial industrial and military applications.
This paper presents a methodology for evaluating text editors on several dimensions the time it takes experts to perform basic editing tasks the time experts spend making and correcting errors the rate at which novices learn to perform basic editing tasks and the functionality of editors over more complex tasks. Time errors and learning are measured experimentally functionality is measured analytically time is also calculated analytically. The methodology has thus far been used to evaluate nine diverse text editors producing an initial database of performance results. The database is used to tell us not only about the editors but also about the usersthe magnitude of individual differences and the factors affecting novice learning.
The coalesced hashing method has been shown to be very fast for dynamic information storage and retrieval. This paper analyzes in a uniform way the performance of coalesced hashing and its variants thus settling some open questions in the literature.In all the variants the range of the hash function is called the address region and extra space reserved for storing colliders is called the cellar. We refer to the unmodified method which was analyzed previously as lateinsertion coalesced hashing. In this paper we analyze late insertion and two new variations called early insertion and varied insertion. When there is no cellar the earlyinsertion method is better than late insertion however past experience has indicated that it might be worse when there is a cellar. Our analysis confirms that it is worse. The variedinsertion method was introduced as a means of combining the advantages of late insertion and early insertion. This paper shows that varied insertion requires fewer probes per search on the average than do the other variants.Each of these three coalesced hashing methods has a parameter that relates the sizes of the address region and the cellar. Techniques in this paper are designed for tuning the parameter in order to achieve optimum search times. We conclude with a list of open problems.
In a onecopy distributed database each data item is stored at exactly one site. In a replicated database some data items may be stored at multiple sites. The main motivation is improved reliability by storing important data at multiple sites the DBS can operate even though some sites have failed.This paper describes an algorithm for handling replicated data which allows users to operate on data so long as one copy is available. A copy is available when (i) its site is up and (ii) the copy is not outofdate because of an earlier crash.The algorithm handles clean detectable site failures but not Byzantine failures or network partitions.
This paper studies the problem of storing singlelevel and multilevel clustered files. Necessary and sufficient conditions for a singlelevel clustered file to have the consecutive retrieval property (CRP) are developed. A linear time algorithm to test the CRP for a given clustered file and to identify the proper arrangement of objects if CRP exists is presented. For the singlelevel clustered files that do not have CRP it is shown that the problem of identifying a storage organization with minimum redundancy is NPcomplete.Consequently an efficient heuristic algorithm to generate a good storage organization for such files is developed. Furthermore it is shown that for certain types of multilevel clustered files there exists a storage organization such that the objects in each cluster for all clusters in each level of the clustering appear in consecutive locations.
This paper addresses the vertical partitioning of a set of logical records or a relation into fragments. The rationale behind vertical partitioning is to produce fragments groups of attribute columns that closely match the requirements of transactions.Vertical partitioning is applied in three contexts a database stored on devices of a single type a database stored in different memory levels and a distributed database. In a twolevel memory hierarchy most transactions should be processed using the fragments in primary memory. In distributed databases fragment allocation should maximize the amount of local transaction processing.Fragments may be nonoverlapping or overlapping. A twophase approach for the determination of fragments is proposed in the first phase the design is driven by empirical objective functions which do not require specific cost information. The second phase performs cost optimization by incorporating the knowledge of a specific application environment. The algorithms presented in this paper have been implemented and examples of their actual use are shown.
First Page of the Article
First Page of the Article
First Page of the Article
Several fairly large sets of programming rules have been developed recently. It is natural to ask whether the process of developing such rule bases may converge. Having developed sets of rules for specific programming tasks and domains will they be helpful when other tasks and domains are considered While it is too early to give definitive answers experience with the rules of the PECOS system has been positive. Both during the process of developing the rule set and while developing rules for another domain the existence of already codified rules proved very helpful.
A specification technique formally equivalent to finitestate machines is offered as an alternative because it is inherently distributed and more comprehensible. When applied to modules whose complexity is dominated by control the technique guides the analyst to an effective decomposition of complexity encourages wellstructured error handling and offers an opportunity for parallel computation. When applied to distributed protocols the technique provides a unique perspective and facilitates automatic detection of some classes of error. These applications are illustrated by a controller for a distributed telephone system and the fullduplex alternatingbit protocol for data communication. Several schemes are presented for executing the resulting specifications.
Until recently informationflow analysis has been used primarily to verify that information transmission between program variables cannot violate security requirements. Here the notion of information flow is explored as an aid to program development and validation.Informationflow relations are presented for whileprograms which identify those program statements whose execution may cause information to be transmitted from or to particular input internal or output values. It is shown with examples how these flow relations can be helpful in writing testing and updating programs they also usefully extend the class of errors which can be detected automatically in the static analysis of a program.
An efficient algorithm for communicating lettershape information from a highspeed computer with a large memory to a typesetting device that has a limited memory is presented. The encoding is optimum in the sense that the total time for typesetting is minimized using a model that generalizes wellknown demand paging strategies to the case where changes to the cache are allowed before the associated information is actually needed. Extensive empirical data show that good results are obtained even when difficult technical material is being typeset on a machine that can store information concerning only 100 characters. The methods of this paper are also applicable to other hardware and software caching applications with restricted lookahead.
Generative communication is the basis of a new distributed programming langauge that is intended for systems programming in distributed settings generally and on integrated network computers in particular. It differs from previous interprocess communication models in specifying that messages be added in tuplestructured form to the computation environment where they exist as named independent entities until some process chooses to receive them. Generative communication results in a number of distinguishing properties in the new language Linda that is built around it. Linda is fully distributed in space and distributed in time it allows distributed sharing continuation passing and structured naming. We discuss these properties and their implications then give a series of examples. Linda presents novel implementation problems that we discuss in Part II. We are particularly concerned with implementation of the dynamic global name space that the generative communication model requires.
One aspect of network design is the extent to which memory is shared among the processing elements. In this paper a model with limited sharing (only two processors connected to each memory) is analyzed and its performance compared with the performance of two other models that have appeared in the literature. One of these is a model of multiple processors sharing a single memory the other model considers a multiprocessor configuration in which each processor has its own dedicated memory. The tasks processed by these networks are described by both time and memory requirements. The largestmemoryfirst (LMF) scheduling algorithm is employed and its performance with respect to an enumerative optimal scheduling algorithm is bounded. On the basis of this measure we conclude that memory sharing is only desirable on very small networks and is disadvantageous on networks of larger size.
Algorithms are described for maintaining clock synchrony in a distributed multiprocess system where each process has its own clock. These algorithms work in the presence of arbitrary clock or process failures including twofaced clocks that present different values to different processes. Two of the algorithms require that fewer than onethird of the processes be faulty. A third algorithm works if fewer than half the processes are faulty but requires digital signatures.
Three different approaches to heuristic search in networks are analyzed. In the first approach as formulated initially by Hart Nilsson and Raphael and later modified by Martelli the basic idea is to choose for expansion that node for which the evaluation function has a minimum value. A second approach has recently been suggested by Nilsson. In this method in contrast to the earlier one a node that is expanded once is not expanded again instead a propagation of values takes place. The third approach is an adaptation for networks of an ANDOR graph marking algorithm originally due to Martelli and Montanari.Five algorithms are presented. Algorithms A and C illustrate the first approach PropA and PropC the second one and MarkA the third one. The performances of these algorithms are compared for both admissible and inadmissible heuristics using the following two criteria (i) cost of the solution found (ii) time of execution in the worst case as measured by the number of node expansions (A C) or node selections (PropA PropC) or arc markings (MarkA).The relative merits and demerits of the algorithms are summarized and indications are given regarding which algorithm to use in different situations.
Two new marking algorithms for ANDOR graphs called CF and CS are presented. For admissible heuristics CS is not needed and CF is shown to be preferable to the marking algorithms of Martelli and Montanari. When the heuristic is not admissible the analysis is carried out with the help of the notion of the first and second discriminants of an ANDOR graph. It is proved that in this case CF can be followed by CS to get optimal solutions provided the sumcost criterion is used and the first discriminant equals the second. Estimates of time and storage requirements are given. Other cost measures such as maxcost are also considered and a number of interesting open problems are enumerated.
Since a nondeterministic and concurrent program may in general communicate repeatedly with its environment its meaning cannot be presented naturally as an inputoutput function (as is often done in the denotational approach to semantics). In this paper an alternative is put forth. First a definition is given of what it is for two programs or program parts to be equivalent for all observers then two program parts are said to be observation congruent if they are in all program contexts equivalent. The behavior of a program part that is its meaning is defined to be its observation congruence class.The paper demonstrates for a sequence of simple languages expressing finite (terminating) behaviors that in each case observation congruence can be axiomatized algebraically. Moreover with the addition of recursion and another simple extension the algebraic language described here becomes a calculus for writing and specifying concurrent programs and for proving their properties.
The subset sum problem is to decide whether or not the 0l integer programming problem Sgrnil aixi  M I xI  0 or 1 has a solution where the ai and M are given positive integers. This problem is NPcomplete and the difficulty of solving it is the basis of publickey cryptosystems of knapsack type. An algorithm is proposed that searches for a solution when given an instance of the subset sum problem. This algorithm always halts in polynomial time but does not always find a solution when one exists. It converts the problem to one of finding a particular short vector v in a lattice and then uses a lattice basis reduction algorithm due to A. K. Lenstra H. W. Lenstra Jr. and L. Lovasz to attempt to find v. The performance of the proposed algorithm is analyzed. Let the density d of a subset sum problem be defined by d  nlog2(maxi ai). Then for almost all problems of density d d n it is proved that the lattice basis reduction algorithm locates v. Extensive computational tests of the algorithm suggest that it works for densities d dc(n) where dc(n) is a cutoff value that is substantially larger than 1n. This method gives a polynomial time attack on knapsack publickey cryptosystems that can be expected to break them if they transmit information at rates below dc(n) as n  .
An algebraic foundation of database schema design is presented. A new database operator namely disaggregation is introduced. Beginning with free families repeated applications of disaggregation and three other operators (matching function Cartesian product and selection) yield families of increasingly elaborate structure. In particular families defined by one join dependency and several embedded functional dependencies can be obtained in this manner.
Sixtyfour small computers are connected by a network of pointtopoint communication channels in the plan of a binary 6cube. This Cosmic Cube computer is a hardware simulation of a future VLSI implementation that will consist of singlechip nodes. The machine offers high degrees of concurrency in applications and suggests that future machines with thousands of nodes are both feasible and attractive.
The Manchester project has developed a powerful dataflow processor based on dynamic tagging. This processor is large enough to tackle realistic applications and exhibits impressive speedup for programs with sufficient parallelism.
A large quantity of wellrespected software is tested against a series of metrics designed to measure program lucidity with intriguing results. Although slanted toward software written in the C language the measures are adaptable for analyzing most highlevel languages.
Microcomputers when properly programmed have sufficient memory and speed to successfully perform serious calculations of modest sizelinear equations least squares matrix inverse or generalized inverse and the symmetric matrix eigenproblem.
First Page of the Article
First Page of the Article
First Page of the Article
First Page of the Article
A popular means of increasing the effective rate of main storageaccesses in a large computer is a multiplicity of memory modulesaccessible in parallel. Although such an organization usuallyachieves a net gain in access rate it also creates new modes ofcongestion at the storage controller. This paper analyzes thevariables that describe such a congestion queue lengths anddelays. A controller that maintains separate register sets toaccommodate the request queue of each module is considered. Thevarious processors attached to the storage are assumed to generatein each memory cycle a number of access requests with the samegiven distribution. The addresses specified by these requests(reduced to the module index) are further assumed to follow thestates of a firstorder Markov chain. The analysis then becomes oneof a singleserver queuing system with constant service time andindexed batch arrival process. Results are derived for severaldescriptors of the congestion and thus of the quality of serviceoffered by such an organization. The aim throughout is to embodythe results in a form readily suitable for numericalevaluation.
The traditional LALR analysis is reexamined using a new operator and an associated graph. An improved method that allows factoring out a crucial part of the computation for defining states of LR(0) canonical collection and for computing LALR(1) lookahead sets is presented. This factorization leads to significantly improved algorithms with respect to execution time as well as storage requirements. Experimental results including comparison with other known methods are presented.
The significant intellectual cost of programming is for problem solving and explaining not for coding. Yet programming systems offer mechanical assistance for the coding process exclusively. We illustrate the use of an implemented program development system called PRL (pearl) that provides automated assistance with the difficult part. The problem and its explained solution are seen as formal objects in a constructive logic of the data domains. These formal explanations can be executed at various stages of completion. The most incomplete explanations resemble applicative programs the most complete are formal proofs.
The problem of generalizing functional specifications for while loops is considered. This problem occurs frequently when trying to verify that an initialized loop satisfies some functional specification i.e. produces outputs which are some function of the program inputs.The notion of a valid generalization of a loop specification is defined. A particularly simple valid generalization a base generalization is discussed. A property of many commonly occurring while loops that of being uniformly implemented is defined. A technique is presented which exploits this property in order to systematically achieve a valid generalization of the loop specification. Two classes of uniformly implemented loops that are particularly susceptible to this form of analysis are defined and discussed. The use of the proposed technique is illustrated with a number of applications. Finally an implication of the concept of uniform loop implementation for the validation of the obtained generalization is explained.
The most effective use of meshconnected array processors is achieved by paying careful attention to the organization of data. Storage and movement of the elements of multidimensional arrays of data within a multidimensional store for a particular problem is a difficult task often handled in an ad hoc way. For an important class of problems considerable conceptual simplification arises from considering a specification of the mapping rather than the physical location of data.
Does Groschs law which postulated that the costs of computer systems increase at a rate equivalent to the square root of their power still hold The age of mini micro and supercomputers seems to have complicated the situation. When computers are grouped according to their size and power Groschs law seems to hold within each group but not between different groups.
The readability of the ten major computing periodicals is analyzed using the Flesch Reading Ease Index.
From NELIAC (via ALGOL 60) to Euler and ALGOL W to Pascal and Modula2 and ultimately Lilith Wirths search for an appropriate formalism for systems programming yields intriguing insights and surprising results.
Both the composition of the selection team and the choice of evaluation criteria should reflect the enduser orientation of DSS software.
Psychometric scaling methods are applied to programmer productivity assessments of 20 tools to recommend a set of minimal as well as more comprehensive tools.
Effective development environments for discrete event simulation models should reduce development costs and improve model performance. A model specification language used in a model development environment is defined. This approach is intended to reduce modeling costs by interposing an intermediate form between a conceptual model (the model as it exists in the mind of the modeler) and an executable representation of that model. As a model specification is constructed the incomplete specification can be analyzed to detect some types of errors and to provide some types of model documentation. The primitives used in this specification language called a condition specification (CS) are carefully defined. A specification for the classical patrolling repairman model is used to illustrate this language. Some possible diagnostics and some untestable model specification properties based on such a representation are summarized.
In this article we study the amortized efficiency of the movetofront and similar rules for dynamically maintaining a linear list. Under the assumption that accessing the ith element from the front of the list takes thgr(i) time we show that movetofront is within a constant factor of optimum among a wide class of list maintenance rules. Other natural heuristics such as the transpose and frequency count rules do not share this property. We generalize our results to show that movetofront is within a constant factor of optimum as long as the access cost is a convex function. We also study paging a setting in which the access cost is not convex. The paging rule corresponding to movetofront is the least recently used (LRU) replacement rule. We analyze the amortized complexity of LRU showing that its efficiency differs from that of the offline paging rule (Beladys MIN algorithm) by a factor that depends on the size of fast memory. No online paging algorithm has better amortized performance.
A simple and efficient method for processing general equijoin queries in a distributed relational database is described. The query is first decomposed into a set of simple queries each being involved with only one of the joining domains and its relevant equijoins. An extended version of Hevner and Yaos STRATEGY PARALLEL or STRATEGY SERIAL is then applied on each of them for generating transmission schedules. These schedules will fully reduce (with respect to a simple query) some specified relations. The latter are then transmitted to the result site for final processing. In the case of minimizing total time our method has a lower order of complexity than ALGORITHM GENERAL studied by Hevner and Apers. Examples show that our method gives better and more efficient solutions than theirs.
the design of ADA is based on an implicitassumption of a hardware model with a single shared memory. This paperidentifies the ADA facilities influenced by this invalid assumption. Itproposes ways to overcome these weaknesses ...
the design of ADA is based on an implicitassumption of a hardware model with a single shared memory. This paperidentifies the ADA facilities influenced by this invalid assumption. Itproposes ways to overcome these weaknesses while restricting the powerof the language as little as possible.From the Authors Abstract
The problem of synthesizing a procedure from example computations is examined. An algorithm for this task is presented and its success is considered. To do this a model of procedures and example computations is introduced and the class of acceptable examples is defined. The synthesis algorithm is shown to be successful with respect to the model of procedures and examples from two perspectives. First it is shown to be sound that is that the procedure synthesized from a set of examples produces the same result as the intended one on the inputs used to generate that set of examples. Second it is shown to be complete that is that for any procedure in the class of procedures there exists a finite set of examples such that the procedure synthesized behaves as the intended one on all inputs for which the intended one halts.
Many database systems maintain the consistency of the data by using a locking protocol to restrict access to data items. It has been previously shown that if no information is known about the method of accessing items in the database then the twophase protocol is optimal. However the use of structural information about the database allows development of nontwophase protocols called graph protocols that can potentially increase efficiency. Yannakakis developed a general class of protocols that included many of the graph protocols. Graph protocols either are only usable in certain types of databases or can incur the performance liability of cascading rollback. In this paper it is demonstrated that if the system has a priori information as to which data items will be locked first by various transactions a new graph protocol that is outside the previous classes of graph protocols and is applicable to arbitrarily structured databases can be constructed. This new protocol avoids cascading rollback and its accompanying performance degradation and extends the class of serializable sequences allowed by nontwophase protocols. This is the first protocol shown to be always as effective as the twophase protocol and it can be more effective for certain types of database systems.
Parallel algorithms for data compression by textual substitution that are suitable for VLSI implementation are studied. Both static and dynamic dictionary schemes are considered.
This paper analyzes decomposition properties of a graph that when they occur permit a polynomial solution of the traveling salesman problem and a description of the traveling salesman polytope by a system of linear equalities and inequalities. The central notion is that of a 3edge cutset namely a set of 3 edges that when removed disconnects the graph. Conversely our approach can be used to construct classes of graphs for which there exists a polynomial algorithm for the traveling salesman problem. The approach is illustrated on two examples Halin graphs and prismatic graphs.
Criteria for adequacy of a data flow semantics are discussed and Kahns successful semantics for functional (deterministic) data flow is reviewed. Problems arising from nondeterminism are introduced and the papers approach to overcoming them is introduced. The approach is based on generalizing the notion of inputoutput relation essentially to a partially ordered multiset of inputoutput histories. The BrockAckerman anomalies concerning the inputoutput relation model of nondeterministic data flow are reviewed and it is indicated how the proposed approach avoids them. A new anomaly is introduced to motivate the use of multisets. A formal theory of asynchronous processes is then developed. The main result is that the operation of forming a process from a network of component processes is associative. This result shows that the approach is not subject to anomalies such as that of Brock and Ackerman.
A distributed computer system that consists of a set of heterogeneous host computers connected in an arbitrary fashion by a communications network is considered. A general model is developed for such a distributed computer system in which the host computers and the communications network are represented by productform queuing networks. In this model a job may be either processed at the host to which it arrives or transferred to another host. In the latter case a transferred job incurs a communication delay in addition to the queuing delay at the host on which the job is processed. It is assumed that the decision of transferring a job does not depend on the system state and hence is static in nature. Performance is optimized by determining the load on each host that minimizes the mean job response time. A nonlinear optimization problem is formulated and the properties of the optimal solution in the special case where the communication delay does not depend on the sourcedestination pair is shown.Two efficient algorithms that determine the optimal load on each host computer are presented. The first algorithm called the parametricstudy algorithm generates the optimal solution as a function of the communication time. This algorithm is suited for the study of the effect of the speed of the communications network on the optimal solution. The second algorithm is a singlepoint algorithm it yields the optimal solution for given system parameters. Queuing models of host computers communications networks and a numerical example are illustrated.
Two of the most powerful classes of programs for which interesting decision problems are known to be solvable are the class of finitememory programs and the class of programs that characterize the Presburger or semilinear sets. In this paper a new class of programs that presents solvable decision problems similar to the other two classes of programs is introduced. However the programs in the new class are shown to be computationally more powerful (i.e. capable of defining larger sets of inputoutput relations).
A projection of a Boolean function is a function obtained by substituting for each of its variables a variable the negation of a variable or a constant. Reducibilities among computational problems under this relation of projection are considered. It is shown that much of what is of everyday relevance in Turingmachinebased complexity theory can be replicated easily and naturally in this elementary framework. Finer distinctions about the computational relationships among natural problems can be made than in previous formulations and some negative results are proved.
The problem of finding a minimum cardinality feedback vertex set of a directed graph is considered. Of the classic NPcomplete problems this is one of the least understood. Although Karp showed the general problem to be NPcomplete a linear algorithm for its solution on reducible flow graphs was given by Shamir. The class of reducible flow graphs is the only nontrivial class of graphs for which a polynomialtime algorithm to solve this problem is known. The main result of this paper is to present a new class of graphsthe cyclically reducible graphsfor which minimum feedback vertex sets can be found in polynomial time. This class is not restricted to flow graphs and most small graphs (10 or fewer nodes) fall into this class. The identification of this class is particularly important since there do not exist approximation algorithms for this problem having a provably good worst case performance. Along with the class and a simple polynomialtime algorithm for finding minimum feedback vertex sets of graphs in the class several related results are presented. It is shown that there is no forbidden subgraph characterization of the class and that there is no particular inclusion relationship between this class and the reducible flow graphs. In addition it is shown that a class of (general) graphs which are related to the reducible flow graphs are contained in the cyclically reducible class.
Given the pairwise probability of conflict p among transactions in a transaction processing system together with the total number of concurrent transactions n the effective level of concurrency E(np) is defined as the expected number of the n transactions that can run concurrently and actually do useful work. Using a random graph model of concurrency we show for three general classes of concurrency control methods examples of which are (1) standard locking (2) strict priority scheduling and (3) optimistic methods that (1) E(n p) les n(1  p2)n1 (2) E(n p) les (1  (1  p)n)p and (3) 1  ((1  p)p)ln(p(n  1)  1) les E(n p) les 1  (1p)ln(p(n  1)  1). Thus for fixed p as n rarrtl ) (1) E rarrtl 0 for standard locking methods (2) E les 1p for strict priority scheduling methods and (3) E rarrtl  for optimistic methods. Also found are bounds on E in the case where conflicts are analyzed so as to maximize E.The predictions of the random graph model are confirmed by simulations of an abstract transaction processing system. In practice though there is a price to pay for the increased effective level of concurrency of methods (2) and (3) using these methods there is more wasted work (i.e. more steps executed by transactions that are later aborted). In response to this problem three new concurrency control methods suggested by the random graph model analysis are developed. Two of these called (a) running priority and (b) older or running priority are shown by the simulation results to perform better than the previously known methods (l)(3) for relatively large n or large p in terms of achieving a high effective level of concurrency at a comparatively small cost in wasted work.
In a distributed database system the partitioning and allocation of the database over the processor nodes of the network can be a critical aspect of the database design effort. In this paper we develop and evaluate algorithms that perform this task in a computationally feasible manner. The network we consider is characterized by a relatively high communication bandwidth considering the processing and input output capacities in its processors. Such a balance is typical if the processors are connected via busses or local networks. The common constraint that transactions have a specific root node no longer exists so that there are more distribution choices. However a poor distribution leads to less efficient computation higher costs and higher loads in the nodes or in the communication network so that the system may not be able to handle the required set of transactions.Our approach is to first split the database into fragments which constitute appropriate units for allocation. The fragments to be allocated are selected based on maximal benefit criteria using a greedy heuristic. The assignment to processor nodes uses a firstfit algorithm. The complete algorithm called GFF is stated in a procedural form.The complexity of the problem and of its candidate solutions are analyzed and several interesting relationships are proven. Alternate benefit metrics are considered since the execution cost of the allocation procedure varies by orders of magnitude with the alternatives of benefit evaluation. A mixed benefit evaluation strategy is eventually proposed.A model for evaluation is presented. Two of the strategies are experimentally evaluated and the reported results support the discussion. The approach should be suitable for other cases where resources have to be allocated subject to resource constraints.
Cubicaliy convergent iterative methods for the solution of nonlinear systems of the multivariate Halley method require first and second partial derivatives of the of the functions comprising the system. Automatic differentiation is used to automate the Halley method HESSIAN and routines for the required operators and functions. A PascalSC which implements this method in a singlestep iteration mode. The program nonlinear systems and the results are compared with Newtons method.
In this paper we discuss the implementation of a primal simplex algorithm for network problems in the MPSIII mathematical programming system. Because of the need to interface with the rest of the MPS this implementation is unorthodox but computationally effective and has a number of advantages over stand alone network optimizers. It is argued that a similar approach is appropriate for other generalpurpose mathematical programming systems and has applications beyond pure network optimization.
We present a method for comparing that part of optimization algorithms that chooses each step direction. It is an example of a general approach to algorithm evaluation in which one tests specific parts of the algorithm rather than making overall evaluations on a set of standard test problems. Our testing procedure can he useful for developing new algorithms and for writing and evaluating optimization software.We use the method to compare two versions of the conjugate gradient algorithm and to compare these with an algorithm based on conic functions.
We introduce fast algorithms for selecting a random sample of n records without replacement from a pool of N records where the value of N is unknown beforehand. The main result of the paper is the design and analysis of Algorithm Z it does the sampling in one pass using constant space and in O(n(1  log(Nn))) expected time which is optimum up to a constant factor. Several optimizations are studied that collectively improve the speed of the naive version of the algorithm by an order of magnitude. We give an efficient Pascallike implementation that incorporates these modifications and that is suitable for general use. Theoretical and empirical results indicate that Algorithm Z outperforms current methods by a significant margin.
A group of 269 firstsemester freshmen was used to predict both performance in an introductory computer science course and firstsemester college grade point average by using information regarding the students programs and performance in high school along with American College Testing Program (ACT) test scores.
Computer science in secondary schools is an area of increasing interest and concern to educators as well as to computer science professionals. Each of the next two reports addresses an issue of major importance regarding computer science in secondary schools. The first report recommends computer science courses for the secondary school curriculum and the second report recommends requirements for teacher certification in computer science.In 1983 the ACM Education Board initiated efforts to formulate recommendations for secondary school computer science. Two task forces one for curriculum recommendations and the other for teacher certification recommendations were established under the Education Boards Elementary and Secondary Schools Subcommittee. The work of the two task forces was also supported by the IEEE Computer Society Educational Activities Board and the final reports from the task forces were jointly approved by the ACM and IEEECS boards in July 1984. Thus the reports are significant not only for the important issues that they address but also because they represent a joint activity between ACM and the IEEE Computer Society.The work of the two task forces is summarized in the next two reports. The full reports are available as the publication Computer Science in Secondary Schools Curriculum and Teacher Certification Order Number 201850 from the ACM Order Department P.O. Box 64145 Baltimore MD 21264.
An existing typesetting system is tied by bridging software to an existing pagepresentation program to effect both hard (typeset) copy and interactive browsing. The typesetting system formats documents for a variety of output devices and the browsing feature appears to the user as simply another output device that happens to be interactive.
This article is both theoretical and empirical. Theoretically it describes three principles of system design which we believe must be followed to produce a useful and easy to use computer system. These principles are early and continual focus on users empirical measurement of usage and iterative design whereby the system (simulated prototype and real) is modified tested modified again tested again and the cycle is repeated again and again. This approach is contrasted to other principled design approaches for example get it right the first time reliance on design guidelines. Empirically the article presents data which show that our design principles are not always intuitive to designers identifies the arguments which designers often offer for not using these principlesand answers them and provides an example in which our principles have been used successfully.
This article studies the effects of queueing delays and users related costs on the management and control of computing resources. It offers a methodology for setting price utilization and capacity taking into account the value of users time and it examines the implications of alternative control structures determined by the financial responsibility assigned to the data processing manager.
An evaluation of a large operational fulltext documentretrieval system (containing roughly 350000 pages of text) shows the system to be retrieving less than 20 percent of the documents relevant to a particular search. The findings are discussed in terms of the theory and practice of fulltext document retrieval.
A major issue in many applications is how to preserve the consistency of data in the presence of concurrency and hardware failures. We suggest addressing this problem by implementing applications in terms of abstract data types with two properties Their objects are atomic (they provide serializability and recoverability for activities using them) and resilient (they survive hardware failures with acceptably high probability). We define what it means for abstract data types to be atomic and resilient. We also discuss issues that arise in implementing such types and describe a particular linguistic mechanism provided in the Argus programming language.
This paper presents a new model for exception handling called the replacement model. The replacement model in contrast to other exceptionhandling proposals supports all the handler responses of resumption termination retry and exception propagation within both statements and expressions in a modular simple and uniform fashion. The model can be embedded in any expressionoriented language and can also be adapted to languages which are not expression oriented with almost all the above advantages. This paper presents the syntactic extensions for embedding the replacement model into Algol 68 and its operational semantics. An axiomatic semantic definition for the model can be found in 27.
Most programming environments are much too complex. One way of simplifying them is to reduce the number of modedependent languages the user has to be familiar with. As a first step towards this end the feasibility of unified commandprogrammingdebugging languages and the concepts on which such languages have to be based are investigated. The unification process is accomplished in two phases. First a unified commandprogramming framework is defined and second this framework is extended by adding an integrated debugging capability to it. Strict rules are laid down by which to judge language concepts presenting themselves as candidates for inclusion in the framework during each phase. On the basis of these rules many of the language design questions that have hitherto been resolved this way or that depending on the taste of the designer lose their vagueness and can be decided in an unambiguous manner.
The CIRCAL calculus is presented as a mathematical framework in which to describe and analyze concurrent systems whether hardware or software.The dot operator is used to compose CIRCAL descriptions and it is this operator which permits the natural modeling of asynchronous and simultaneous behavior thus allowing the representation and analysis of system timing properties such as those found in circuits.The CIRCAL framework uses an abstraction operator to permit the modeling of a system at different levels of detail. Behavioral complexity of real systems makes abstraction crucial when producing a tractable model and we illustrate how abstraction introduces nondeterminisim into system representations.An operational semantics acceptance semantics is introduced and it is in terms of this active experimentation that meaning is given to the CIRCAL syntax thus allowing proof of system properties to be constructed.
Sufficient criteria are given for replacing all occurrences of the store argument in a ScottStrachey denotational definition of a programming language by a single global variable. The criteria and transformation are useful for transforming denotational definitions into compilers and interpreters for imperative machines for optimizing applicative programs and for judging the suitability of semantic notations for describing imperative languages. An example transformation of a semantics of a repeatloop language to one which uses a global store variable is given to illustrate the technique.
Linear hashing is a file structure for dynamic files. In this paper a new simple method for handling overflow records in connection with linear hashing is proposed. The method is based on linear probing and does not rely on chaining. No dedicated overflow area is required. The expansion sequence of liner hashing is modified to improve the performance which requires changes in the address computation. A new address computation algorithm and an expansion algorithm are given. The performance of the method is studied by simulation. The algorithms for the basic file operations are very simple and the overall performance is competitive with that of other variants of linear hashing.
A graph model is presented to analyze the performance of a relational join. The amount of page reaccesses the page access sequence and the amount of buffer needed are represented in terms of graph parameters. By using the graph model formed from the index on the join attributes we determine the relationships between these parameters. Two types of buffer allocation strategies are studied and the upper bound on the buffer size with no page reaccess is given. This bound is shown to be the maximum cut value of a graph. Hence the problem of computing this upper bound is NPhard. We also give algorithms to determine a page access sequence requiring a near optimal buffer size with no page reaccess. The optimal page access sequence for a fixed buffer size has also been considered.
Batching yields significant savings in access costs in sequential tree structured and random files. A direct and simple expression is developed for computing the average number of recordspages accessed to satisfy a batched query of a sequential tile. The advantages of batching for sequential and random files are discussed. A direct equation is provided for the number of nodes accessed in unhatched queries of hierarchical files. An exact recursive expression is developed for node accesses in batched queries of hierarchical files. In addition to the recursive relationship good closedform upper and lowerbound approximations are provided for the case of batched queries of hierarchical files.
Database system support has become an essential part of many computer applications which have extended beyond the more traditional commercial applications to among others engineering applications. Correspondingly application programming with the need to access databases has progressively shifted to scientifically oriented languages.Modern developments in these languages are characterized by advanced mechanisms for the liberal declaration of data types for type checking and facilities for modularization of large programs. The present paper examines how a DBMS can be accessed from such a language in a way that conforms to its syntax and utilizes its typechecking facilities without modifying the language specification itself and hence its compilers. The basic idea is to rely on facilities for defining modules as separately compilable units and to use these to declare userdefined abstract data types.The idea is demonstrated by an experiment in which a specific DBMS (ADABAS) is hosted in the programming language (LIS). The paper outlines a number of approaches and their problems shows how to embed the DML into LIS and how a more useroriented DML can be provided in LIS.
As computers become more powerful and sophisticated computational astrophysicists will be able to find out more about stellar evolution and other astronomical phenomena.
Although lattice field theorists have been able to develop new approaches to the Monte Carlo method and to successfully apply them in Bosonic calculations faster and larger computers are needed for Fermionfield evaluations.
Since computers are able to simulate the equilibrium properties of model systems they may also prove useful for solving the hard optimization problems that arise in the engineering of complex systems.
Computers have expanded the range of nonlinear phenomena that can be explored mathematically. An experimental mathematics facility containing both specialpurpose dedicated machines and generalpurpose mainframes may someday provide the ideal context for complex nonlinear problems.
Specially designed processors can provide a method for attacking some of the difficult computational problems facing theoretical physicists.
Standard programming languages are inadequate for the kind of symbolic mathematical computations that theoretical physicists need to perform. Higher mathematics systems like SMP address this problem.
Bsort a variation of Quicksort combines the interchange technique used in Bubble sort with the Quicksort algorithm to improve the average behavior of Quicksort and eliminate the worst case situation of O(n2) comparisons for sorted or nearly sorted lists. Bsort works best for nearly sorted lists or nearly sorted in reverse.
The performance of sequential search can be enhanced by the use of heuristics that move elements closer to the front of the list as they are found. Previous analyses have characterized the performance of such heuristics probabilistically. In this article we use amortization to analyze the heuristics in a worstcase sense the relative merit of the heuristics in this analysis is different in the probabilistic analyses. Experiments show that the behavior of the heuristics on real data is more closely described by the amortized analyses than by the probabilistic analyses.
A generalized algorithm for graph coloring by implicit enumeration is formulated. A number of backtracking sequential methods are discussed in terms of the generalized algorithm. Some are revealed to be partially correct and inexact. A few corrections to the invalid algorithms which cause these algorithms to guarantee optimal solutions are proposed. Finally some computational results and remarks on the practical relevance of improved implicit enumeration algorithms are given.
Given a general arithmetic expression we find a computation binary tree representation in O(log n) time using nlog n processors on a concurrentread exclusivewrite parallel randomaccess machine.A new algorithm is introduced for this purpose. Unlike previous serial and parallel solutions it is not based on using a stack.
We introduce POINTY an interactive system for constructing worldmodelbased programs for robots. POINTY combines an interactive programming environment with the teachingbyguiding methodology that has been successful in industrial robotics. Owing to its ability to control robots in real time and to interact with the user POINTY provides a friendly and powerful programming environment for robot applications. In the past few years POINTY has been in use at Stanford to write test and debug various robot programs.
We describe a program transformation technique for programs in a general stream language L whereby a datadriven evaluation of the transformed program performs exactly the same computation as a demanddriven evaluation of the original program. The transformational technique suggests a simple denotational characterization of demanddriven evaluation.
If the unique informationprocessing capabilities of protein enzymes could be adapted for computers then evolvable more efficient systems for such applications as pattern recognition and process control are in principle possible.
A survey of application development techniques in 43 organizations identifies the methods and tools found most effective in application software development.
Inexact or realworld queueing techniques are used to determine that the number of buffers provided in system design is indeed adequate to guard against message loss.
Tested against extremes in the characteristics of arrival patterns to dynamic allocation software the workingset approach outperforms the FIFO method except in the case of completely random request patternswhere the workingset method performs as well as the FIFO method.
An empirical study of 282 users of home computers was conducted to explore the relationship between computer use and shifts in time allocation patterns in the household. Major changes in time allocated to various activities were detected. Prior experience with computers (i.e. prior to purchase of the home computer) was found to have a significant impact on the time allocation patterns in the household. The study provides evidence that significant behavior changes can occur when people adopt personal computers in their homes.
A perfect hash function PHF is an injection F from a set W of M objects into the set consisting of the first N nonnegative integers where N ges M. If N  M then F is a minimal perfect hash function MPHF. PHFs are useful for the compact storage and fast retrieval of frequently used objects such as reserved words in a programming language or commonly employed words in a natural language.The mincycle algorithm for finding PHFs executes with an expected time complexity that is polynomial in M and has been used successfully on sets of cardinality up to 512. Given three pseudorandom functions h0 h1 and h2 the mincycle algorithm searches for a function g such that F(w)  (h0(w)  g  h1(w)  g  h2(w)) mod N is a PHF.
cfloadingtexthtmlcfcontextpathcfajaxscriptsrcCFIDEscriptsajaxcfjsonprefixcfclientidB1635C99F93453C48833EF35FB746D23Proc. of a symposium on Software validation inspectiontestingverificationalternatives function settab()  var mytabs  ColdFusion.Layout.getTabLayout(citationdetails) mytabs.on(tabchange function(tabpanelactivetab)  document.cookie  picked  3541    activetab.id ) function letemknow() ColdFusion.Window.show(letemknow)function testthis()alert(test)function loadalert() alert(I am in the load alert) function loadalert2() alert(I am in the load alert2)  google.load(visualization 1 packagesorgchart) google.setOnLoadCallback(drawChart) function drawChart()  var data  new google.visualization.DataTable() data.addColumn(string Name) data.addColumn(string Manager) data.addColumn(string ToolTip) data.addRows( v0 fCCS for this Proceeding
Knowledgeintensive rather than laborintensive processes are being advanced to spur programming productivity.
Models of large and complex systems can often be reduced to smaller submodels for easier analysis by a process known as decomposition. Certain criteria for successful decompositions can be established.
Both static and dynamic Huffman coding techniques are applied to test data consisting of 530 source programs in four different languages. The results indicate that for small files a savings of 2291 percent in compression can be achieved by using the static instead of dynamic techniques.
A displayscreen management system for user interaction with an arbitrary application program is simple enough so that the end user controls the dialogue and screens yet powerful enough to provide for user specification of screen geometry input constraints computation facilities and display logicquite independently of the application system.
A voice interactive natural language system which allows users to solve problems with spoken English commands has been constructed. The system utilizes a commercially available discrete speech recognizer which requires that each word be followed by approximately a 300 millisecond pause. In a test of the system subjects were able to learn its use after about two hours of training. The system correctly processed about 77 percent of the over 6000 input sentences spoken in problemsolving sessions. Subjects spoke at the rate of about three sentences per minute and were able to effectively use the system to complete the given tasks. Subjects found the system relatively easy to learn and use and gave a generally positive report of their experience.
Randomized protocols for signing contracts certified mail and flipping a coin are presented. The protocols use a 1outof2 oblivious transfer subprotocol which is axiomatically defined.The 1outof2 oblivious transfer allows one party to transfer exactly one secret out of two recognizable secrets to his counterpart. The first (second) secret is received with probability one half while the sender is ignorant of which secret has been received.An implementation of the 1outof2 oblivious transfer using any public key cryptosystem is presented.
In a nonnegative edgeweighted network the weight of an edge represents the effort required by an attacker to destroy the edge and the attacker derives a benefit for each new component created by destroying edges. The attacker may want to minimize over subsets of edges the difference between (or the ratio of) the effort incurred and the benefit received. This idea leads to the definition of the strength of the network a measure of the resistance of the network to such attacks. Efficient algorithms for the optimal attack problem the problem of computing the strength and the problem of finding a minimum cost reinforcement to achieve a desired strength are given. These problems are also solved for a different model in which the attacker wants to separate vertices from a fixed central vertex.
This paper reports several properties of heuristic bestfirst search strategies whose scoring functions  depend on all the information available from each candidate path not merely on the current cost g and the estimated completion cost h. It is shown that several known properties of A retain their form (with the minmax of f playing the role of the optimal cost) which helps establish general tests of admissibility and general conditions for node expansion for these strategies. On the basis of this framework the computational optimality of A in the sense of never expanding a node that can be skipped by some other algorithm having access to the same heuristic information that A uses is examined. A hierarchy of four optimality types is defined and three classes of algorithms and four domains of problem instances are considered. Computational performances relative to these algorithms and domains are appraised. For each classdomain combination we then identify the strongest type of optimality that exists and the algorithm for achieving it. The main results of this paper relate to the class of algorithms that like A return optimal solutions (i.e. admissible) when all cost estimates are optimistic (i.e. h  h). On this class A is shown to be not optimal and it is also shown that no optimal algorithm exists but if the performance tests are confirmed to cases in which the estimates are also consistent then A is indeed optimal. Additionally A is also shown to be optimal over a subset of the latter class containing all bestfirst algorithms that are guided by pathdependent evaluation functions.
A new performance model for dynamic locking is proposed. It is based on a flow diagram and uses only the steady state average values of the variables. It is general enough to handle nonuniform access shared locks static locking multiple transaction classes and transactions of indeterminate length. The analysis is restricted to the case in which all conflicts are resolved by restarts. It has been shown elsewhere that under certain conditions this pure restart policy is as good as if not better than a policy that uses both blocking and restarts.The analysis is straightforward and the computational complexity of the solution given some nonrestrictive approximations does not depend on the input parameters. The solution is also well defined and well behaved. The models predictions agree well with simulation results.The model shows that data contention can cause the throughput to thrash and gives a limit on the workload that will prevent this. It also shows that systems with a particular kind of nonuniform access and systems in which transactions share locks are equivalent to systems in which there is uniform access and only exclusive locking. Static locking has higher throughput but longer response time than dynamic locking. Replacing updates by queries in a multiprogramming mix may degrade performance if the queries are longer than the updates.
An analysis of structured flowcharts is presented where size is measured by the number n of decision nodes (IFTHENELSE and DOWHILE nodes). For all classes of structured flowcharts considered the number of charts is approximately cn32ggrn for large n where cand ggr are parameters that depend on the class. It is also shown that most large flowcharts consist of a short sequence of basic charts (IFTHENELSE and DOWHILE charts). The average length of such sequences is 2.5.
The onedimensional online binpacking problem is considered A simple O(1)space and O(n)time algorithm called HARMONICM is presented. It is shown that this algorithm can achieve a worstcase performance ratio of less than 1.692 which is better than that of the O(n)space and O(n log n)time algorithm FIRST FIT. Also shown is that 1.691  is a lower bound for all 0(1)space online binpacking algorithms. Finally a revised version of HARMONICM  an O(n)space and O(n) time algorithm is presented and is shown to have a worstcase performance ratio of less than 1.636.d
A new model of computation for VLSI based on the assumption that time for propagating information is at least linear in the distance is proposed. While accommodating for basic laws of physics the model is designed to be general and technology independent. Thus from a complexity viewpoint it is especially suited for deriving lower bounds and tradeoffs. New results for a number of problems including fanin transitive functions matrix multiplication and sorting are presented. As regards upper bounds it must be noted that because of communication costs the model clearly favors regular and pipelined architectures (e.g. systolic arrays).
The splay tree a selfadjusting form of binary search tree is developed and analyzed. The binary search tree is a data structure for representing tables and lists so that accessing inserting and deleting items is easy. On an nnode splay tree all the standard search tree operations have an amortized time bound of O(log n) per operation where by amortized time is meant the time per operation averaged over a worstcase sequence of operations. Thus splay trees are as efficient as balanced trees when total running time is the measure of interest. In addition for sufficiently long access sequences splay trees are as efficient to within a constant factor as static optimum search trees. The efficiency of splay trees comes not from an explicit structural constraint as with balanced trees but from applying a simple restructuring heuristic called splaying whenever the tree is accessed. Extensions of splaying give simplified forms of two other data structures lexicographic or multidimensional search trees and linkcut trees.
It was conjectured by J. Ullman that uniform hashing is optimal in its expected retrieval cost among all openaddress hashing schemes 4. In this paper we show that for any openaddress hashing scheme the expected cost of retrieving a record from a large table that is agrfraction full is at least (1agr) log (1(1  agr))  o(1). This proves Ullmans conjecture to be true in the asymptotic sense.
The complexity of satisfiability and determination of truth in a particular finite structure are considered for different propositional linear temporal logics. It is shown that these problems are NPcomplete for the logic with F and are PSPACEcomplete for the logics with F X with U with U S X operators and for the extended logic with regular operators given by Wolper.
This work generalizes decision trees in order to study lower bounds on the running times of algorithms that allow probabilistic nondeterministic or alternating control. It is shown that decision trees that are allowed internal randomization (at the expense of introducing a small probability of error) run no faster asymptotically than ordinary decision trees for a collection of natural problems. Two geometric techniques from the literature for proving lower bounds on the time required by ordinary decision trees are shown to be special cases of one unified technique that in fact applies to nondeterministic decision trees as well. Finally it is shown that any lower bound on alternating decision tree time also applies to alternating Turing machine time.
A database is said to allow range restrictions if one may request that only records with some specified field in a specified range be considered when answering a given query. A transformation is presented that enables range restrictions to be added to an arbitrary dynamic data structure on n elements provided that the problem satisfies a certain decomposability condition and that one is willing to allow increases by a factor of O(log n) in the worstcase time for an operation and in the space used. This is a generalization of a known transformation that works for static structures. This transformation is then used to produce a data structure for range queries in k dimensions with worstcase times of O(logk n) for each insertion deletion or query operation.
We study restricted classes of Btrees called H(bgr ggr dgr) trees. A class is defined by three parameters bgr the size of a node ggr the minimal number of grandsons a node must have and dgr the minimal number of leaves bottom nodes must have. This generalizes the brother condition of 23 brother trees in a uniform way to Btrees of higher order. The class of Btrees of order m is obtained by choosing bgr  m ggr  (m2)2 and dgr  m2. An algorithm to construct Htrees for any given number of keys is given in Section 1. Insertion and deletion algorithms are given in Section 2. The costs of these algorithms increase smoothly as the parameters are increased. Furthermore it is proved that the insertion can be done in time O(bgr  log N) where N is the number of nodes in the tree. Deletion can also be accomplished without reconstructing the entire tree. Properties of Htrees are given in Section 3. It is shown that the height of Htrees decreases as ggr increases and the storage utilization increases significantly as dgr increases. Finally comparisons with other restricted classes of Btrees are given in Section 4 to show the attractiveness of Htrees.
Galileo a programming language for database applications is presented. Galileo is a stronglytyped interactive programming language designed specifically to support semantic data model features (classification aggregation and specialization) as well as the abstraction mechanisms of modern programming languages (types abstract types and modularization). The main contributions of Galileo are (a) a flexible type system to model database structure and semantic integrity constraints (b) the inclusion of type hierarchies to support the specialization abstraction mechanisms of semantic data models (c) a modularization mechanism to structure data and operations into interrelated units (d) the integration of abstraction mechanisms into an expressionbased language that allows interactive use of the database without resorting to a new standalone query language.Galileo will be used in the immediate future as a tool for database design and in the long term as a highlevel interface for DBMSs.
We propose a new class of schedulers called cautious schedulers that grant an input request if it will not necessitate any rollback in the future. In particular we investigate cautious WRWschedulers that output schedules in class WRW only. Class WRW consists of all schedules that are serializable while preserving the writeread and readwrite conflict and is the largest polynomially recognizable subclass of serializable schedules currently known. It is shown in this paper however that cautious WRWscheduling is in general NPcomplete. Therefore we introduce a special type (type 1R) of transaction which consists of no more than one read step (an indivisible set of read operations) followed by multiple write steps. It is shown that cautious WRWscheduling can be performed efficiently if all transactions are of type 1R and if admission control can be exercised. Admission control rejects a transaction unless its first request is immediately grantable.
An algorithm for record clustering is presented. It is capable of detecting sudden changes in users access patterns and then suggesting an appropriate assignment of records to blocks. It is conceptually simple highly intuitive does not need to classify queries into types and avoids collecting individual query statistics. Experimental results indicate that it converges rapidly its performance is about 50 percent better than that of the total sort method and about 100 percent better than that of randomly assigning records to blocks.
We introduce the notion of an IO interface for optical digital (writeonce) disks which is quite different from earlier research. The purpose of an IO interface is to allow existing operating systems and application programs that use magnetic disks to use optical disks instead with minimal change. We define what it means for an IO interface to be diskefficient. We demonstrate a practical disk efficient IO interface and show that its IO performance in many cases is optimum up to a constant factor among all diskefficient interfaces. The interface is most effective for applications that are not updateintensive. An additional capability is a builtin history mechanism that provides software support for accessing previous versions of records. Even if not implemented the IO interface can be used as a programming tool to develop efficient special purpose applications for use with optical disks.
In this paper cost formulas are derived for the updates of data and indexes in a relational database. The costs depend on the data scan type and the predicates involved in the update statements. We show that update costs have a considerable influence both in the context of the physical database design problem and in access path selection in query optimization for relational DBMSs.
We describe a prototype electronic encyclopedia implemented on a powerful personal computer in which user interface media presentation and knowledge representation techniques are applied to improving access to a knowledge resource. In itself an electronic encyclopedia is an important information resource but this work also illustrates the issues and approaches for many types of electronic information retrieval environments. In the prototype we make dynamic use of the structure and semantics of the text articles and index of an existing encyclopedia while experimenting with other forms of representation such as simulation and videodisc images. We present a longterm vision of an intelligent userinterface agent summarize previous work related to futuristic encyclopedias electronic books decision support systems and knowledge libraries and outline current and potential research directions.
Local area networks are becoming widely used as the database communication framework for sophisticated information systems. Databases can be distributed among stations on a network to achieve the advantages of performance reliability availability and modularity. Efficient distributed query optimization algorithms are presented here for two types of local area networks address ring networks and broadcast networks. Optimal algorithms are designed for simple queries. Optimization principles from these algorithms guide the development of effective heuristic algorithms for general queries on both types of networks. Several examples illustrate distributed query processing on local area networks.
Nine participants used a fullscreen computer text editor (XEDIT) with an IBM 3277 terminal to edit markedup documents at each of three cursor speeds (3.3 4.7 and 11.0 cms). These speeds occur when a user continuously holds down an arrow key to move the cursor more than one character position (i.e. in repeat or typamatic mode). Results show that cursor speed did not seem to act as a pacing device for the entire editing task. Since cursor speed is a form of system response this finding is in contrast with the generally found positive relation between systemresponse time and userresponse time. Participants preferred the Fast cursor speed however. Overall more than onethird of all keystrokes were used to move the cursor. We estimate that 914 percent of editing time was spent controlling and moving the cursor regardless of cursor speed.
Manufacturers of integrated electronic office systems have included electronic versions of the calendar in almost every system they offer. This paper describes a survey of office workers carried out to examine their use both of paper calendars and of electronic calendars that are commercially available as part of integrated office systems. It assesses the degree to which electronic calendars meet the needs of users. Our survey shows that the simple paper calendar is a tool whose power and flexibility is matched by few if any of the current commercially available electronic calendars. Recommendations for features that should be included in electronic calendars and automatic schedulers are included.
A model and methodology for describing the information objects in an office information system and how such objects flow among the components of such a system are presented. The model and methodology support the specification of information objects at multiple levels of abstraction. An interactive prototype design tool based on the methodology and model has been designed and experimentally implemented.
Unless computermediated communication systems are structured users will be overloaded with information. But structure should be imposed by individuals and user groups according to their needs and abilities rather than through general software features.
Strategic Computing a 10year initiative to build faster and more intelligent systems is ambitious flawed by overscheduling perhaps and problems of definition but basically sound.
A wholesystem designer fire fighter mediator and jackofalltrades the system architect brings unity and continuity to a development projectoffsetting the inevitable compartmentalization of modern modular designs.
An examination of the respective advantages and disadvantages of three characteristic paradigms of design and implementation in Ada illustrates the importance of choosing the appropriate paradigm for a given set of circumstances.
This article tests two competing theories of system development referred to here as environmental and institutional models. These models form the basis for most explanations of why systems are developed and utilized. We will examine both models in detail and apply them to a single set of data concerned with the emerging national computerized criminal history system (CCH). A hybrid model which combines elements of environmental and institutional approaches is also developed and tested. A substantive result of this new model will alter our understanding of why a national CCH system is being developed. At the theoretical level we conclude that a hybrid model is more powerful than either an environmental or an institutional model taken separately and that future research must take this into account.
The transient probabilistic structure of MEm1 and EmM1 queues initialized in an arbitrary deterministic state is derived in discrete time. Computational algorithms for obtaining the required probabilities are provided and their application in calculating a variety of system performance measures is illustrated. The results are used to investigate the question of initializing simulations of systems such as these to promote rapid convergence to steady state if that is the object of the simulation. These results are consistent with earlier studies for transient queueing systems such as the MMs but allow greater flexibility in specification of interarrival or servicetime models inherent in the Erlang distributions.
Algorithms are described and analyzed for the efficient evaluation of the primitive operators of a relational algebra on a database machine architecture. The architecture contains a RAM divided up into partitions each partition having a separate server. Tuples from the first operand relation are stored in linked lists in the partitioned RAM via bit and pointer arrays based on hashed column values. Each tuple in the second operand relation is either deposited in the corresponding partition server queue based on the hashed value or it is ignored. Crossreferencing involved in the operations is removed without performing a sorting operation which significantly reduces the time complexity. A procedure is also presented for computing the optimal number of partition servers for different applications.
In order to remain tractable mhany reliability models do not include the states and transitions necessary to represent faulterrorhandling details. Instead the effectiveness of fault errorhandling mechanisms is represented by the use of instantaneous coverage probabilities. This paper investigates the effect of the error introduced by the assumption of instantaneous coverage probabilities on the predictions of the reliability model and it shows that the reliability estimates thus obtained are lower bounds on the reliability estimates of the composite model with embedded faulterrorhandling states and transitions. The paper also discusses the choice of the calculation method for the instantaneous coverage probabilities and defines a nearcoincidentfault coverage model that yields conservative instantaneous coverage probabilities.
A computer system is modeled by an exponential queueing network with different classes of customers and a general class of service policies. The mean cost per unit time is taken as the loss function. Three lower bounds of the loss function under the whole class of service policies as well as an upper bound of the minimal loss are derived. The loss function is expressed in a convenient form from which we derive a simple heuristic service policy so called Klimov policy. This policy is applied to several examples of computer systems and is evaluated by the bounds. The examples recommend using Klimov policy in any case so we have a first rule for deciding how to provide service capacity to different customers in a network of queues.
Two conversion techniques based on the Chinese remainder theorem are developed for use in residue number systems. The new implementations are fast and simple mainly because adders modulo a large and arbitrary integer M are effectively replaced by binary adders and possibly a lookup table of small address space. Although different in form both techniques share the same principle that an appropriate representation of the summands must be employed in order to evaluate a sum modulo M efficiently. The first technique reduces the sum modulo M in the conversion formula to a sum modulo 2 through the use of fractional representation which also exposes the sign bit of numbers. Thus this technique is particularly useful for sign detection and for any operation requiring a comparison with a binary fraction of M. The other technique is preferable for the full conversion from residues to unsigned or 2s complement integers. By expressing the summands in terms of quotients and remainders with respect to a properly chosen divisor the second technique systematically replaces the sum modulo M by two binary sums one accumulating the quotients modulo a power of 2 and the other accumulating the remainders the ordinary way. A final recombination step is required but is easily implemented with a small lookup table and binary adders.
This paper is concerned with some of the issues arising in the development of a domainindependent English interface to IBM SQLbased program products. The TQA system falls into the class of multilayered natural language processing systems. As a result there is a large number of potential points at which customization to a particular database can be done. Of these we discuss procedures that affect the reader the lexicon the lowest level of grammar rules the semantic interpreter and the output formatter. Our tests lead us to believe that the approach we are taking will make it possible for database administrators to generate robust English interfaces to particular databases without help from linguistic experts.
Program transformations are frequently performed by optimizing compilers and the correctness of applying them usually depends on data flow information. For languagetosamelanguage transformations it is shown how a denotational setting can be useful for validating such program transformations.Strong equivalence is obtained for transformations that exploit information from a class of forward data flow analyses whereas only weak equivalence is obtained for transformations that exploit information from a class of backward data flow analyses. To obtain strong equivalence both the original and the transformed program must be data flow analysed but consideration of a transformationexploiting liveness of variables indicates that a more satisfactory approach may be possible.
Analytic models based on closed queuing networks (CQNS) are widely used for performance prediction in practical systems. In using such models there is always a prediction error that is a difference between the predicted performance and the actual outcome. This prediction error is due both to modeling errors and estimation errors the latter being the difference between the estimated values of the CQN parameters and the actual outcomes. This paper considers the second class of errors in particular it studies the effect of small estimation errors and provides bounds on prediction errors based on bounds on estimation errors. Estimation errors may be divided into two types (1) the difference between the estimated value and the average value of the outcome and (2) the deviation of the actual value from its average. The analysis first studies the sum of both types of errors then the second type alone. The results are illustrated with three examples.
The current trend of computer system technology is toward CPUs with rapidly increasing processing power and toward disk drives of rapidly increasing density but with disk performance increasing very slowly if at all. The implication of these trends is that at some point the processing power of computer systems will be limited by the throughput of the inputoutput (IO) system.A solution to this problem which is described and evaluated in this paper is disk cache. The idea is to buffer recently used portions of the disk address space in electronic storage. Empirically it is shown that a large (e.g. 8090 percent) fraction of all IO requests are captured by a cache of an 8Mbyte orderofmagnitude size for our workload sample. This paper considers a number of design parameters for such a cache (called cache disk or disk cache) including those that can be examined experimentally (cache location cache size migration algorithms block sizes etc.) and others (access time bandwidth multipathing technology consistency error recovery etc.) for which we have no relevant data or experiments. Consideration is given to both caches located in the IO system as with the storage controller and those located in the CPU main memory. Experimental results are based on extensive tracedriven simulations using traces taken from three large IBM or IBMcompatible mainframe data processing installations. We find that disk cache is a powerful means of extending the performance limits of highend computer systems.
Optimistic Recovery is a new technique supporting applicationindependent transparent recovery from processor failures in distributed systems. In optimistic recovery communication computation and checkpointing proceed asynchronously. Synchronization is replaced by causal dependency tracking which enables a posteriori reconstruction of a consistent distributed system state following a failure using process rollback and message replay.Because there is no synchronization among computation communication and checkpointing optimistic recovery can tolerate the failure of an arbitrary number of processors and yields better throughput and response time than other general recovery techniques whenever failures are infrequent.
In Part I Markov chains were shown to be associated with solutions to several standard problems in computeraided geometric design. Constraints on these Markov chains were also derived. Examples are given here of Markov chains that either satisfy some of these constraints or solve one of these problems. Subdivision matrices are also studied in special detail.
Quadtree representation of twodimensional objects is performed with a tree that describes the recursive subdivision of the more complex parts of a picture until the desired resolution is reached. At the end all the leaves of the tree are square cells that lie completely inside or outside the object. There are two great disadvantages in the use of quadtrees as a representation scheme for objects in geometric modeling system The amount of memory required for polygonal objects is too great and it is difficult to recompute the boundary representation of the object after some Boolean operations have been performed. In the present paper a new class of quadtrees in which nodes may contain zero or one edge is introduced. By using these quadtrees storage requirements are reduced and it is possible to obtain the exact backward conversion to boundary representation. Algorithms for the generation of the quadtree Boolean operations and recomputation of the boundary representation are presented and their complexities in time and space are discussed. Threedimensional algorithms working on octrees are also presented. Their use in the geometric modeling of threedimensional polyhedral objects is discussed.
Two incremental linear interpolation algorithms are derived and analyzed for speed and accuracy. The first is a version of a simple digital differential analyzer (DDA) employing fixedpoint arithmetic whereas the second is a new algorithm that uses only integral arithmetic and is a generalization of Bresenhams linedrawing algorithm. The new algorithm is shown to achieve perfect accuracy and depending on the underlying processor may be faster than the fixedpoint algorithm.
We examine methods of implementing queries about relational databases in the case where these queries are expressed in firstorder logic as a collection of Horn clauses. Because queries may be defined recursively straightforward methods of query evaluation do not always work and a variety of strategies have been proposed to handle subsets of recursive queries. We express such query evaluation techniques as capture rules on a graph representing clauses and predicates. One essential property of capture rules is that they can be applied independently thus providing a clean interface for queryevaluation systems that use several different strategies in different situations. Another is that there be an efficient test for the applicability of a given rule. We define basic capture rules corresponding to application of operators from relational algebra a topdown capture rule corresponding to backward chaining that is repeated resolution of goals a bottomup rule corresponding to forward chaining where we attempt to deduce all true facts in a given class and a sideways rule that allows us to pass results from one goal to another.
We discuss a recently launched longterm project in natural language processing the primary concern of which is that natural language applications be transportable among human languages. In particular we seek to develop system tools and linguistic processing techniques that are themselves languageindependent to the maximum extent practical. In this paper we discuss our project goals and outline our intended approach address some crosslinguistic requirements and then present some new linguistic data that we feel support our approach.
This paper presents a discussion of the techniques developed and problems encountered during the design implementation and experimental use of a portable natural language processor. Datalog (for database dialogue) is an experimental natural language query system which was designed to achieve a maximum degree of portability and extendibility. Datalog uses a threelevel architecture to provide both portability of syntax to new and extended tasks and portability of semantics to new database applications. The implementation of each of the three levels the structures and conventions that control the interactions among them and the way in which different aspects of the design contribute to portability are described. Finally two specific implemented examples are presented showing how it was possible to transport or extend Datalog by changing only one layer of the systems knowledge and achieve correct processing of the extended input by the entire system.
This paper is a discussion of the technical issues and solutions encountered in making the ASK System transportable. A natural language system can be transportable in a number of ways. Although transportability to a new domain is most prominent other ways are also important if the system is to have viability in the commercial marketplace.On the one hand transporting a system to a new domain may start with the system prior to adding any domain of knowledge and extend it to incorporate the new domain. On the other hand one may wish to add to a system that already has knowledge of one domain the knowledge concerning a second domain that is to extend the system to cover this second domain. In the context of ASK it has been natural to implement extending and then achieve transportability as a special case.In this paper we consider six ways in which the ASK System can be extended to include new capabilitiesto a new domainto a new object typeto access data from a foreign databaseto a new natural languageto a new programming languageto a new computer family.Specialpurpose applications such as those to accommodate standard office tasks would make use of these various means of extension.
The Linguistic String Project (LSP) natural language processing system has been developed as a domainindependent natural language processing system. Initially utilized for processing sets of medical messages and other texts in the medical domain it has been used at the Naval Research Laboratory for processing Navy messages about shipboard equipment failures. This paper describes the structure of the LSP system and the features that make it transportable from one domain to another. The processing procedures encourage the isolation of domainspecific information yet take advantage of the syntactic and semantic similarities between the medical and Navy domains. From our experience in transporting the LSP system we identify the features that are required for transportable natural language systems.
PRE (Purposefully Restricted English) is a restricted English database query language whose implementation has addressed engineering goals namely habitability interapplication transportability performance and use with a reliable database management system that supports large numbers of concurrent users and large databases. Habitability has not been demonstrated but initial indications are encouraging. The other goals have clearly been achieved. The existence of the PRE system demonstrates that an explicitly minimalist approach to natural language processing can facilitate achievement of transportability.
Our goal is to develop formal methods for abstracting a given set of programs into a program schemaand for instantiating a given schema to satisfy concrete specifications. Abstraction and instantiationare two important phases in software development which allow programmers to apply knowledgelearned in the solutions of past problems when faced with new situations. For example from twoprograms using a linear (or binary) search technique an abstract schema can be derived that embodiesthe shared idea and that can be instantiated to solve similar new problems. Along similar lines thedevelopment and application of program transformations are considered.We suggest the formulation of analogies as a basic tool in program abstraction. An analogy is firstsought between the specifications of the given programs this yields an abstract specification thatmay be instantiated to any of the given concrete specifications. The analogy is then used as a basisfor transforming the existing programs into an abstract schema that represents the embeddedtechnique with the invariant assertions and correctness proofs of the given programs helping toverify and complete the analogy. A given concrete specification of a new problem may then becompared with the abstract specification of the schema to suggest an instantiation of the schemathat yields a correct program.
An important goal of programming language research is to isolate the fundamenal concepts of languages those basic ideas that allow us to understand the relationships among various language features. This paper examines one of these underlying notions that of data type with particular attention to the treatment of generic or polymorphic procedures and static typechecking.
Virtual time is a new paradigm for organizing and synchronizing distributed systems which can be applied to such problems as distributed discrete event simulation and distributed database concurrency control. Virtual time provides a flexible abstraction of real time in much the same way that virtual memory provides an abstraction of real memory. It is implemented using the Time Warp mechanism a synchronization protocol distinguished by its reliance on lookaheadrollback and by its implementation of rollback via antimessages.
In this paper we outline an approach to describing and analyzing designs for distributed software systems. A descriptive notation is introduced and analysis techniques applicable to designs expressed in that notation are presented. The usefulness of the approach is illustrated by applying it to a realistic distributed softwaresystem design problem involving mutual exclusion in a computer network.
This paper introduces data distortion by probability distribution a probability distortion that involves three steps. The first step is to identify the underlying density function of the original series and to estimate the parameters of this density function. The second step is to generate a series of data from the estimated density function. And the final step is to map and replace the generated series for the original one. Because it is replaced by the distorted data set probability distortion guards the privacy of an individual belonging to the original data set. At the same time the probability distorted series provides asymptotically the same statistical properties as those of the original series since both are under the same distribution. Unlike conventional point distortion probability distortion is difficult to compromise by repeated queries and provides a maximum exposure for statistical analysis.
VLSI CAD applications deal with design objects that have an interface description and an implementation description. Versions of design objects have a common interface but differ in their implementations. A molecular object is a modeling construct which enables a database entity to be represented by two sets of heterogeneous records one set describes the objects interface and the other describes its implementation. Thus a reasonable starting point for modeling design objects is to begin with the concept of molecular objects.In this paper we identify modeling concepts that are fundamental to capturing the semantics of VLSI CAD design objects and versions in terms of molecular objects. A provisional set of user operations on design objects consistent with these modeling concepts is also defined. The modeling framework that we present has been found useful for investigating physical storage techniques and change notification problems in version control.
A parser must be able to continue parsing after encountering a syntactic error to check the remainder of the input. To achieve this it is not necessary to perform corrections on either the input text or the stack contents. A formal framework is provided in which noncorrecting syntax error recovery concepts are defined and investigated. The simplicity of these concepts allows the statement of provable properties such as the absence of spurious error messages or the avoidance of skipping input text. These properties are due to the fact that no assumptions about the nature of the errors need be made to continue parsing.
The CIRRUS banking network makes coasttocoast automatic banking transactions possible. The system will soon be able to handle international currency transactions and pointofsale transactions in stores.
ALGLIBa library of procedures that perform analytic differentiation and other simple symbolic manipulationshas certain advantages over existing and more comprehensive packages. It can be implemented in a highlevel language of the users choice using a pseudocode available from the authors and it is easily interfaced with the users programs.
A new simplified methodology for relationaldatabase design overcomes the difficulties associated with nonloss decomposition. It states dependencies between data fields on a dependency list and then depicts them unambiguously as interlinked bubbles and doublebubbles on a dependency diagram. From the dependency diagram a set of fully normalized tables is derived.
In this article we develop some algorithms and tools for solving matrix problems on parallel processing computers. Operations are synchronized through dataflow alone which makes global synchronization unnecessary and enables the algorithms to be implemented on machines with very simple operating systems and communication protocols. As examples we present algorithms that form the main modules for solving Liapounov matrix equations. We compare this approach to wave front array processors and systolic arrays and note its advantages in handling missized problems in evaluating variations of algorithms or architectures in moving algorithms from system to system and in debugging parallel algorithms on sequential machines.
Diehr and Faaland developed an algorithm that finds the minimum sum of key length pagination of a scroll of n items and which uses O(n log n) time solving a problem posed by McCreight. An improved algorithm is given which uses O(n) time.
Model building is identified as the most important part of the analysis and design process for software systems. A set of primitives to support this process is presented along with a formal language MSG.84 for recording the results of analysis and design. The semantics of the notation is defined in terms of the actor formalism which is based on a message passing paradigm. The automatic derivation of a graphical form of the specification for user review is discussed. Potentials for computeraided design based on MSG.84 are indicated.
Few examples of formal specification of the semantics of user interfaces exist in the literature. This paper presents a comparison of four axiomatic approaches which we have applied to the specification of a commercial user interfacethe line editor for the Tandy PC1 Pocket Computer. These techniques are shown to result in complete and relatively concise descriptions. A number of useful and nontrivial properties of the interface are formally deduced from one of the specifications. In addition a direct implementation of the interface is constructed from a formal specification. Limitations of these specification examples are discussed along with future research work.
Forms have become widely used as a user interface for database systems. By analyzing the components of forms a unified treatment of aggregation operators headings subheadings and internal logic of a form is possible. In this paper we present a theory and a system based on that theory which allows users to easily specify displays. Displays are composed of components and the user can specify geometry of the components and grouping relationships between the components which allow for the generation of the desired display.
User Software Engineering is a methodology for the specification and implementation of interactive information systems. An early step in the methodology is the creation of a formal executable description of the user interaction with the system based on augmented state transition diagrams. This paper shows the derivation of the USE transition diagrams based on perceived shortcomings of the pure state transition diagram approach. In this way the features of the USE specification notation are gradually presented and illustrated. The paper shows both the graphical notation and the textual equivalent of the notation and briefly describes the automated tools that support direct execution of the specification.
Aesthetics in user interfaces addresses font definitions typesetting conventions color combinations graphics design considerations high resolution for viewscreens and the shapes of windows. Computer viewscreens are evolving into pictorial media communicating information with visual immediacy. The more interesting interfaces make use of multiple windows menus icons and other visual effects to waken and sustain user interest and effectiveness. A pretty window is a viewscreen window with the dimensions of a golden rectangle a rectangle whose width and height form the golden ratio of Euclid. Psychologists believe golden rectangles are aesthetically more pleasing than arbitrary rectangles and subjects tend to select them in preference to other rectangles in tests.
In the development of programs for novel applications a series of designimplementation phases may be necessary in order to acquire a deeper understanding of the problem. This paper develops a feedback version development approach incorporating systematic knowledge integration techniques. Our experiences in applying these methods to various projects are also discussed.
An office procedure is a structured set of office activities for accomplishing a specific office task. A unified model called office procedure model (OPM) is presented to model office procedures. The OPM describes the relationships among messages databases alerters and activities. The OPM can be used to coordinate and integrate the activities of an office procedure. The OPM also allows the specification of office protocols in an office information system. A methodology for the verification of office procedures is presented. With this methodology potential problems in office procedure specification such as deadlock unspecified message reception etc. can be analyzed effectively.
When the state of a program in execution is accidentally altered a recovery action may be needed before the execution can proceed on. Two approaches exist for the design of recovery actions backward recovery consists of retrieving a previously saved correct state and restarting the computation forward recovery consists of generating  (sufficiently) correct state from the current (not too) contaminated state. This paper presents a tentative framework for the study of forward error recovery and then discusses some preliminary results and some future research within the proposed framework.
This paper examines the use of cluster analysis as a tool for system modularization. Several clustering techniques are discussed and used on two mediumsize systems and a group of small projects. The small projects are presented because they provide examples (that will fit into a paper) of certain types of phenomena. Data bindings between the routines of the system provide the basis for the bindings. It appears that the clustering of data bindings provides a meaningful view of system modularization.
The notion of a qualified function is introduced as a general means of representing the parameters of dynamic systems. Two specific types of qualified functions are defined for the analysis of the behavior and performance of structured programs. Transformation functions represent the values of variables during execution and timing algorithms express the execution times of programs symbolically. Complete rules of derivation for transformation functions and timing algorithms are given for the control mechanisms of sequence selection fixed loop and while statement. Deterministic and stochastic simplification of transformation functions and timing algorithms are investigated and methods of eliminating recursion for expressions corresponding to while statements are studied.
A distributed system is viewed as a set of objects and processes utilizing the objects. If a shared object known as a resource is accessed concurrently some mechanism is necessary to control use of the resource in order to satisfy the consistency and fairness requirements associated with the resource. These mechanisms are termed resource controllers.
A class of transformations of functional programs based on symbolic execution and simplification of conditionals is presented. The operational symbolic semantics of a family of functional languages is defined exploiting a settheoretic notion of symbolic constants. An effective transformation able to simplify a functional program via removal of conditionals is discussed. Finally it is shown that a structural approach based on abstract data type specifications provides a suitable representation for symbolic constants.
This paper describes the query optimizer of the Mermaid system which provides a user with a unified view of multiple preexisting databases which may be stored under different DBMSs. The algorithm is designed for databases which may contain replicated or fragmented relations and for users who are primarily making interactive ad hoc queries. Although the implementation of the algorithm is a frontend system not an integrated distributed DBMS it should be applicable to a distributed DBMS also.
This paper describes a generic image processing language IPL and a programming environment supporting the language primitives for an image information system. The central notion of IPL is that it allows the user to navigate through the image database and manipulate images using generalized icons. The image processing language IPL consists of three subsets the logical image processing language LIPL the interactive image processing language IIPL and the physical image processing language PIPL. This paper presents the main concepts of this generic language some examples and a scenario.
Mathematical models when simulating the behavior of physical chemical and biological systems often include one or more ordinary differential equations (ODEs). To study the system behavior predicted by a model these equations are usually solved numerically.Although many of the current methods for solving ODEs were developed around the turn of the century the past 15 years or so has been a period of intensive research. The emphasis of this survey is on the methods and techniques used in software for solving ODEs.ODEs can be classified as stiff or nonstiff and may be stiff for some parts of an interval and nonstiff for others. We discuss stiff equations why they are difficult to solve and methods and software for solving both nonstiff and stiff equations. We conclude this review by looking at techniques for dealing with special problems that may arise in some ODEs for example discontinuities.Although important theoretical developments have also taken place we report only those developments which have directly affected the software and provide a review of this research. We present the basic concepts involved but assume that the reader has some background in numerical computing such as a first course in numerical methods.
This paper compares text retrieval methods intended for office systems. The operational requirements of the office environment are discussed and retrieval methods from database systems and from information retrieval systems are examined. We classify these methods and examine the most interesting representatives of each class. Attempts to speed up retrieval with special purpose hardware are also presented and issues such as approximate string matching and compression are discussed. A qualitative comparison of the examined methods is presented. The signature file method is discussed in more detail.
A generalpurpose computer vision system must be capable of recognizing threedimensional (3D) objects. This paper proposes a precise definition of the 3D object recognition problem discusses basic concepts associated with this problem and reviews the relevant literature. Because range images (or depth maps) are often used as sensor input instead of intensity images techniques for obtaining processing and characterizing range data are also surveyed.
A major portion of the effort expended in developing commercial software today is associated with program testing. Schedule and or resource constraints frequently require that testing be conducted so as to uncover the greatest number of errors possible in the time allowed. In this paper we describe a study undertaken to assess the potential usefulness of various productand processrelated measures in identifying errorprone software. Our goal was to establish an empirical basis for the efficient utilization of limited testing resources using objective measurable criteria. Through a detailed analysis of three software products and their error discovery histories we have found simple metrics related to the amount of data and the structural complexity of programs to be of value for this purpose.
In this paper we present a translator from a relevant subset of SQL into relational algebra. The translation is syntaxdirected with translation rules associated with grammar productions each production corresponds to a particular type of SQL subquery.
The performance of transaction processing systems is determined by the contention for hardware as well as software resources (database locks) due to the concurrency control mechanism of the database being accessed by transactions. We consider a transaction processing system with a set of dominant transcation classes. Each class needs to acquire a certain subset of the locks in the database before it can be processed i.e. predeclared lock requests with static locking. Straightforward application of the decomposition method requires the numerical solution of a twodimensional Markov chain. Equivalently a hierarchical simulation method where the computer system is represented by a composite queue with exponential service rates can be used to analyze the system. We propose an inexpensive analytic solution method also based on hierarchical decomposition such that the throughput of the computer system ic characterized by the number of active transactions (regardless of class). Numerical results are provided to show that the new method is adequately accurate compared to the other two rather costly methods. It can be used to determine the effect of granularity of locking on system performance. The solution method is also applicable to multiresource queueing systems with multiple contention points.
SEES is a database system to support program testing. The program database is automatically created during the compilation of the program by a compiler built using the YACC compilercompiler.
This paper defines a family of program test data selection criteria derived from data flow analysis techniques similar to those used in compiler optimization. It is argued that currently used path selection criteria which examine only the control flow of a program are inadequate quate. Our procedure associates with each point in a program at which a variable is defined those points at which the value is used. Several test data selection criteria differing in the type and number of these associations are defined and compared.
In this paper an approach to the modeling of software testing is described. A major aim of this approach is to allow the assessment of the effects of different testing (and debugging) strategies in different situations. It is shown how the techniques developed can be used to estimate prior to the commencement of testing the optimum allocation of test effort for software which is to be nonuniformly executed in its operational phase. In addition the question of application of statistical models in cases where the data environment undergoes changes is discussed. Finally two models are presented for the assessment of the effects of imperfections in the debugging process.
Researchers at Bell Labs have recently developed a silicon compiler named Plex that automatically generates VLSI layouts of high performance and area efficient microprocessors. Plex takes as input a specification of the function to be executed and generates a complete masklevel layout of a customized microprocessor to execute that function. The Plex microprocessor interacts with the external world via input and output wires and interrupts. The dedicated function performed by a Plex microprocessor typically involves realtime handling of inputs and interruptsand the realtime generation of output signals.
A distributed program is a collection of several processes which execute concurrently possibly in different nodes of a distributed system and which cooperate with each other to realize a common goal. In this paper we present a design of communication and synchronization primitives for distributed programs. The primitives are designed such that they can be provided by a kernel of a distributed operating system. An important feature of the design is that the configuration of a process i.e. identities of processes with which the process communicates is specified separately from the computation performed by the process. This permits easy configuration and reconfiguration of processes. We identify different kinds of communication failures and provide distinct mechanisms for handling them. The communication primitives are not atomic actions. To enable the construction of atomic actions two new program components atomic agent and manager are introduced. These are devoid of policy decisions regarding concurrency control and atomic commitment. We introduce the notion of conflicts relation using which a designer can construct either an optimistic or a pessimistic concurrency control scheme. The design also incorporates primitives for constructing nested atomic actions.
Basic graph models of processes such as Petri nets have usually omitted the concept of time as a parameter. Time has been added to the Petri net model in two ways. The timed Petri net (TPN) uses a fixed number of discrete time intervals. The stochastic Petri net (SPN) uses an exponentially distributed random variable. In this paper a discrete time stochastic Petri model is described. These discrete time SPNs fill the gap between TPN and normal SPN. However the use of discrete time complicates the SPN model in that more than one transition may fire at a time step. Finally an example of a live and bounded Petri net which has nonempty disjoint recurrent subsets of markings is given.
Dynamic system configuration is the ability to modify and extend a system while it is running. The facility is a requirement in large distributed systems where it may not be possible or economic to stop the entire system to allow modification to part of its hardware or software. It is also useful during production of the system to aid incremental integration of component parts and during operation to aid system evolution. The paper introduces a model of the configuration process which permits dynamic incremental modification and extension. Using this model we determine the properties required by languages and their execution environments to support dynamic configuration. CONIC the distributed system which has been developed at Imperial College with the specific objective of supporting dynamic configuration is described to illustrate the feasibility of the model.
A class of communication networks which is suitable for multiple processor systems was studied by Pradhan and Reddy. The underlying graph (to be called Shift and Replace graph or SRG) is based on DeBruijn digraphs and is a function of two parameters r and m. Pradhan and Reddy have shown that the nodeconnectivity of SRG is at least r. The same authors give a routing algorithm which generally requires 2m hops if the number of node failures is (r 1). In this paper we show that the nodeconnectivity of SRG is (2r  2). This would immediately imply that the system can tolerate up to (2r  3) node failures. We then present routing methods for situations with a certain number of node failures. When this number is (r  2) our routing algorithm requires at most m  3  logr m hops if 3  logr m m. When the number of node failures is (2r  3) our routing algorithm requires at most m  5  logr m hops if 4  logr m  m. In all the other situations our routing algorithm requires no more than 2m hops. The routing algorithms are shown to be computationally efficient.
A highspeed VLSI multiplication algorithm internally using redundant binary representation is proposed. In n bit binary integer multiplication n partial products are first generated and then added up pairwise by means of a binary tree of redundant binary adders. Since parallel addition of two ndigit redundant binary numbers can be performed in a constant time independent of n without carry propagation n bit multiplication can be performed in a time proportional to log2 n. The computation time is almost the same as that by a multiplier with a Wallace tree in which three partial products will be converted into two in contrast to our twotoone conversion and is much shorter than that by an array multiplier for longer operands. The number of computation elements of an n bit multiplier based on the algorithm is proportional to n2. It is almost the same as those of conventional ones. Furthermore since the multiplier has a regular cellular array structure similar to an array multiplier it is suitable for VLSI implementation. Thus the multiplier is excellent in both computation speed and regularity in layout. It can be implemented on a VLSI chip with an area proportional to n2 log2 n. The algorithm can be directly applied to both unsigned and 2s complement binary integer multiplication.
A cryptographic scheme for controlling access to information within a group of users organized in a hierarchy was proposed in 1. The scheme enables a user at some level to compute from his own cryptographic key the keys of the users below him in the organization.
This paper presents several new properties of D sequences that have applications to encryption and error coding. It also considers the problem of joint encryption and errorcorrection coding and proposes a solution using D sequences. The encryption operation considered is equivalent to exponentiation which forms the basis of several publickey schemes. An application of D sequences to generating events with specified probabilities is also presented.
This paper specifies procedures for defining a monitor circuit that can detect faults in microprogram sequencers. The monitor and the sequencer operate in parallel and errors are detected by comparing outputs from the monitor circuit with outputs from the sequencer. Faults that cause errors in the flow of control are detectable as well as some faults that cause errors only in the microinstruction fields. The design procedure presented for monitors consists of four parts. First a model of the program flow is constructed that only retains the information required to define a monitor. Second faults in a specified fault set are modeled by the errors they cause in the program flow model. Third the functional requirements of the monitor are specified in terms of partitions on the states of the program flow model. Fourth the logic design of the monitor is completed.
We have designed and built the Orrery a special computer for highspeed highprecision orbital mechanics computations. On the problems the Orrery was designed to solve it achieves approximately 10 Mflops in about 1 ft3 of space while consuming 150 W of power. The specialized parallel architecture of the Orrery which is well matched to orbital mechanics problems is the key to obtaining such high performance. In this paper we discuss the design construction and programming of the Orrery.
An evaluation stack used exclusively to store temporary values in expression evaluation is known to be an effective mechanism in the implementation of high level languages. This work considers the efficient management of evaluation stacks for concurrent programming languages. Techniques for sharing a single evaluation stack among many processes without copying on process switches are developed. The best strategy for managing the evaluation stack is shown to depend strongly upon the scheduling paradigm adopted by the runtime support of the language. Simulation studies driven by synthetic workloads show that the techniques described in this paper exhibit substantial performance improvements over traditional temporary storage management schemes for concurrent languages.
A systolic binary tree machine which can handle all the dictionary machine and priority queue operations such as Insert Delete ExtractMin ExtractMax Member and Near is designed in this paper. The operations can be fed into the tree machine in a pipeline manner at a constant rate and the output is correspondingly generated in a pipeline manner. Each processor in the machine stores at most one data element which consists of a key value and a record associated with the key. The machine has optimal performance since if the number of data elements present in the tree is n then each operation takes O(log n) steps. Unlike some recent designs this machine does not use any links other than the binary tree links provides optimal performance without the need to store data elements in any sorted order by exploiting dynamic rebalancing has higher throughput and keeps the logical last level of the tree on one physical level of the tree.
