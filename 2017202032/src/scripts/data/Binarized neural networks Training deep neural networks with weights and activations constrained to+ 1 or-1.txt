6
1
0
2

 
r
a

 

M
7
1

 
 
]

G
L
.
s
c
[
 
 

3
v
0
3
8
2
0

.

2
0
6
1
:
v
i
X
r
a

Binarized Neural Networks: Training Neural Networks with Weights and

Activations Constrained to +1 or −1

Matthieu Courbariaux*1
Itay Hubara*2
Daniel Soudry3
Ran El-Yaniv2
Yoshua Bengio1,4
1Universit´e de Montr´eal
2Technion - Israel Institute of Technology
3Columbia University
4CIFAR Senior Fellow
*Indicates equal contribution. Ordering determined by coin ﬂip.

MATTHIEU.COURBARIAUX@GMAIL.COM
ITAYHUBARA@GMAIL.COM
DANIEL.SOUDRY@GMAIL.COM
RANI@CS.TECHNION.AC.IL
YOSHUA.UMONTREAL@GMAIL.COM

Abstract

We introduce a method to train Binarized Neu-
ral Networks (BNNs) - neural networks with bi-
nary weights and activations at run-time. At
training-time the binary weights and activations
are used for computing the parameters gradi-
ents. During the forward pass, BNNs drastically
reduce memory size and accesses, and replace
most arithmetic operations with bit-wise opera-
tions, which is expected to substantially improve
power-efﬁciency. To validate the effectiveness of
BNNs we conduct two sets of experiments on the
Torch7 and Theano frameworks. On both, BNNs
achieved nearly state-of-the-art results over the
MNIST, CIFAR-10 and SVHN datasets. Last but
not least, we wrote a binary matrix multiplication
GPU kernel with which it is possible to run our
MNIST BNN 7 times faster than with an unopti-
mized GPU kernel, without suffering any loss in
classiﬁcation accuracy. The code for training and
running our BNNs is available on-line.

Introduction
Deep Neural Networks (DNNs) have substantially pushed
Artiﬁcial Intelligence (AI) limits in a wide range of tasks,
including but not limited to object recognition from im-
ages (Krizhevsky et al., 2012; Szegedy et al., 2014), speech
recognition (Hinton et al., 2012; Sainath et al., 2013), sta-

tistical machine translation (Devlin et al., 2014; Sutskever
et al., 2014; Bahdanau et al., 2015), Atari and Go games
(Mnih et al., 2015; Silver et al., 2016), and even abstract
art (Mordvintsev et al., 2015).
Today, DNNs are almost exclusively trained on one or
many very fast and power-hungry Graphic Processing
Units (GPUs) (Coates et al., 2013). As a result, it is of-
ten a challenge to run DNNs on target low-power devices,
and substantial research efforts are invested in speeding
up DNNs at run-time on both general-purpose (Vanhoucke
et al., 2011; Gong et al., 2014; Romero et al., 2014; Han
et al., 2015) and specialized computer hardware (Farabet
et al., 2011a;b; Pham et al., 2012; Chen et al., 2014a;b;
Esser et al., 2015).
This paper makes the following contributions:

• We introduce a method to train Binarized-Neural-
Networks (BNNs), neural networks with binary
weights and activations, at run-time, and when com-
puting the parameters gradients at train-time (see Sec-
tion 1).

• We conduct two sets of experiments, each imple-
mented on a different framework, namely Torch7
(Collobert et al., 2011) and Theano (Bergstra et al.,
2010; Bastien et al., 2012), which show that it is pos-
sible to train BNNs on MNIST, CIFAR-10 and SVHN
and achieve nearly state-of-the-art results (see Section
2).

• We show that during the forward pass (both at run-
time and train-time), BNNs drastically reduce mem-
ory consumption (size and number of accesses), and

Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1

replace most arithmetic operations with bit-wise oper-
ations, which potentially lead to a substantial increase
in power-efﬁciency (see Section 3). Moreover, a bi-
narized CNN can lead to binary convolution kernel
repetitions; We argue that dedicated hardware could
reduce the time complexity by 60% .

• Last but not least, we programed a binary matrix mul-
tiplication GPU kernel with which it is possible to run
our MNIST BNN 7 times faster than with an unopti-
mized GPU kernel, without suffering any loss in clas-
siﬁcation accuracy (see Section 4).

• The code for training and running our BNNs is avail-
able on-line (In both Theano framework 1 and Torch
framework 2).

1. Binarized Neural Networks
In this section, we detail our binarization function, show
how we use it to compute the parameters gradients, and
how we backpropagate through it.

1.1. Deterministic vs Stochastic Binarization

When training a BNN, we constrain both the weights and
the activations to either +1 or −1. Those two values are
very advantageous from a hardware perspective, as we ex-
plain in Section 4.
In order to transform the real-valued
variables into those two values, we use two different bi-
narization functions, as in (Courbariaux et al., 2015). Our
ﬁrst binarization function is deterministic:

(cid:26) +1

−1

if x ≥ 0,
otherwise,

xb = Sign(x) =

(1)

(2)

where xb is the binarized variable (weight or activation)
and x the real-valued variable. It is very straightforward to
implement and works quite well in practice. Our second
binarization function is stochastic:

(cid:26) +1 with probability p = σ(x),

xb =

−1 with probability 1 − p,

where σ is the “hard sigmoid” function:

σ(x) = clip(

x + 1

2

, 0, 1) = max(0, min(1,

x + 1

2

)). (3)

The stochastic binarization is more appealing than the sign
function, but harder to implement as it requires the hard-
ware to generate random bits when quantizing. As a re-
sult, we mostly use the deterministic binarization function
(i.e, the sign function), with the exception of activations at
train-time in some of our experiments.

1https://github.com/MatthieuCourbariaux/

BinaryNet

2https://github.com/itayhubara/BinaryNet

1.2. Gradient Computation and Accumulation

Although our BNN training method uses binary weights
and activation to compute the parameters gradients, the
real-valued gradients of the weights are accumulated in
real-valued variables, as per Algorithm 1. Real-valued
weights are likely required for Stochasic Gradient Descent
(SGD) to work at all. SGD explores the space of param-
eters in small and noisy steps, and that noise is averaged
out by the stochastic gradient contributions accumulated in
each weight. Therefore, it is important to keep sufﬁcient
resolution for these accumulators, which at ﬁrst glance sug-
gests that high precision is absolutely required.
Moreover, adding noise to weights and activations when
computing the parameters gradients provide a form of reg-
ularization that can help to generalize better, as previ-
ously shown with variational weight noise (Graves, 2011),
Dropout (Srivastava, 2013; Srivastava et al., 2014) and
DropConnect (Wan et al., 2013). Our method of training
BNNs can be seen as a variant of Dropout, in which instead
of randomly setting half of the activations to zero when
computing the parameters gradients, we binarize both the
activations and the weights.

1.3. Propagating Gradients Through Discretization

The derivative of the sign function is zero almost every-
where, making it apparently incompatible with backpropa-
gation, since the exact gradient of the cost with respect to
the quantities before the discretization (pre-activations or
weights) would be zero. Note that this remains true even
if stochastic quantization is used. Bengio (2013) studied
the question of estimating or propagating gradients through
stochastic discrete neurons. They found in their experi-
ments that the fastest training was obtained when using the
“straight-through estimator,” previously introduced in Hin-
ton (2012)’s lectures.
We follow a similar approach but use the version of
the straight-through estimator that takes into account the
saturation effect, and does use deterministic rather than
stochastic sampling of the bit. Consider the sign function
quantization

q = Sign(r),

∂q has
and assume that an estimator gq of the gradient ∂C
been obtained (with the straight-through estimator when
∂r is sim-
needed). Then, our straight-through estimator of ∂C
ply

gr = gq1|r|≤1.

(4)

Note that this preserves the gradient’s information and can-
cels the gradient when r is too large. Not cancelling the
gradient when r is too large signiﬁcantly worsens the per-
formance. The use of this straight-through estimator is il-
lustrated in Algorithm 1. The derivative 1|r|≤1 can also be

Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1

Algorithm 1 Training a BNN. C is the cost function for
minibatch, λ - the learning rate decay factor and L the num-
ber of layers. ◦ indicates element-wise multiplication. The
function Binarize() speciﬁes how to (stochastically or de-
terministically) binarize the activations and weights, and
Clip(), how to clip the weights. BatchNorm() speciﬁes how
to batch-normalize the activations, using either batch nor-
malization (Ioffe & Szegedy, 2015) or its shift-based vari-
ant we describe in Algorithm 3. BackBatchNorm() speci-
ﬁes how to backpropagate through the normalization. Up-
date() speciﬁes how to update the parameters when their
gradients are known, using either ADAM (Kingma & Ba,
2014) or the shift-based AdaMax we describe in Algorithm
4.
Require: a minibatch of inputs and targets (a0, a∗), pre-
vious weights W , previous BatchNorm parameters θ,
weights initialization coefﬁcients from (Glorot & Ben-
gio, 2010) γ, and previous learning rate η.

Ensure: updated weights W t+1, updated BatchNorm pa-

rameters θt+1 and updated learning rate ηt+1.
{1. Computing the parameters gradients:}
{1.1. Forward propagation:}
for k = 1 to L do

k ← Binarize(Wk)
W b
sk ← ab
ak ← BatchNorm(sk, θk)
if k < L then

k−1W b
k

k ← Binarize(ak)
ab
end if
end for
{1.2. Backward propagation:}
{Please note that the gradients are not binary.}
Compute gaL = ∂C
∂aL
for k = L to 1 do
if k < L then
gak ← gab

knowing aL and a∗

◦ 1|ak|≤1

← gsk W b
k
← g(cid:62)
ab
k−1

end if
(gsk , gθk ) ← BackBatchNorm(gak , sk, θk)
gab
k−1
gW b
end for
{2. Accumulating the parameters gradients:}
for k = 1 to L do

sk

k

k

k ← Update(θk, η, gθk )
θt+1
k ← Clip(Update(Wk, γkη, gW b
W t+1
ηt+1 ← λη

k

),−1, 1)

end for

Algorithm 2 Shift based Batch Normalizing Transform,
applied to activation x over a mini-batch. AP 2(x) =
sign(x) × 2round(log2|x|) is the approximate power-of-2 3,
and (cid:28)(cid:29) stands for both left and right binary shift.
Require: Values of x over a mini-batch: B = {x1...m};
Ensure: {yi = BN(xi,γ, β)}

(cid:80)m
Parameters to be learned: γ, β
(cid:80)m
i=1 xi {mini-batch mean}
µB ← 1
C(xi) ← (xi − µB) {centered input}
i=1(C(xi)(cid:28)(cid:29)AP 2(C(xi))){apx variance}
B ← 1
σ2
B + )−1) {normalize}
yi ← AP 2(γ) (cid:28)(cid:29) ˆxi {scale and shift}

ˆxi ← C(xi) (cid:28)(cid:29) AP 2(((cid:112)σ2

m

m

Algorithm 3 Shift based Batch Normalizing Transform,
applied to activation (x) over a mini-batch. Where AP2 is
the approximate power-of-2 and (cid:28)(cid:29) stands for both left
and right binary shift.
Require: Values of x over a mini-batch: B = {x1...m};
Ensure: {yi = BN(xi,γ, β)}

(cid:80)m
Parameters to be learned: γ, β
(cid:80)m
µB ← 1
i=1 xi {mini-batch mean}
C(xi) ← (xi − µB) {centered input}
i=1(C(xi)(cid:28)(cid:29)AP 2(C(xi))){apx variance}
B ← 1
σ2
B + )−1) {normalize}
yi ← AP 2(γ) (cid:28)(cid:29) ˆxi {scale and shift}

ˆxi ← C(xi) (cid:28)(cid:29) AP 2(((cid:112)σ2

m

m

seen as propagating the gradient through hard tanh, which
is the following piece-wise linear activation function:
Htanh(x) = Clip(x,−1, 1) = max(−1, min(1, x)). (5)

For hidden units, we use the sign function non-linearity to
obtain binary activations, and for weights we combine two
ingredients:

• Constrain each real-valued weight between -1 and 1,
by projecting wr to -1 or 1 when the weight update
brings wr outside of [−1, 1], i.e., clipping the weights
during training, as per Algorithm 1. The real-valued
weights would otherwise grow very large without any
impact on the binary weights.

• When using a weight wr, quantize it using wb =

Sign(wr).

This is consistent with the gradient canceling when |wr| >
1, according to Eq. 4.

3Hardware implementation of AP2 is as simple as extracting
the index of the most signiﬁcant bit from the number’s binary
representation.

Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1

Algorithm 4 Shift based AdaMax learning rule (Kingma
t indicates the element-wise square gt ◦ gt.
& Ba, 2014). g2
Good default settings are α = 2−10, 1−β1 = 2−3, 1−β2 =
2−10 . All operations on vectors are element-wise. With βt
and βt
Require: Previous parameters θt−1 and their gradient gt,

2 we denote β1 and β2 to the power t.

1

Ensure: Updated parameters θt

and learning rate α.
{Biased 1st and 2nd raw moment estimates:}
mt ← β1 · mt−1 + (1 − β1) · gt
vt ← max(β2 · vt−1,|gt|)
{Updated parameters:}
θt ← θt−1 − (α (cid:28)(cid:29) (1 − β1)) · ˆm (cid:28)(cid:29) v−1

t

Algorithm 5 Running a BNN. L is the number of layers.
Require: a vector of 8-bit inputs a0, the binary weights

Ensure: the MLP output aL.

W b, and the BatchNorm parameters θ.
{1. First layer:}
a1 ← 0
for n = 1 to 8 do

a1 ← a1 + 2n−1 × XnorDotProduct(an
end for
1 ← Sign(BatchNorm(a1, θ1))
ab
{2. Remaining hidden layers:}
for k = 2 to L − 1 do
ak ← XnorDotProduct(ab
k−1, W b
k )
k ← Sign(BatchNorm(ak, θk))
ab

end for
{3. Output layer:}
aL ← XnorDotProduct(ab
aL ← BatchNorm(aL, θL)

L−1, W b
L)

0, Wb
1 )

1.4. Shift based Batch Normalization

Batch Normalization (BN) (Ioffe & Szegedy, 2015), accel-
erates the training and also seems to reduces the overall
impact of the weights’ scale. The normalization noise may
also help to regularize the model. However, at train-time,
BN requires many multiplications (calculating the standard
deviation and dividing by it), namely, dividing by the run-
ning variance (the weighted mean of the training set acti-
vation variance). Although the number of scaling calcula-
tions is the same as the number of neurons, in the case of
ConvNets this number is quite large. For example, in the
CIFAR-10 dataset (using our architecture), the ﬁrst convo-
lution layer, consisting of only 128×3×3 ﬁlter masks, con-
verts an image of size 3× 32× 32 to size 3× 128× 28× 28,
which is two orders of magnitude larger than the number of
weights. To achieve the results that BN would obtain, we
use a shift-based batch normalization (SBN) technique. de-
tailed in Algorithm 3. SBN approximates BN almost with-
out multiplications.
In the experiment we conducted we

)

1.6. First Layer

did not observe accuracy loss when using the shift based
BN algorithm instead of the vanilla BN algorithm.

1.5. Shift based AdaMax

The ADAM learning rule (Kingma & Ba, 2014) also seems
to reduce the impact of the weight scale. Since ADAM re-
quires many multiplications, we suggest using instead the
shift-based AdaMax we detail in Algorithm 4. In the ex-
periment we conducted we did not observe accuracy loss
when using the shift-based AdaMax algorithm instead of
the vanilla ADAM algorithm.

In a BNN, only the binarized values of the weights and ac-
tivations are used in all calculations. As the output of one
layer is the input of the next, all the layers inputs are bi-
nary, with the exception of the ﬁrst layer. However, we
do not believe this to be a major issue. First, in computer
vision, the input representation typically has much fewer
channels (e.g, Red, Green and Blue) than internal repre-
sentations (e.g, 512). As a result, the ﬁrst layer of a Con-
vNet is often the smallest convolution layer, both in terms
of parameters and computations (Szegedy et al., 2014).
Second, it is relatively easy to handle continuous-valued
inputs as ﬁxed point numbers, with m bits of precision. For
example, in the common case of 8-bit ﬁxed point inputs:

s = x · wb

8(cid:88)

s =

2n−1(xn · wb),

(6)

(7)

n=1

where x is a vector of 1024 8-bit inputs, x8
1 is the most
signiﬁcant bit of the ﬁrst input, wb is a vector of 1024 1-bit
weights, and s is the resulting weighted sum. This trick is
used in Algorithm 5.

2. Benchmark Results
We conduct two sets of experiments, each based on a differ-
ent framework, namely Torch7 (Collobert et al., 2011) and
Theano (Bergstra et al., 2010; Bastien et al., 2012). Other
than the framework, the two sets of experiments are very
similar:

• In both sets of experiments, we obtain near state-of-
the-art results with BNNs on MNIST, CIFAR-10 and
the SVHN benchmark datasets.

• In our Torch7 experiments, the activations are stochas-
tically binarized at train-time, whereas in our Theano
experiments they are deterministically binarized.

• In our Torch7 experiments, we use the shift-based BN

Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1

Table 1. Classiﬁcation test error rates of DNNs trained on MNIST (MLP architecture without unsupervised pretraining), CIFAR-10
(without data augmentation) and SVHN.

Data set

MNIST

SVHN CIFAR-10

Binarized activations+weights, during training and test

BNN (Torch7)
BNN (Theano)
Committee Machines’ Array (Baldassi et al., 2015)

1.40%
0.96%
1.35%

2.53%
2.80%

-

BinaryConnect (Courbariaux et al., 2015)

Binarized weights, during training and test

1.29± 0.08% 2.30%

EBP (Cheng et al., 2015)
Bitwise DNNs (Kim & Smaragdis, 2016)

Binarized activations+weights, during test
2.2± 0.1%

1.33%

Ternary weights, binary activations, during test

(Hwang & Sung, 2014)

-
-

-

10.15%
11.40%

-

9.90%

-
-

-

1.45%
No binarization (standard results)
0.94%

Maxout Networks (Goodfellow et al.)
Network in Network (Lin et al.)
Gated pooling (Lee et al., 2015)

-
-

2.47%
2.35%
1.69%

11.68%
10.41%
7.62%

Figure 1. Training curves of a ConvNet on CIFAR-10 depend-
ing on the method. The dotted lines represent the training costs
(square hinge losses) and the continuous lines the corresponding
validation error rates. Although BNNs are slower to train, they
are nearly as accurate as 32-bit ﬂoat DNNs.

Figure 2. Binary weight ﬁlters, sampled from of the ﬁrst convolu-
tion layer. Since we have only 2k2 unique 2D ﬁlters (where k is
the ﬁlter size), ﬁlter replication is very common. For instance, on
our CIFAR-10 ConvNet, only 42% of the ﬁlters are unique.

and AdaMax variants, which are detailed in Algo-
rithms 3 and 4, whereas in our Theano experiments,
we use vanilla BN and ADAM.

2.1. MLP on MNIST (Theano)

MNIST is an image classiﬁcation benchmark dataset (Le-
Cun et al., 1998). It consists of a training set of 60K and
a test set of 10K 28 × 28 gray-scale images represent-
ing digits ranging from 0 to 9.
In order for this bench-
mark to remain a challenge, we did not use any convo-
lution, data-augmentation, preprocessing or unsupervised
learning. The MLP we train on MNIST consists of 3 hid-
den layers of 4096 binary units (see Section 1) and a L2-
SVM output layer; L2-SVM has been shown to perform
better than Softmax on several classiﬁcation benchmarks

(Tang, 2013; Lee et al., 2014). We regularize the model
with Dropout (Srivastava, 2013; Srivastava et al., 2014).
The square hinge loss is minimized with the ADAM adap-
tive learning rate method (Kingma & Ba, 2014). We use
an exponentially decaying global learning rate, as per Al-
gorithm 1, and also scale the learning rates of the weights
with their initialization coefﬁcients from (Glorot & Bengio,
2010), as suggested by Courbariaux et al. (2015). We use
Batch Normalization with a minibatch of size 100 to speed
up the training. As is typical, we use the last 10K samples
of the training set as a validation set for early stopping and
model selection. We report the test error rate associated
with the best validation error rate after 1000 epochs (we do
not retrain on the validation set). The results are reported
in Table 1.

2.2. MLP on MNIST (Torch7)

We use a similar architecture as in our Theano experiments,
without dropout, and with 2048 binary units per layer in-
stead of 4096. Additionally, we use the shift base AdaMax

Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1

and BN (with a minibatch of size 100) instead of the vanilla
implementations, to reduce the number of multiplications.
Likewise, we decay the learning rate by using a 1-bit right
shift every 10 epochs. The results are presented in Table 1.

2.3. ConvNet on CIFAR-10 (Theano)

CIFAR-10 is an image classiﬁcation benchmark dataset. It
consists of a training set of size 50K and a test set of size
10K, where instance are 32 × 32 color images represent-
ing airplanes, automobiles, birds, cats, deer, dogs, frogs,
horses, ships and trucks. We do not use any preprocessing
or data-augmentation (which can really be a game changer
for this dataset (Graham, 2014)). The architecture of our
ConvNet is the same architecture as ?’s except for the bi-
narization of the activations. Courbariaux et al. (2015)’s
architecture is itself mainly inspired by VGG (Simonyan
& Zisserman, 2015). The square hinge loss is minimized
with ADAM. We use an exponentially decaying learning
rate, as we did for MNIST. We scale the learning rates of
the weights with their initialization coefﬁcients from (Glo-
rot & Bengio, 2010). We use Batch Normalization with a
minibatch of size 50 to speed up the training. We use the
last 5000 samples of the training set as a validation set. We
report the test error rate associated with the best validation
error rate after 500 training epochs (we do not retrain on
the validation set). The results are presented in Table 1 and
Figure 1.

2.4. ConvNet on CIFAR-10 (Torch7)

We use the same architecture as in our Theano experiments.
We apply shift-based AdaMax and BN (with a minibatch
of size 200) instead of the vanilla implementations to re-
duce the number of multiplications. Likewise, we decay
the learning rate by using a 1-bit right shift every 50 epochs.
The results are presented in Table 1 and Figure 1.

2.5. ConvNet on SVHN

SVHN is also an image classiﬁcation benchmark dataset. It
consists of a training set of size 604K examples and a test
set of size 26K, where instances are 32 × 32 color images
representing digits ranging from 0 to 9.
In both sets of
experiments, we follow the same procedure used for the
CIFAR-10 experiments, with a few notable exceptions: we
use half the number of units in the convolution layers, and
we train for 200 epochs instead of 500 (because SVHN is a
much larger dataset than CIFAR-10). The results are given
in Table 1.

3. Very Power Efﬁcient in Forward Pass
Computer hardware, be it general-purpose or specialized,
is composed of memories, arithmetic operators and control

Table 2. Energy
(Horowitz, 2014)

consumption

of multiply-accumulations

Operation
8bit Integer
32bit Integer
16bit Floating Point
32tbit Floating Point

MUL ADD
0.03pJ
0.2pJ
0.1pJ
3.1pJ
0.4pJ
1.1pJ
3.7pJ
0.9pJ

Table 3. Energy consumption of memory accesses (Horowitz,
2014)

Memory size
8K
32K
1M
DRAM

64-bit memory access

10pJ
20pJ
100pJ

1.3-2.6nJ

logic. During the forward pass (both at run-time and train-
time), BNNs drastically reduce memory size and accesses,
and replace most arithmetic operations with bit-wise op-
erations, which might lead to a great increase in power-
efﬁciency. Moreover, a binarized CNN can lead to binary
convolution kernel repetitions, and we argue that dedicated
hardware could reduce the time complexity by 60% .

3.1. Memory Size and Accesses

Improving computing performance has always been and re-
mains a challenge. Over the last decade, power has been the
main constraint on performance (Horowitz, 2014). This is
why much research effort has been devoted to reducing the
energy consumption of neural networks. Horowitz (2014)
provides rough numbers for the computations’ energy con-
sumption (the given numbers are for 45nm technology) as
summarized in Tables 2 and 3.
Importantly, we can see
that memory accesses typically consume more energy than
arithmetic operations, and memory access’ cost augments
with memory size. In comparison with 32-bit DNNs, BNNs
require 32 times smaller memory size and 32 times fewer
memory accesses. This is expected to reduce energy con-
sumption drastically (i.e., more than 32 times).

3.2. XNOR-Count

Applying a DNN mainly consists of convolutions and ma-
trix multiplications. The key arithmetic operation of deep
learning is thus the multiply-accumulate operation. Artiﬁ-
cial neurons are basically multiply-accumulators comput-
ing weighted sums of their inputs. In BNNs, both the ac-
tivations and the weights are constrained to either −1 or
+1. As a result, most of the 32-bit ﬂoating point multiply-
accumulations are replaced by 1-bit XNOR-count opera-
tions. This could have a big impact on deep learning ded-
icated hardware. For instance, a 32-bit ﬂoating point mul-
tiplier costs about 200 Xilinx FPGA slices (Govindu et al.,
2004; Beauchamp et al., 2006), whereas a 1-bit XNOR gate

Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1

only costs a single slice.

3.3. Exploiting Filter Repetitions

When using a ConvNet architecture with binary weights,
the number of unique ﬁlters is bounded by the ﬁlter size.
For example, in our implementation we use ﬁlters of size
3 × 3, so the maximum number of unique 2D ﬁlters is
29 = 512. However, this should not prevent expanding
the number of feature maps beyond this number, since the
actual ﬁlter is a 3D matrix. Assuming we have M(cid:96) ﬁl-
ters in the (cid:96) convolutional layer, we have to store a 4D
weight matrix of size M(cid:96) × M(cid:96)−1 × k × k. Consequently,
the number of unique ﬁlters is 2k2M(cid:96)−1. When necessary,
we apply each ﬁlter on the map and perform the required
multiply-accumulate (MAC) operations (in our case, using
XNOR and popcount operations). Since we now have bi-
nary ﬁlters, many 2D ﬁlters of size k×k repeat themselves.
By using dedicated hardware/software, we can apply only
the unique 2D ﬁlters on each feature map and sum the re-
sult wisely to receive each 3D ﬁlter’s convolutional result.
Note that an inverse ﬁlter (i.e., [-1,1,-1] is the inverse of
[1,-1,1]) can also be treated as a repetition; it is merely a
multiplication of the original ﬁlter by -1. For example, in
our ConvNet architecture trained on the CIFAR-10 bench-
mark, there are only 42% unique ﬁlters per layer on av-
erage. Hence we can reduce the number of the XNOR-
popcount operations by 3.

4. Seven Times Faster on GPU at Run-Time
It is possible to speed up GPU implementations of BNNs,
by using a method sometimes called SIMD (single in-
struction, multiple data) within a register (SWAR). The
basic idea of SWAR is to concatenate groups of 32 bi-
nary variables into 32-bit registers, and thus obtain a 32-
times speed-up on bitwise operations (e.g, XNOR). Using
SWAR, it is possible to evaluate 32 connections with only
3 instructions:

a1+ = popcount(xnor(a32b

0

, w32b

1

)),

(8)

1

0

and w32b
where a1 is the resulting weighted sum, and a32b
are the concatenated inputs and weights. Those 3 instruc-
tions (accumulation, popcount, xnor) take 1 + 4 + 1 = 6
clock cycles on recent Nvidia GPUs (and if they were to be-
come a fused instruction, it would only take a single clock
cycle). Consequently, we obtain a theoretical Nvidia GPU
speed-up of factor of 32/6 ≈ 5.3. In practice, this speed-up
is quite easy to obtain as the memory bandwidth to compu-
tation ratio is also increased by 6 times.
In order to validate those theoretical results, we programed
two GPU kernels:

• The ﬁrst kernel (baseline) is a quite unoptimized ma-

Figure 3. The ﬁrst three columns represent the time it takes to
perform a 8192 × 8192 × 8192 (binary) matrix multiplication on
a GTX750 Nvidia GPU, depending on which kernel is used. We
can see that our XNOR kernel is 23 times faster than our baseline
kernel and 3.4 times faster than cuBLAS. The next three columns
represent the time it takes to run the MLP from Section 2 on the
full MNIST test set. As MNIST’s images are not binary, the ﬁrst
layer’s computations are always performed by the baseline ker-
nel. The last three columns show that the MLP accuracy does not
depend on which kernel is used.

trix multiplication kernel.

• The second kernel (XNOR) is nearly identical to the
baseline kernel, except that it uses the SWAR method,
as in Equation (8).

The two GPU kernels return identical outputs when their
inputs are constrained to −1 or +1 (but not otherwise). The
XNOR kernel is about 23 times faster than the baseline ker-
nel and 3.4 times faster than cuBLAS, as shown in Figure
3. Last but not least, the MLP from Section 2 runs 7 times
faster with the XNOR kernel than with the baseline kernel,
without suffering any loss in classiﬁcation accuracy (see
Figure 3).

5. Discussion and Related Work
Until recently, the use of extremely low-precision networks
(binary in the extreme case) was believed to be highly de-
structive to the network performance (Courbariaux et al.,
2014). Soudry et al. (2014); ? showed the contrary by
showing that good performance could be achieved even if
all neurons and weights are binarized to ±1 . This was
done using Expectation BackPropagation (EBP), a varia-
tional Bayesian approach, which infers networks with bi-

Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1

nary weights and neurons by updating the posterior distri-
butions over the weights. These distributions are updated
by differentiating their parameters (e.g., mean values) via
the back propagation (BP) algorithm. Esser et al. (2015)
implemented a fully binary network at run time using a very
similar approach to EBP, showing signiﬁcant improvement
in energy efﬁciency. The drawback of EBP is that the bina-
rized parameters were only used during inference.
The probabilistic idea behind EBP was extended in the Bi-
naryConnect algorithm of Courbariaux et al. (2015).
In
BinaryConnect, the real-valued version of the weights is
saved and used as a key reference for the binarization pro-
cess. The binarization noise is independent between dif-
ferent weights, either by construction (by using stochas-
tic quantization) or by assumption (a common simpliﬁca-
tion; see Spang (1962). The noise would have little effect
on the next neuron’s input because the input is a summa-
tion over many weighted neurons. Thus, the real-valued
version could be updated by the back propagated error by
simply ignoring the binarization noise in the update. Us-
ing this method, Courbariaux et al. (2015) were the ﬁrst
to binarize weights in CNNs and achieved near state-of-
the-art performance on several datasets. They also argued
that noisy weights provide a form of regularization, which
could help to improve generalization, as previously shown
in (Wan et al., 2013). This method binarized weights while
still maintaining full precision neurons.
Lin et al. (2015) carried over the work of Courbariaux et al.
(2015) to the back-propagation process by quantizing the
representations at each layer of the network, to convert
some of the remaining multiplications into binary shifts by
restricting the neurons values of power-of-two integers. Lin
et al. (2015)’s work and ours seem to share similar charac-
teristics . However, their approach continues to use full pre-
cision weights during the test phase. Moreover, Lin et al.
(2015) quantize the neurons only during the back propaga-
tion process, and not during forward propagation.
Other research (Baldassi et al., 2015) showed that fully bi-
nary training and testing is possible in an array of com-
mittee machines with randomized input, where only one
weight layer is being adjusted.
and Gong
et al. aimed to compress a fully trained high precision net-
work by using a quantization or matrix factorization meth-
ods. These methods required training the network with full
precision weights and neurons, thus requiring numerous
MAC operations avoided by the proposed BNN algorithm.
Hwang & Sung (2014) focused on a ﬁxed-point neural net-
work design and achieved performance almost identical to
that of the ﬂoating-point architecture. Kim et al. (2014)
provided evidence that DNNs with ternary weights, used
on a dedicated circuit, consume very low power and can
be operated with only on-chip memory, at run time. Sung

Judd et al.

et al. also indicated satisfactory empirical performance of
neural networks with 8-bit precision. Kim & Paris (2015)
retrained neural networks with binary weights and activa-
tions.
So far, to the best of our knowledge, no work has succeeded
in binarizing weights and neurons, at the inference phase
and the entire training phase of a deep network. This was
achieved in the present work. We relied on the idea that bi-
narization can be done stochastically, or be approximated
as random noise. This was previously done for the weights
by Courbariaux et al. (2015), but our BNNs extend this to
the activations. Note that the binary activations are espe-
cially important for ConvNets, where there are typically
many more neurons than free weights. This allows highly
efﬁcient operation of the binarized DNN at run time, and
at the forward propagation phase during training. More-
over, our training method has almost no multiplications,
and therefore might be implemented efﬁciently in dedi-
cated hardware. However, we have to save the value of the
full precision weights. This is a remaining computational
bottleneck during training, since it requires relatively high
energy resources. Novel memory devices might be used to
alleviate this issue in the future; see e.g. (Soudry et al.).

Conclusion
We have introduced BNNs, DNNs with binary weights and
activations at run-time and when computing the parame-
ters gradients at train-time (see Section 1). We have con-
ducted two sets of experiments on two different frame-
works, Torch7 and Theano, which show that it is possible to
train BNNs on MNIST, CIFAR-10 and SVHN, and achieve
nearly state-of-the-art results (see Section 2). Moreover,
during the forward pass (both at run-time and train-time),
BNNs drastically reduce memory size and accesses, and re-
place most arithmetic operations with bit-wise operations,
which might lead to a great increase in power-efﬁciency
(see Section 3). Last but not least, we programed a binary
matrix multiplication GPU kernel with which it is possible
to run our MNIST MLP 7 times faster than with an unopti-
mized GPU kernel, without suffering any loss in classiﬁca-
tion accuracy (see Section 4). Future works should explore
how to extend the speed-up to train-time (e.g., by binariz-
ing some gradients), and also extend benchmark results to
other models (e.g, RNN) and datasets (e.g, ImageNet).

Acknowledgments
We would like to express our appreciation to Elad Hoffer,
for his technical assistance and constructive comments. We
thank our fellow MILA lab members who took the time to
read the article and give us some feedback. We thank the
developers of Torch, (Collobert et al., 2011) a Lua based

Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1

environment, and Theano (Bergstra et al., 2010; Bastien
et al., 2012), a Python library which allowed us to easily
develop a fast and optimized code for GPU. We also thank
the developers of Pylearn2 (Goodfellow et al., 2013) and
Lasagne (Dieleman et al., 2015), two Deep Learning li-
braries built on the top of Theano. We thank Yuxin Wu
for helping us compare our GPU kernels with cuBLAS. We
are also grateful for funding from CIFAR, NSERC, IBM,
Samsung, and the Israel Science Foundation (ISF).

References
Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neu-
ral machine translation by jointly learning to align and trans-
late. In ICLR’2015, arXiv:1409.0473, 2015.

Baldassi, Carlo, Ingrosso, Alessandro, Lucibello, Carlo, Saglietti,
Luca, and Zecchina, Riccardo. Subdominant Dense Clusters
Allow for Simple Learning and High Computational Perfor-
mance in Neural Networks with Discrete Synapses. Physical
Review Letters, 115(12):1–5, 2015.

Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Bergstra,
James, Goodfellow, Ian J., Bergeron, Arnaud, Bouchard, Nico-
las, and Bengio, Yoshua. Theano: new features and speed im-
provements. Deep Learning and Unsupervised Feature Learn-
ing NIPS 2012 Workshop, 2012.

Beauchamp, Michael J, Hauck, Scott, Underwood, Keith D, and
Hemmert, K Scott. Embedded ﬂoating-point units in FPGAs.
In Proceedings of the 2006 ACM/SIGDA 14th international
symposium on Field programmable gate arrays, pp. 12–20.
ACM, 2006.

Bengio, Yoshua. Estimating or propagating gradients through
stochastic neurons. Technical Report arXiv:1305.2982, Uni-
versite de Montreal, 2013.

Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lam-
blin, Pascal, Pascanu, Razvan, Desjardins, Guillaume, Turian,
Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a
CPU and GPU math expression compiler. In Proceedings of
the Python for Scientiﬁc Computing Conference (SciPy), June
2010. Oral Presentation.

Chen, Tianshi, Du, Zidong, Sun, Ninghui, Wang, Jia, Wu,
Chengyong, Chen, Yunji, and Temam, Olivier. Diannao:
A small-footprint high-throughput accelerator for ubiquitous
In Proceedings of the 19th international
machine-learning.
conference on Architectural support for programming lan-
guages and operating systems, pp. 269–284. ACM, 2014a.

Chen, Yunji, Luo, Tao, Liu, Shaoli, Zhang, Shijin, He, Liqiang,
Wang, Jia, Li, Ling, Chen, Tianshi, Xu, Zhiwei, Sun, Ninghui,
et al. Dadiannao: A machine-learning supercomputer. In Mi-
croarchitecture (MICRO), 2014 47th Annual IEEE/ACM Inter-
national Symposium on, pp. 609–622. IEEE, 2014b.

Cheng, Zhiyong, Soudry, Daniel, Mao, Zexi, and Lan, Zhen-
zhong. Training binary multilayer neural networks for image
classiﬁcation using expectation backpropgation. arXiv preprint
arXiv:1503.03562, 2015.

Coates, Adam, Huval, Brody, Wang, Tao, Wu, David, Catanzaro,
Bryan, and Andrew, Ng. Deep learning with COTS HPC sys-
tems. In Proceedings of the 30th international conference on
machine learning, pp. 1337–1345, 2013.

Collobert, Ronan, Kavukcuoglu, Koray, and Farabet, Cl´ement.
Torch7: A matlab-like environment for machine learning. In
BigLearn, NIPS Workshop, 2011.

Courbariaux, Matthieu, Bengio, Yoshua, and David, Jean-Pierre.
Training deep neural networks with low precision multiplica-
tions. ArXiv e-prints, abs/1412.7024, December 2014.

Courbariaux, Matthieu, Bengio, Yoshua, and David, Jean-Pierre.
Binaryconnect: Training deep neural networks with binary
weights during propagations. ArXiv e-prints, abs/1511.00363,
November 2015.

Devlin, Jacob, Zbib, Rabih, Huang, Zhongqiang, Lamar, Thomas,
Schwartz, Richard, and Makhoul, John. Fast and robust neu-
ral network joint models for statistical machine translation. In
Proc. ACL’2014, 2014.

Dieleman, Sander, Schlter, Jan, Raffel, Colin, Olson, Eben,
Snderby, Sren Kaae, Nouri, Daniel, Maturana, Daniel, Thoma,
Martin, Battenberg, Eric, Kelly, Jack, Fauw, Jeffrey De, Heil-
man, Michael, diogo149, McFee, Brian, Weideman, Hendrik,
takacsg84, peterderivaz, Jon, instagibbs, Rasul, Dr. Kashif,
CongLiu, Britefury, and Degrave, Jonas. Lasagne: First re-
lease., August 2015.

Esser, Steve K, Appuswamy, Rathinakumar, Merolla, Paul,
Arthur, John V, and Modha, Dharmendra S. Backpropagation
for energy-efﬁcient neuromorphic computing. In Advances in
Neural Information Processing Systems, pp. 1117–1125, 2015.

Farabet, Cl´ement, LeCun, Yann, Kavukcuoglu, Koray, Culur-
ciello, Eugenio, Martini, Berin, Akselrod, Polina, and Talay,
Selcuk. Large-scale FPGA-based convolutional networks. Ma-
chine Learning on Very Large Data Sets, 1, 2011a.

Farabet, Cl´ement, Martini, Berin, Corda, Benoit, Akselrod,
Polina, Culurciello, Eugenio, and LeCun, Yann. Neuﬂow: A
runtime reconﬁgurable dataﬂow processor for vision. In Com-
puter Vision and Pattern Recognition Workshops (CVPRW),
2011 IEEE Computer Society Conference on, pp. 109–116.
IEEE, 2011b.

Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁ-
In AIS-

culty of training deep feedforward neural networks.
TATS’2010, 2010.

Gong, Yunchao, Liu, Liu, Yang, Ming, and Bourdev, Lubomir.
Compressing Deep Convolutional Networks using Vector
Quantization. pp. 1–10.

Gong, Yunchao, Liu, Liu, Yang, Ming, and Bourdev, Lubomir.
Compressing deep convolutional networks using vector quan-
tization. arXiv preprint arXiv:1412.6115, 2014.

Goodfellow,

Ian J., Warde-Farley, David, Mirza, Mehdi,
Courville, Aaron, and Bengio, Yoshua. Maxout Networks.
arXiv preprint, pp. 1319–1327.

Goodfellow,

Ian J., Warde-Farley, David, Lamblin, Pas-
cal, Dumoulin, Vincent, Mirza, Mehdi, Pascanu, Razvan,
Bergstra, James, Bastien, Fr´ed´eric, and Bengio, Yoshua.
Pylearn2: a machine learning research library. arXiv preprint
arXiv:1308.4214, 2013.

Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1

Govindu, Gokul, Zhuo, Ling, Choi, Seonil, and Prasanna, Vik-
tor. Analysis of high-performance ﬂoating-point arithmetic on
In Parallel and Distributed Processing Symposium,
FPGAs.
2004. Proceedings. 18th International, pp. 149. IEEE, 2004.

Graham, Benjamin. Spatially-sparse convolutional neural net-

works. arXiv preprint arXiv:1409.6070, 2014.

Graves, Alex. Practical variational inference for neural networks.
In Advances in Neural Information Processing Systems, pp.
2348–2356, 2011.

Han, Song, Pool, Jeff, Tran, John, and Dally, William. Learn-
ing both weights and connections for efﬁcient neural network.
In Advances in Neural Information Processing Systems, pp.
1135–1143, 2015.

Hinton, Geoffrey. Neural networks for machine learning. Cours-

era, video lectures, 2012.

Hinton, Geoffrey, Deng, Li, Dahl, George E., Mohamed, Abdel-
rahman, Jaitly, Navdeep, Senior, Andrew, Vanhoucke, Vincent,
Nguyen, Patrick, Sainath, Tara, and Kingsbury, Brian. Deep
neural networks for acoustic modeling in speech recognition.
IEEE Signal Processing Magazine, 29(6):82–97, Nov. 2012.

Horowitz, Mark. Computing’s Energy Problem (and what we can
do about it). IEEE Interational Solid State Circuits Conference,
pp. 10–14, 2014.

Hwang, Kyuyeon and Sung, Wonyong. Fixed-point feedforward
deep neural network design using weights+ 1, 0, and- 1.
In
Signal Processing Systems (SiPS), 2014 IEEE Workshop on,
pp. 1–6. IEEE, 2014.

Ioffe, Sergey and Szegedy, Christian. Batch normalization: Ac-
celerating deep network training by reducing internal covariate
shift. 2015.

Judd, Patrick, Albericio, Jorge, Hetherington, Tayler, Aamodt,
Tor, Jerger, Natalie Enright, Urtasun, Raquel, and Moshovos,
Andreas. Reduced-Precision Strategies for Bounded Memory
in Deep Neural Nets. pp. 12.

Kim, Jonghong, Hwang, Kyuyeon, and Sung, Wonyong. X1000
real-time phoneme recognition vlsi using feed-forward deep
neural networks. In Acoustics, Speech and Signal Processing
(ICASSP), 2014 IEEE International Conference on, pp. 7510–
7514. IEEE, 2014.

Kim, M. and Smaragdis, P. Bitwise Neural Networks. ArXiv e-

prints, January 2016.

Kim, Minje and Paris, Smaragdis. Bitwise Neural Networks.
ICML Workshop on Resource-Efﬁcient Machine Learning, 37,
2015.

Kingma, Diederik and Ba, Jimmy. Adam: A method for stochas-

tic optimization. arXiv preprint arXiv:1412.6980, 2014.

Krizhevsky, A., Sutskever, I., and Hinton, G. ImageNet classiﬁca-
tion with deep convolutional neural networks. In NIPS’2012.
2012.

Lee, Chen-Yu, Xie, Saining, Gallagher, Patrick, Zhang,
Zhengyou, and Tu, Zhuowen. Deeply-supervised nets. arXiv
preprint arXiv:1409.5185, 2014.

Lee, Chen-Yu, Gallagher, Patrick W, and Tu, Zhuowen. Gen-
eralizing pooling functions in convolutional neural networks:
arXiv preprint arXiv:1509.08985,
Mixed, gated, and tree.
2015.

Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network In Net-

work. arXiv preprint, pp. 10.

Lin, Zhouhan, Courbariaux, Matthieu, Memisevic, Roland, and
Bengio, Yoshua. Neural networks with few multiplications.
ArXiv e-prints, abs/1510.03009, October 2015.

Mnih, Volodymyr, Kavukcuoglo, Koray, Silver, David, Rusu, An-
drei A., Veness, Joel, Bellemare, Marc G., Graves, Alex, Ried-
miller, Martin, Fidgeland, Andreas K., Ostrovski, Georg, Pe-
tersen, Stig, Beattie, Charles, Sadik, Amir, Antonoglou, Ioan-
nis, King, Helen, Kumaran, Dharsan, Wierstra, Daan, Legg,
Shane, and Hassabis, Demis. Human-level control through
deep reinforcement learning. Nature, 518:529–533, 2015.

Mordvintsev, Alexander, Olah, Christopher, and Tyka, Mike. In-
ceptionism: Going deeper into neural networks, 2015. Ac-
cessed: 2015-06-30.

Pham, Phi-Hung, Jelaca, Darko, Farabet, Clement, Martini,
Berin, LeCun, Yann, and Culurciello, Eugenio. Neuﬂow:
dataﬂow vision processing system-on-a-chip. In Circuits and
Systems (MWSCAS), 2012 IEEE 55th International Midwest
Symposium on, pp. 1044–1047. IEEE, 2012.

Romero, Adriana, Ballas, Nicolas, Kahou, Samira Ebrahimi,
Chassang, Antoine, Gatta, Carlo, and Bengio, Yoshua. Fit-
nets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550,
2014.

Sainath, Tara, rahman Mohamed, Abdel, Kingsbury, Brian, and
Ramabhadran, Bhuvana. Deep convolutional neural networks
for LVCSR. In ICASSP 2013, 2013.

Silver, David, Huang, Aja, Maddison, Chris J., Guez, Arthur,
Sifre, Laurent, van den Driessche, George, Schrittwieser,
Julian, Antonoglou, Ioannis, Panneershelvam, Veda, Lanc-
tot, Marc, Dieleman, Sander, Grewe, Dominik, Nham, John,
Kalchbrenner, Nal, Sutskever, Ilya, Lillicrap, Timothy, Leach,
Madeleine, Kavukcuoglu, Koray, Graepel, Thore, and Hass-
abis, Demis. Mastering the game of go with deep neural net-
works and tree search. Nature, 529(7587):484–489, Jan 2016.
Article.

Simonyan, Karen and Zisserman, Andrew. Very deep convolu-
In ICLR,

tional networks for large-scale image recognition.
2015.

Soudry, Daniel, Di Castro, Dotan, Gal, Asaf, Kolodny, Avinoam,
and Kvatinsky, Shahar. Memristor-Based Multilayer Neu-
IEEE
ral Networks With Online Gradient Descent Training.
Transactions on Neural Networks and Learning Systems, (10):
2408–2421.

LeCun, Yann, Bottou, Leon, Bengio, Yoshua, and Haffner,
Patrick. Gradient-based learning applied to document recogni-
tion. Proceedings of the IEEE, 86(11):2278–2324, November
1998.

Soudry, Daniel, Hubara, Itay, and Meir, Ron. Expectation back-
propagation: Parameter-free training of multilayer neural net-
In NIPS’2014,
works with continuous or discrete weights.
2014.

Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1

Srivastava, Nitish. Improving neural networks with dropout. Mas-

ter’s thesis, U. Toronto, 2013.

Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever,
Ilya, and Salakhutdinov, Ruslan. Dropout: A simple way to
prevent neural networks from overﬁtting. Journal of Machine
Learning Research, 15:1929–1958, 2014.

Sung, Wonyong, Shin, Sungho, and Hwang, Kyuyeon. Resiliency

of Deep Neural Networks under Quantization. (2014):1–9.

Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence to
sequence learning with neural networks. In NIPS’2014, 2014.

Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre,
Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Van-
houcke, Vincent, and Rabinovich, Andrew. Going deeper with
convolutions. Technical report, arXiv:1409.4842, 2014.

Tang, Yichuan. Deep learning using linear support vector ma-
chines. Workshop on Challenges in Representation Learning,
ICML, 2013.

Vanhoucke, Vincent, Senior, Andrew, and Mao, Mark Z.

Im-
proving the speed of neural networks on CPUs. In Proc. Deep
Learning and Unsupervised Feature Learning NIPS Workshop,
2011.

Wan, Li, Zeiler, Matthew, Zhang, Sixin, LeCun, Yann, and Fer-
gus, Rob. Regularization of neural networks using dropcon-
nect. In ICML’2013, 2013.

