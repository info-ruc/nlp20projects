Long-term Recurrent Convolutional Networks for

Visual Recognition and Description

Lisa Anne Hendricks?

Sergio Guadarrama?

Marcus Rohrbach?⇤

Jeff Donahue?
Subhashini Venugopalan†

†UT Austin
Austin, TX

Kate Saenko‡
‡UMass Lowell
Lowell, MA

Trevor Darrell?⇤

?UC Berkeley, ⇤ICSI

Berkeley, CA

vsub@cs.utexas.edu

saenko@cs.uml.edu

{jdonahue, lisa anne, sguada,

rohrbach, trevor}@eecs.berkeley.edu

Abstract

Models based on deep convolutional networks have dom-
inated recent image interpretation tasks; we investigate
whether models which are also recurrent, or “temporally
deep”, are effective for tasks involving sequences, visual
and otherwise. We develop a novel recurrent convolutional
architecture suitable for large-scale visual learning which
is end-to-end trainable, and demonstrate the value of these
models on benchmark video recognition tasks, image de-
scription and retrieval problems, and video narration chal-
lenges. In contrast to current models which assume a ﬁxed
spatio-temporal receptive ﬁeld or simple temporal averag-
ing for sequential processing, recurrent convolutional mod-
els are “doubly deep” in that they can be compositional
in spatial and temporal “layers”. Such models may have
advantages when target concepts are complex and/or train-
ing data are limited. Learning long-term dependencies is
possible when nonlinearities are incorporated into the net-
work state updates. Long-term RNN models are appealing
in that they directly can map variable-length inputs (e.g.,
video frames) to variable length outputs (e.g., natural lan-
guage text) and can model complex temporal dynamics; yet
they can be optimized with backpropagation. Our recurrent
long-term models are directly connected to modern visual
convnet models and can be jointly trained to simultaneously
learn temporal dynamics and convolutional perceptual rep-
resentations. Our results show such models have distinct
advantages over state-of-the-art models for recognition or
generation which are separately deﬁned and/or optimized.

1. Introduction

Recognition and description of images and videos is
a fundamental challenge of computer vision. Dramatic

1

Figure 1: We propose Long-term Recurrent Convolutional Net-
works (LRCNs), a class of architectures leveraging the strengths
of rapid progress in CNNs for visual recognition problem, and the
growing desire to apply such models to time-varying inputs and
outputs. LRCN processes the (possibly) variable-length visual in-
put (left) with a CNN (middle-left), whose outputs are fed into a
stack of recurrent sequence models (LSTMs, middle-right), which
ﬁnally produce a variable-length prediction (right).

progress has been achieved by supervised convolutional
models on image recognition tasks, and a number of exten-
sions to process video have been recently proposed. Ideally,
a video model should allow processing of variable length
input sequences, and also provide for variable length out-
puts, including generation of full-length sentence descrip-
tions that go beyond conventional one-versus-all prediction
tasks. In this paper we propose long-term recurrent convo-
lutional networks (LRCNs), a novel architecture for visual
recognition and description which combines convolutional
layers and long-range temporal recursion and is end-to-end
trainable (see Figure 1). We instantiate our architecture for
speciﬁc video activity recognition, image caption genera-

tion, and video description tasks as described below.

To date, CNN models for video processing have success-
fully considered learning of 3-D spatio-temporal ﬁlters over
raw sequence data [13, 2], and learning of frame-to-frame
representations which incorporate instantaneous optic ﬂow
or trajectory-based models aggregated over ﬁxed windows
or video shot segments [16, 33]. Such models explore two
extrema of perceptual time-series representation learning:
either learn a fully general time-varying weighting, or apply
simple temporal pooling. Following the same inspiration
that motivates current deep convolutional models, we advo-
cate for video recognition and description models which are
also deep over temporal dimensions; i.e., have temporal re-
currence of latent variables. RNN models are well known
to be “deep in time”; e.g., explicitly so when unrolled, and
form implicit compositional representations in the time do-
main. Such “deep” models predated deep spatial convolu-
tion models in the literature [31, 45].

Recurrent Neural Networks have long been explored in
perceptual applications for many decades, with varying re-
sults. A signiﬁcant limitation of simple RNN models which
strictly integrate state information over time is known as the
“vanishing gradient” effect: the ability to backpropogate an
error signal through a long-range temporal interval becomes
increasingly impossible in practice. A class of models
which enable long-range learning was ﬁrst proposed in [12],
and augments hidden state with nonlinear mechanisms to
cause state to propagate without modiﬁcation, be updated,
or be reset, using simple memory-cell like neural gates.
While this model proved useful for several tasks, its util-
ity became apparent in recent results reporting large-scale
learning of speech recognition [10] and language transla-
tion models [39, 5].

We show here that long-term recurrent convolutional
models are generally applicable to visual time-series mod-
eling; we argue that in visual tasks where static or ﬂat tem-
poral models have previously been employed, long-term
RNNs can provide signiﬁcant improvement when ample
training data are available to learn or reﬁne the representa-
tion. Speciﬁcally, we show LSTM-type models provide for
improved recognition on conventional video activity chal-
lenges and enable a novel end-to-end optimizable mapping
from image pixels to sentence-level natural language de-
scriptions. We also show that these models improve gen-
eration of descriptions from intermediate visual representa-
tions derived from conventional visual models.

We instantiate our proposed architecture in three experi-
mental settings (see Figure 3). First, we show that directly
connecting a visual convolutional model to deep LSTM net-
works, we are able to train video recognition models that
capture complex temporal state dependencies (Figure 3 left;
Section 4). While existing labeled video activity datasets
may not have actions or activities with extremely complex

time dynamics, we nonetheless see improvements on the or-
der of 4% on conventional benchmarks.

Second, we explore direct end-to-end trainable image to
sentence mappings. Strong results for machine translation
tasks have recently been reported [39, 5]; such models are
encoder/decoder pairs based on LSTM networks. We pro-
pose a multimodal analog of this model, and describe an
architecture which uses a visual convnet to encode a deep
state vector, and an LSTM to decode the vector into an natu-
ral language string (Figure 3 middle; Section 5). The result-
ing model can be trained end-to-end on large-scale image
and text datasets, and even with modest training provides
competitive generation results compared to existing meth-
ods.

Finally, we show that LSTM decoders can be driven di-
rectly from conventional computer vision methods which
predict higher-level discriminative labels, such as the se-
mantic video role tuple predictors in [30] (Figure 3 right;
Section 6). While not end-to-end trainable, such models
offer architectural and performance advantages over previ-
ous statistical machine translation-based approaches, as re-
ported below.

We have realized a generalized “LSTM”-style RNN
model in the widely-adopted open source deep learning
framework Caffe [14], incorporating the speciﬁc LSTM
units of [47, 39, 5].

2. Background: Recurrent Neural Networks

(RNNs)
Traditional RNNs (Figure 2, left) can learn complex tem-
poral dynamics by mapping input sequences to a sequence
of hidden states, and hidden states to outputs via the follow-
ing recurrence equations (Figure 2, left):

ht = g(Wxhxt + Whhht 1 + bh)
zt = g(Whzht + bz)

where g is an element-wise non-linearity, such as a sigmoid
or hyperbolic tangent, xt is the input, ht 2 RN is the hidden
state with N hidden units, and yt is the output at time t.
For a length T input sequence hx1, x2, ..., xTi, the updates
above are computed sequentially as h1 (letting h0 = 0), y1,
h2, y2, ..., hT , yT .

Though RNNs have proven successful on tasks such as
speech recognition [43] and text generation [38], it can be
difﬁcult to train them to learn long-term dynamics, likely
due in part to the vanishing and exploding gradients prob-
lem [12] that can result from propagating the gradients
down through the many layers of the recurrent network,
each corresponding to a particular timestep. LSTMs pro-
vide a solution by incorporating memory units that allow
the network to learn when to forget previous hidden states
and when to update hidden states given new information.

As research on LSTMs has progressed, hidden units with
varying connections within the memory unit have been pro-
posed. We use the LSTM unit as described in [46] (Figure 2,
right), which is a slight simpliﬁcation of the one described
in [10]. Letting  (x) = (1 + e x) 1 be the sigmoid non-
linearity which squashes real-valued inputs to a [0, 1] range,
and letting  (x) = ex e x
ex+e x = 2 (2x)   1 be the hyper-
bolic tangent nonlinearity, similarly squashing its inputs to
a [ 1, 1] range, the LSTM updates for timestep t given in-
puts xt, ht 1, and ct 1 are:

it =  (Wxixt + Whiht 1 + bi)
ft =  (Wxf xt + Whf ht 1 + bf )
ot =  (Wxoxt + Whoht 1 + bo)
gt =  (Wxcxt + Whcht 1 + bc)
ct = ft   ct 1 + it   gt
ht = ot    (ct)

Figure 2: A diagram of a basic RNN cell (left) and an LSTM mem-
ory cell (right) used in this paper (from [46], a slight simpliﬁcation
of the architecture described in [9], which was derived from the
LSTM initially proposed in [12]).

In addition to a hidden unit ht 2 RN, the LSTM in-
cludes an input gate it 2 RN, forget gate ft 2 RN, output
gate ot 2 RN, input modulation gate gt 2 RN, and mem-
ory cell ct 2 RN. The memory cell unit ct is a summation
of two things: the previous memory cell unit ct 1 which
is modulated by ft, and gt, a function of the current input
and previous hidden state, modulated by the input gate it.
Because it and ft are sigmoidal, their values lie within the
range [0, 1], and it and ft can be thought of as knobs that
the LSTM learns to selectively forget its previous memory
or consider its current input. Likewise, the output gate ot
learns how much of the memory cell to transfer to the hid-
den state. These additional cells enable the LSTM to learn
extremely complex and long-term temporal dynamics the
RNN is not capable of learning. Additional depth can be
added to LSTMs by stacking them on top of each other, us-
ing the hidden state of the LSTM in layer l   1 as the input
to the LSTM in layer l.
Recently, LSTMs have achieved impressive results on
language tasks such as speech recognition [10] and ma-
chine translation [39, 5]. Analogous to CNNs, LSTMs are

attractive because they allow end-to-end ﬁne-tuning. For
example, [10] eliminates the need for complex multi-step
pipelines in speech recognition by training a deep bidirec-
tional LSTM which maps spectrogram inputs to text. Even
with no language model or pronunciation dictionary, the
model produces convincing text translations. [39] and [5]
translate sentences from English to French with a multi-
layer LSTM encoder and decoder. Sentences in the source
language are mapped to a hidden state using an encoding
LSTM, and then a decoding LSTM maps the hidden state
to a sequence in the target language. Such an encoder de-
coder scheme allows sequences of different lengths to be
mapped to each other. Like [10] the sequence-to-sequence
architecture for machine translation circumvents the need
for language models.

The advantages of LSTMs for modeling sequential data
in vision problems are twofold. First, when integrated with
current vision systems, LSTM models are straightforward
to ﬁne-tune end-to-end. Second, LSTMs are not conﬁned
to ﬁxed length inputs or outputs allowing simple modeling
for sequential data of varying lengths, such as text or video.
We next describe a uniﬁed framework to combine LSTMs
with deep convolutional networks to create a model which
is both spatially and temporally deep.

3. Long-term Recurrent Convolutional Net-

work (LRCN) model
This work proposes a Long-term Recurrent Convolu-
tional Network (LRCN) model combinining a deep hier-
archical visual feature extractor (such as a CNN) with a
model that can learn to recognize and synthesize temporal
dynamics for tasks involving sequential data (inputs or out-
puts), visual, linsguistical or otherwise. Figure 1 depicts the
core of our approach. Our LRCN model works by pass-
ing each visual input vt (an image in isolation, or a frame
from a video) through a feature transformation  V (vt)
parametrized by V to produce a ﬁxed-length vector rep-
resentation  t 2 Rd. Having computed the feature-space
representation of the visual input sequence h 1,  2, ...,  Ti,
the sequence model then takes over.
In its most general form, a sequence model parametrized
by W maps an input xt and a previous timestep hidden state
ht 1 to an output zt and updated hidden state ht. There-
fore, inference must be run sequentially (i.e., from top to
bottom, in the Sequence Learning box of Figure 1), by
computing in order: h1 = fW (x1, h0) = fW (x1, 0), then
h2 = fW (x2, h1), etc., up to hT . Some of our models stack
multiple LSTMs atop one another as described in Section 2.
The ﬁnal step in predicting a distribution P (yt) at
timestep t is to take a softmax over the outputs zt of the
sequential model, producing a distribution over the (in our
case, ﬁnite and discrete) space C of possible per-timestep

outputs: P (yt = c) = exp(Wzczt,c+bc)

exp(Wzczt,c0 +bc).

Pc02C

The success of recent very deep models for object recog-
nition [22, 34, 40] suggests that strategically composing
many “layers” of non-linear functions can result in very
powerful models for perceptual problems. For large T ,
the above recurrence indicates that the last few predictions
from a recurrent network with T timesteps are computed by
a very “deep” (T -layered) non-linear function, suggesting
that the resulting recurrent model may have similar repre-
sentational power to a T -layer neural network. Critically,
however, the sequential model’s weights W are reused at
every timestep, forcing the model to learn generic timestep-
to-timestep dynamics (as opposed to dynamics directly con-
ditioned on t, the sequence index) and preventing the pa-
rameter size from growing in proportion to the maximum
number of timesteps.

In most of our experiments, the visual feature transfor-
mation   corresponds to the activations in some layer of
a deep CNN. Using a visual transformation  V (.) which
is time-invariant and independent at each timestep has the
important advantage of making the expensive convolutional
inference and training parallelizable over all timesteps of
the input, facilitating the use of fast contemporary CNN im-
plementations whose efﬁciency relies on independent batch
processing, and end-to-end optimization of the visual and
sequential model parameters V and W .

We consider three vision problems (activity recognition,
image description and video description) which instantiate
one of the following broad classes of sequential learning
tasks:

1. Sequential

inputs, ﬁxed outputs (Figure 3,

left):
hx1, x2, ..., xTi 7! y. The visual activity recognition
problem can fall under this umbrella, with videos of
arbitrary length T as input, but with the goal of pre-
dicting a single label like running or jumping drawn
from a ﬁxed vocabulary.

2. Fixed inputs, sequential outputs (Figure 3, middle):
x 7! hy1, y2, ..., yTi. The image description problem
ﬁts in this category, with a non-time-varying image as
input, but a much larger and richer label space consist-
ing of sentences of any length.

3. Sequential

inputs and outputs (Figure 3,

right):
hx1, x2, ..., xTi 7! hy1, y2, ..., yT 0i. Finally, it’s easy
to imagine tasks for which both the visual input and
output are time-varying, and in general the number of
input and output timesteps may differ (i.e., we may
have T 6= T 0). In the video description task, for exam-
ple, the input and output are both sequential, and the
number of frames in the video should not constrain the
length of (number of words in) the natural-language
description.

In the previously described formulation, each instance
has T inputs hx1, x2, ..., xTi and T outputs hy1, y2, ..., yTi.
We describe how we adapt this formulation in our hybrid
model to tackle each of the above three problem settings.
With sequential inputs and scalar outputs, we take a late
fusion approach to merging the per-timestep predictions
hy1, y2, ..., yTi into a single prediction y for the full se-
quence. With ﬁxed-size inputs and sequential outputs, we
simply duplicate the input x at all T timesteps xt := x (not-
ing this can be done cheaply due to the time-invariant vi-
sual feature extractor). Finally, for a sequence-to-sequence
problem with (in general) different input and output lengths,
we take an “encoder-decoder” approach inspired by [47]. In
this approach, one sequence model, the encoder, is used to
map the input sequence to a ﬁxed-length vector, then an-
other sequence model, the decoder, is used to unroll this
vector to sequential outputs of arbitrary length. Under this
model, the system as a whole may be thought of as having
T + T 0 timesteps of input and output, wherein the input is
processed and the decoder outputs are ignored for the ﬁrst
T timesteps, and the predictions are made and “dummy”
inputs are ignored for the latter T 0 timesteps.

L(V, W ) =  PT

Under the proposed system, the weights (V, W ) of the
model’s visual and sequential components can be learned
jointly by maximizing the likelihood of the ground truth
outputs yt conditioned on the input data and labels up to that
point (x1:t, y1:t 1) In particular, for a particular training se-
t=1, we minimize the negative log likelihood
quence (xt, yt)T
t=1 log PV,W (yt|x1:t, y1:t 1).

One of the most appealing aspects of the described sys-
tem is the ability to learn the parameters “end-to-end,” such
that the parameters V of the visual feature extractor learn
to pick out the aspects of the visual input that are rele-
vant to the sequential classiﬁcation problem. We train our
LRCN models using stochastic gradient descent with mo-
mentum, with backpropagation used to compute the gradi-
ent rL(V, W ) of the objective L with respect to all param-
eters (V, W ).
We next demonstrate the power of models which are both
deep in space and deep in time by exploring three appli-
cations: activity recognition, image description, and video
description.
4. Activity recognition

Activity recognition is an example of the ﬁrst sequen-
tial learning task described above; T individual frames are
inputs into T convolutional networks which are then con-
nected to a single-layer LSTM with 256 hidden units. A
large body of recent work has proposed deep architectures
for activity recognition ([16, 33, 13, 2, 1]).
[33, 16] both
propose convolutional networks which learn ﬁlters based on
a stack of N input frames. Though we analyze clips of 16
frames in this work, we note that the LRCN system is more

Figure 3: Task-speciﬁc instantiations of our LRCN model for activity recognition, image description, and video description.

ﬂexible than [33, 16] since it is not constrained to analyz-
ing ﬁxed length inputs and could potentially learn to rec-
ognize complex video sequences (e.g., cooking sequences
as presented in 6). [1, 2] use recurrent neural networks to
learn temporal dynamics of either traditional vision features
([1]) or deep features ([2]), but do not train their models
end-to-end and do not pre-train on larger object recognition
databases for important performance gains.

We explore two variants of the LRCN architecture: one
in which the LSTM is placed after the ﬁrst fully connected
layer of the CNN (LRCN-fc6) and another in which the
LSTM is placed after the second fully connected layer of
the CNN (LRCN-fc7). We train the LRCN networks with
video clips of 16 frames. The LRCN predicts the video class
at each time step and we average these predictions for ﬁnal
classiﬁcation. At test time, we extract 16 frame clips with a
stride of 8 frames from each video and average across clips.
We also consider both RGB and ﬂow inputs. Flow is
computed with [4] and transformed into a “ﬂow image”
by centering x and y ﬂow values around 128 and mul-
tiplying by a scalar such that ﬂow values fall between 0
and 255. A third channel for the ﬂow image is created
by calculating the ﬂow magnitude. The CNN base of the
LRCN is a hybrid of the Caffe [14] reference model, a mi-
nor variant of AlexNet [22], and the network used by Zeiler
& Fergus [48]. The net is pre-trained on the 1.2M image
ILSVRC-2012 [32] classiﬁcation training subset of the Im-
ageNet [7] dataset, giving the network a strong initialization
to facilitate faster training and prevent over-ﬁtting to the rel-
atively small video datasets. When classifying center crops,
the top-1 classiﬁcation accuracy is 60.2% and 57.4% for
the hybrid and Caffe reference models, respectively. In our
baseline model, T video frames are individually classiﬁed
by a CNN. As in the LSTM model, whole video classiﬁca-

tion is done by averaging scores across all video frames.
4.1. Evaluation

We evaluate our architecture on the UCF-101 dataset
[37] which consists of over 12,000 videos categorized into
101 human action classes. The dataset is split into three
splits, with a little under 8,000 videos in the training set for
each split.

Table 1, columns 2-3, compare video classiﬁcation of
our proposed models (LRCN-fc6, LRCN-fc7) against the
baseline architecture for both RGB and ﬂow inputs. Each
LRCN network is trained end-to-end. The LRCN-fc6 net-
work yields the best results for both RGB and ﬂow and im-
proves upon the baseline network by 0.49 % and 5.27% re-
spectively.

RGB and ﬂow networks can be combined by comput-
ing a weighted average of network scores as proposed in
[33]. Like [33], we report two weighted averages of the
predictions from the RGB and ﬂow networks in Table 1
(right). Since the ﬂow network outperforms the RGB net-
work, weighting the ﬂow network higher unsurprisingly
leads to better accuracy. In this case, LRCN outperforms
the baseline single-frame model by 3.82%.

The LRCN shows clear improvement over the baseline
single-frame system and approaches the accuracy achieved
by other deep models. [33] report the results on UCF-101
by computing a weighted average between ﬂow and RGB
networks (86.4% for split 1 and 87.6% averaging over all
splits). [16] reports 65.4% accuracy on UCF-101, which is
substantially lower than our LRCN model.
5. Image description

In contrast to activity recognition, the static image de-
scription task only requires a single convolutional network

Model
Single frame (split-1)
LRCN-fc6 (split-1)
LRCN-fc7 (split-1)
Single frame (all splits)
LRCN-fc6 (all splits)

Single Input Type Weighted Average
RGB
1/3, 2/3
79.04
69.00
82.92
71.12
80.51
70.68
78.84
67.70
68.19
82.66

1/2, 1/2
75.71
81.97
79.01
75.87
80.62

Flow
72.20
76.95
69.36
72.19
77.46

Table 1: Activity recognition: Comparing single frame models
to LRCN networks for activity recognition in the UCF-101 [37]
dataset, with both RGB and ﬂow inputs. Values for split-1 as
well as the average across all three splits are shown. Our LRCN
model consistently and strongly outperforms a model based on
predictions from the underlying convolutional network architec-
ture alone. On split-1, we show that placing the LSTM on fc6
performs better than fc7.

since the input consists of a single image. A variety of deep
and multi-modal models [8, 36, 19, 15, 25, 20, 18] have
been proposed for image description; in particular, [20, 18]
combine deep temporal models with convolutional repre-
sentations.
[20], utilizes a “vanilla” RNN as described
in Section 2, potentially making learning long-term tempo-
ral dependencies difﬁcult. Contemporaneous with and most
similar to our work is [18], which proposes a different ar-
chitecture that uses the hidden state of an LSTM encoder
at time T as the encoded representation of the length T in-
put sequence.
It then maps this sequence representation,
combined with the visual representation from a convnet,
into a joint space from which a separate decoder predicts
words. This is distinct from our arguably simpler architec-
ture, which takes as per-timestep input a copy of the static
input image, along with the previous word. We present
empirical results showing that our integrated LRCN archi-
tecture outperforms these prior approaches, none of which
comprise an end-to-end optimizable system over a hierar-
chy of visual and temporal parameters.

We now describe our instantiation of the LRCN architec-
ture for the image description task. At each timestep, both
the image features and the previous word are provided as in-
puts to the sequential model, in this case a stack of LSTMs
(each with 1000 hidden units), which is used to learn the
dynamics of the time-varying output sequence, natural lan-
guage. At timestep t, the input to the bottom-most LSTM is
the embedded ground truth word from the previous timestep
wt 1. For sentence generation, the input becomes a sample
ˆwt 1 from the model’s predicted distribution at the previous
timestep. The second LSTM in the stack fuses the outputs
of the bottom-most LSTM with the image representation
 V (x) to produce a joint representation of the visual and
language inputs up to time t. (The visual model  V (x) used
in this experiment is the base Caffe [14] reference model,
very similar to the well-known AlexNet [22], pre-trained on
ILSVRC-2012 [32] as in Section 4.) Any further LSTMs
in the stack transform the outputs of the LSTM below, and
the fourth LSTM’s outputs are inputs to the softmax which

produces a distribution over words p(wt|w1:t 1,  V (x)).
Following [19], we refer to the use of the bottom-most
LSTM to exclusively process the language input (with no
visual input) as the factored version of the model. We study
the importance of this in Appendix ?? by comparing it to an
unfactored variant. See Figure 6 for details on the variants
we study.

Without any explicit language modeling or deﬁned syn-
tax structure, the described LRCN system learns mappings
from pixel intensity values to natural language descriptions
that are often semantically descriptive and grammatically
correct.

5.1. Evaluation

We evaluate our image description model for retrieval
and generation tasks. We ﬁrst show the effectiveness of
our model by quantitatively evaluating it on the image
and caption retrieval tasks proposed by [26] and seen in
[25, 15, 36, 8, 18]. We report results on Flickr30k [28],
and the newly released COCO2014 [24] datasets, both with
ﬁve sentence annotations per image.

Retrieval results are recorded in Table 2. We report me-
dian rank, Medr, of the ﬁrst retrieved ground truth image
or caption and Recall@K, the number of images or cap-
tions for which a correct caption or image is retrieved within
the top K results. Our model consistently outperforms the
strong baselines from recent work [18, 25, 15, 36, 8] as can
be seen in Table 2. Here, we note that the new Oxford-
Net model in [18] outperforms our model on the retrieval
task. However, OxfordNet [18] utilizes a better-performing
convolutional network [35] to get the additional edge over
the base ConvNet [18] result. The strength of our temporal
model (and integration of the temporal and visual models)
can be more directly measured against the ConvNet [18] re-
sult, which uses the same base CNN architecture [22] pre-
trained on the same data.

To evaluate sentence generation, we primarily use the
BLEU [27] metric which was designed for automated evalu-
ation of statistical machine translation. BLEU is a modiﬁed
form of precision that compares N-gram fragments of the
hypothesis translation with multiple reference translations.
We use BLEU as a measure of similarity of the descriptions.
The unigram scores (B-1) account for the adequacy of (or
the information retained) by the translation, while longer N-
gram scores (B-2, B-3) account for the ﬂuency. We compare
our results with [25] (on Flickr30k), and two strong near-
est neighbor baselines computed using AlexNet fc7 and fc8
layer outputs. We used 1-nearest neighbor to retrieve the
most similar image in the training database and average the
BLEU score over the captions. The results on Flickr30k are
reported in Table 3. Additionally, we report results on the
new COCO2014 [24] dataset which has 80,000 training im-
ages, and 40,000 validation images. Similar to Flickr30k,

R@1

R@5

R@10 Medr

Correctness

Grammar

Relevance

Caption to Image (Flickr30k)

DeViSE [8]
SDT-RNN [36]
DeFrag [15]
m-RNN [25]
ConvNet [18]
LRCN2f (ours)

6.7
8.9
10.3
12.6
11.8
17.5

21.9
29.8
31.4
31.2
34.0
40.3

32.7
41.1
44.5
41.5
46.3
50.8

Image to Caption (Flickr30k)

DeViSE [8]
SDT-RNN [36]
DeFrag [15]
m-RNN [25]
ConvNet [18]
LRCN2f (ours)

4.5
9.6
16.4
18.4
14.8
23.6

18.1
29.8
40.2
40.2
39.2
46.6

29.2
41.1
54.7
50.9
50.9
58.3

Caption to Image (COCO)

LRCN2f (ours)

29.0

61.6

74.8

Image to Caption (COCO)

LRCN2f (ours)

39.1

69.0

80.9

25
16
13
16
13
9

26
16
8
10
10
7

3

2

Table 2: Image description: retrieval results for the Flickr30k [28]
and COCO2014 [24] datasets. R@K is the average recall at rank
K (high is good). Medr is the median rank (low is good). Note
that [18] achieves better retrieval performance using a stronger
CNN architecture see text.

m-RNN [25]
1NN fc8 base (ours)
1NN fc7 base (ours)
LRCN (ours)

1NN fc7 base (c5)
LRCN (ours)

B-1
54.79
37.34
38.81
58.72

B-1
46.23
66.86

Flickr30k [28]
B-3
B-2
19.52
23.92
9.39
18.66
20.16
10.37
39.06
25.12
COCO 2014 [24]

B-2
26.39
48.92

B-3
15.07
34.89

B-4
-

4.88
5.54
16.46

B-4
08.73
24.92

Table 3: Image description: Sentence generation results (BLEU
scores (%) – ours are adjusted with the brevity penalty) for the
Flickr30k [28] and COCO 2014 [24] test sets.

each image is annotated with 5 or more image annotations.
We isolate 5,000 images from the validation set for testing
purposes and the results are reported in Table 3.

Based on the B-1 scores in Table 3, generation using
LRCN performs comparably with m-RNN [25] in terms of
the information conveyed in the description. Furthermore,
LRCN signiﬁcantly outperforms the baselines and the m-
RNN with regard to the ﬂuency (B-2, B-3) of the genera-
tion, indicating the LRCN retains more of the bigrams and
trigrams from the human-annotated descriptions.

In addition to standard quantitative evaluations, we also
employ Amazon Mechnical Turkers (AMT) to evaluate the
generated sentences. Given an image and a set of descrip-
tions from different models, we ask Turkers to rank the
sentences based on correctness, grammar and relevance.
We compared sentences from our model to the ones made
publicly available by [18]. As seen in Table 4, our ﬁne-
tuned (FT) LRCN model performs on par with the Nearest
Neighbour (NN) on correctness and relevance, and better on

TreeTalk [23]
OxfordNet [18]
NN [18]
LRCN fc8 (ours)
LRCN FT (ours)
Captions

4.08
3.71
3.44
3.74
3.47
2.55

4.35
3.46
3.20
3.19
3.01
3.72

3.98
3.70
3.49
3.72
3.50
2.59

Table 4: Image description: Human evaluator rankings from 1-6
(low is good) averaged for each method and criterion. We eval-
uated on 785 Flickr images selected by the authors of [18] for
the purposes of comparison against this similar contemporary ap-
proach.

grammar. We show example sentence generations in Fig-
ure 7.

6. Video description

In video description we want to generate a variable
length stream of words, similar to Section 5. [11, 30, 17,
3, 6, 17, 41, 42] propose methods for generating sentence
descriptions for video, but to our knowledge we present
the ﬁrst application of deep models to the video description
task.

The LSTM framework allows us to model the video as
a variable length input stream as discussed in Section 3.
However, due to limitations of available video description
datasets we take a different path. We rely on more “tra-
ditional” activity and video recognition processing for the
input and use LSTMs for generating a sentence.

We ﬁrst distinguish the following architectures for video
description (see Figure 10). For each architecture, we as-
sume we have predictions of objects, subjects, and verbs
present in the video from a CRF based on the full video in-
put. In this way, we observe the video as whole at each time
step, not incrementally frame by frame.

(a) LSTM encoder & decoder with CRF max. (Fig-
ure 10(a)) The ﬁrst architecture is motivated by the video
description approach presented in [30]. They ﬁrst recognize
a semantic representation of the video using the maximum a
posterior estimate (MAP) of a CRF taking in video features
as unaries. This representation, e.g. hperson,cut,cutting
boardi, is then concatenated to a input sentence (person cut
cutting board) which is translated to a natural sentence (a
person cuts on the board) using phrase-based statistical ma-
chine translation (SMT) [21]. We replace the SMT with an
LSTM, which has shown state-of-the-art performance for
machine translation between languages [39, 5]. The archi-
tecture (shown in Figure 10(a)) has an encoder LSTM (or-
ange) which encodes the one-hot vector (binary index vec-
tor in a vocabulary) of the input sentence as done in [39].
This allows for variable-length inputs. (Note that the input
sentence might have a different number of words than el-
ements of the semantic representation.) At the end of the

Figure 4: Our approaches to video description. (a) LSTM encoder & decoder with CRF max (b) LSTM decoder with CRF max (c) LSTM
decoder with CRF probabilities. (For larger ﬁgure zoom or see supplemental).

Architecture
SMT [30]
SMT [29]
(a) LSTM Encoder-Decoder (ours)
(b) LSTM Decoder (ours)
(c) LSTM Decoder (ours)

Input
BLEU
CRF max 24.9
CRF prob 26.9
CRF max 25.3
CRF max 27.4
CRF prob 28.8

Table 5: Video description: Results on detailed description of
TACoS multilevel[29], in %, see Section C.3 for details.

encoder stage, the ﬁnal hidden unit must remember all nec-
essary information before being input into the decoder stage
(pink) in which the hidden representation is decoded into a
sentence, one word at each time step. We use the same two-
layer LSTM for encoding and decoding.

(b) LSTM decoder with CRF max. (Figure 10(b)) In
this variant we exploit that the semantic representation can
be encoded as a single ﬁxed length vector. We provide the
entire visual input representation at each time step to the
LSTM, analogous to how an entire image is provided as an
input to the LSTM in image description.

(c) LSTM decoder with CRF prob. (Figure 10(c)) A
beneﬁt of using LSTMs for machine translation compared
to phrase-based SMT [21] is that it can naturally incorpo-
rate probability vectors during training and test time which
allows the LSTM to learn uncertainties in visual generation
rather than relying on MAP estimates. The architecture is
the the same as in (b), but we replace max predictions with
probability distributions.
6.1. Evaluation

We evaluate our approach on the TACoS multilevel
[29] dataset, which has 44,762 video/sentence pairs (about
40,000 for training/validation). We compare to [30] who
use max prediction as well as a variant presented in [29]
which takes CRF probabilities at test time and uses a word
lattice to ﬁnd an optimal sentence prediction. Since we use
the max prediction as well as the probability scores pro-
vided by [29], we have an identical visual representation.
[29] uses dense trajectories [44] and SIFT features as well
as temporal context reasoning modeled in a CRF.

Table 5 shows the BLEU-4 score. The results show that

(1) the LSTM outperforms an SMT-based approach to video
description; (2) the simpler decoder architecture (b) and (c)
achieve better performance than (a), likely because the in-
put does not need to be memorized; and (3) our approach
achieves 28.8%, clearly outperforming the best reported
number of 26.9% on TACoS multilevel by [29].

More broadly, these results show that our architecture
is not restricted to deep neural networks inputs but can be
cleanly integrated with other ﬁxed or variable length inputs
from other vision systems.
7. Conclusions

We’ve presented LRCN, a class of models that is both
spatially and temporally deep, and has the ﬂexibility to be
applied to a variety of vision tasks involving sequential
inputs and outputs. Our results consistently demonstrate
that by learning sequential dynamics with a deep sequence
model, we can improve on previous methods which learn a
deep hierarchy of parameters only in the visual domain, and
on methods which take a ﬁxed visual representation of the
input and only learn the dynamics of the output sequence.
As the ﬁeld of computer vision matures beyond tasks
with static input and predictions, we envision that “dou-
bly deep” sequence modeling tools like LRCN will soon
become central pieces of most vision systems, as convo-
lutional architectures recently have. The ease with which
these tools can be incorporated into existing visual recog-
nition pipelines makes them a natural choice for perceptual
problems with time-varying visual input or sequential out-
puts, which these methods are able to produce with little
input preprocessing and no hand-designed features.
Acknowledgements

The authors thank Oriol Vinyals for valuable advice and
helpful discussion throughout this work. This work was
supported in part by DARPA’s MSEE and SMISC pro-
grams, NSF awards IIS-1427425 and IIS-1212798, and the
Berkeley Vision and Learning Center. The GPUs used for
this research were donated by the NVIDIA Corporation.

Marcus Rohrbach was supported by a fellowship within the
FITweltweit-Program of the German Academic Exchange
Service (DAAD).
References
[1] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and
A. Baskurt. Action classiﬁcation in soccer videos with long
short-term memory recurrent neural networks.
In ICANN.
2010. 4, 5

[2] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and
A. Baskurt.
Sequential deep learning for human action
recognition. In Human Behavior Understanding. 2011. 2,
4, 5

[3] A. Barbu, A. Bridge, Z. Burchill, D. Coroian, S. Dickin-
son, S. Fidler, A. Michaux, S. Mussman, S. Narayanaswamy,
D. Salvi, L. Schmidt, J. Shangguan, J. M. Siskind, J. Wag-
goner, S. Wang, J. Wei, Y. Yin, and Z. Zhang. Video in sen-
tences out. In UAI, 2012. 7

[4] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High ac-
curacy optical ﬂow estimation based on a theory for warping.
In ECCV. 2004. 5

[5] K. Cho, B. van Merri¨enboer, D. Bahdanau, and Y. Bengio.
On the properties of neural machine translation: Encoder-
decoder approaches. arXiv preprint arXiv:1409.1259, 2014.
2, 3, 7

[6] P. Das, C. Xu, R. Doell, and J. Corso. Thousand frames
in just a few words: Lingual description of videos through
latent topics and sparse object stitching. In CVPR, 2013. 7
[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. ImageNet: A large-scale hierarchical image database.
In CVPR, 2009. 5

[8] A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ran-
zato, and T. Mikolov. DeViSE: A deep visual-semantic em-
bedding model. In NIPS, 2013. 6, 7

[9] A. Graves. Generating sequences with recurrent neural net-

works. arXiv preprint arXiv:1308.0850, 2013. 3

[10] A. Graves and N. Jaitly. Towards end-to-end speech recog-
In ICML, 2014. 2,

nition with recurrent neural networks.
3

[11] S. Guadarrama, N. Krishnamoorthy, G. Malkarnenkar,
S. Venugopalan, R. Mooney, T. Darrell, and K. Saenko.
YouTube2Text: Recognizing and describing arbitrary activ-
ities using semantic hierarchies and zero-shoot recognition.
In ICCV, 2013. 7

[12] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural Computation, 1997. 2, 3

[13] S. Ji, W. Xu, M. Yang, and K. Yu. 3D convolutional neu-
ral networks for human action recognition. Pattern Analysis
and Machine Intelligence, IEEE Transactions on, 35(1):221–
231, 2013. 2, 4

[14] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. In ACM MM, 2014.
2, 5, 6

[15] A. Karpathy, A. Joulin, and L. Fei-Fei. Deep fragment em-
beddings for bidirectional image sentence mapping. NIPS,
2014. 6, 7

[16] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In CVPR, 2014. 2, 4, 5, 12

[17] M. U. G. Khan, L. Zhang, and Y. Gotoh. Human focused
video description. In Proceedings of the IEEE International
Conference on Computer Vision Workshops (ICCV Work-
shops), 2011. 7

[18] R. Kiros, R. Salakhuditnov, and R. S. Zemel. Unifying
visual-semantic embeddings with multimodal neural lan-
guage models. arXiv preprint arXiv:1411.2539, 2014. 6,
7

[19] R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal neu-

ral language models. In ICML, 2014. 6, 15

[20] R. Kiros, R. Zemel, and R. Salakhutdinov. Multimodal neu-
ral language models. In Proc. NIPS Deep Learning Work-
shop, 2013. 6

[21] P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens,
C. Dyer, O. Bojar, A. Constantin, and E. Herbst. Moses:
Open source toolkit for statistical machine translation.
In
ACL, 2007. 7, 8

[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012. 4, 5, 6

ImageNet
In

[23] P. Kuznetsova, V. Ordonez, T. L. Berg, U. C. Hill, and
Y. Choi. Treetalk: Composition and compression of trees
for image descriptions. Transactions of the Association for
Computational Linguistics, 2(10):351–362, 2014. 7

[24] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. arXiv preprint arXiv:1405.0312,
2014. 6, 7, 13, 16

[25] J. Mao, W. Xu, Y. Yang, J. Wang, and A. L. Yuille. Explain
images with multimodal recurrent neural networks. arXiv
preprint arXiv:1410.1090, 2014. 6, 7

[26] P. Y. Micah Hodosh and J. Hockenmaier. Framing image
description as a ranking task: Data, models and evaluation
metrics. JAIR, 47:853–899, 2013. 6

[27] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. BLEU: a
method for automatic evaluation of machine translation. In
ACL, 2002. 6

[28] M. H. Peter Young, Alice Lai and J. Hockenmaier. From im-
age descriptions to visual denotations: New similarity met-
rics for semantic inference over event descriptions. TACL,
2:67–78, 2014. 6, 7

[29] A. Rohrbach, M. Rohrbach, W. Qiu, A. Friedrich, M. Pinkal,
and B. Schiele. Coherent multi-sentence video description
with variable level of detail. In GCPR, 2014. 8, 18, 20

[30] M. Rohrbach, W. Qiu, I. Titov, S. Thater, M. Pinkal, and
B. Schiele. Translating video content to natural language
descriptions. In ICCV, 2013. 2, 7, 8

[31] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learn-
ing internal representations by error propagation. Technical
report, DTIC Document, 1985. 2

[32] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge, 2014. 5, 6

[33] K. Simonyan and A. Zisserman. Two-stream convolutional
networks for action recognition in videos. arXiv preprint
arXiv:1406.2199, 2014. 2, 4, 5, 12

[34] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 4

[35] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
6

[36] R. Socher, Q. Le, C. Manning, and A. Ng. Grounded com-
positional semantics for ﬁnding and describing images with
sentences. In NIPS Deep Learning Workshop, 2013. 6, 7

[37] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A dataset
of 101 human actions classes from videos in the wild. arXiv
preprint arXiv:1212.0402, 2012. 5, 6, 14

[38] I. Sutskever, J. Martens, and G. E. Hinton. Generating text

with recurrent neural networks. In ICML, 2011. 2

[39] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence

learning with neural networks. In NIPS, 2014. 2, 3, 7

[40] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabi-
novich. Going deeper with convolutions. arXiv preprint
arXiv:1409.4842, 2014. 4

[41] C. C. Tan, Y.-G. Jiang, and C.-W. Ngo. Towards textually
describing complex video contents with audio-visual concept
classiﬁers. In MM, 2011. 7

[42] J. Thomason, S. Venugopalan, S. Guadarrama, K. Saenko,
and R. J. Mooney. Integrating language and vision to gen-
erate natural language descriptions of videos in the wild. In
COLING, 2014. 7

[43] O. Vinyals, S. V. Ravuri, and D. Povey. Revisiting recurrent

neural networks for robust ASR. In ICASSP, 2012. 2

[44] H. Wang, A. Kl¨aser, C. Schmid, and C. Liu. Dense trajecto-
ries and motion boundary descriptors for action recognition.
IJCV, 2013. 8

[45] R. J. Williams and D. Zipser. A learning algorithm for con-
tinually running fully recurrent neural networks. Neural
Computation, 1989. 2

[46] W. Zaremba and I. Sutskever. Learning to execute. arXiv

preprint arXiv:1410.4615, 2014. 3

[47] W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent neu-
ral network regularization. arXiv preprint arXiv:1409.2329,
2014. 2, 4

[48] M. D. Zeiler and R. Fergus. Visualizing and understanding

convolutional networks. In ECCV. 2014. 5

