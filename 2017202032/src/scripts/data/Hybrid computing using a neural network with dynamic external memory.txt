ARTICLE
Hybrid computing using a neural 
network with dynamic external memory

doi:10.1038/nature20101

Alex Graves1*, Greg Wayne1*, Malcolm Reynolds1, Tim Harley1, Ivo Danihelka1, Agnieszka Grabska-Barwińska1, 
Sergio Gómez Colmenarejo1, Edward Grefenstette1, Tiago Ramalho1, John Agapiou1, Adrià Puigdomènech Badia1, 
Karl Moritz Hermann1, Yori Zwols1, Georg Ostrovski1, Adam Cain1, Helen King1, Christopher Summerfield1, Phil Blunsom1, 
Koray Kavukcuoglu1 & Demis Hassabis1

Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, 
but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to 
the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer 
(DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the 
random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent 
and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained 
with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate 
reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest 
path between specified points and inferring the missing links in randomly generated graphs, and then generalize these 
tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC 
can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, 
our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural 
networks without external read–write memory.

Modern computers separate computation and memory. Computation 
is performed by a processor, which can use an addressable memory to 
bring operands in and out of play. This confers two important benefits: 
the use of extensible storage to write new information and the ability 
to treat the contents of memory as variables. Variables are critical to 
algorithm generality: to perform the same procedure on one datum or 
another, an algorithm merely has to change the address it reads from. 
In contrast to computers, the computational and memory resources of 
artificial neural networks are mixed together in the network weights 
and neuron activity. This is a major liability: as the memory demands 
of a task increase, these networks cannot allocate new storage dynam-
ically, nor easily learn algorithms that act independently of the values 
realized by the task variables.

Although recent breakthroughs demonstrate that neural networks 
are remarkably adept at sensory processing1, sequence learning2,3 and 
reinforcement learning4, cognitive scientists and neuroscientists have 
argued that neural networks are limited in their ability to represent 
variables and data structures5–9, and to store data over long timescales 
without interference10,11. We aim to combine the advantages of neu-
ral and computational processing by providing a neural network with 
read–write access to external memory. The access is narrowly focused, 
minimizing interference among memoranda and enabling long-term 
storage12,13. The whole system is differentiable, and can therefore be 
trained end-to-end with gradient descent, allowing the network to learn 
how to operate and organize the memory in a goal-directed manner.

System overview
A DNC is a neural network coupled to an external memory matrix. 
(The behaviour of the network is independent of the memory size as 
long as the memory is not filled to capacity, which is why we view the 
memory as ‘external’.) If the memory can be thought of as the DNC’s 

1Google DeepMind, 5 New Street Square, London EC4A 3TW, UK.
*These authors contributed equally to this work.

RAM, then the network, referred to as the ‘controller’, is a differentiable 
CPU whose operations are learned with gradient descent. The DNC 
architecture differs from recent neural memory frameworks14,15 in 
that the memory can be selectively written to as well as read, allowing 
iterative modification of memory content. An earlier form of DNC, 
the neural Turing machine16, had a similar structure, but more limited 
memory access methods (see Methods for further discussion).

Whereas conventional computers use unique addresses to  
access memory contents, a DNC uses differentiable attention  
mechanisms2,16–18 to define distributions over the N rows, or ‘locations’, 
in the N ×  W memory matrix M. These distributions, which we call 
weightings, represent the degree to which each location is involved in a  
read or write operation. The read vector r returned by a read weighting  
wr over memory M is a weighted sum over the memory locations: 
 , where the ‘·’ denotes all j =  1, …, W. Similarly,  
r
= ∑
the write operation uses a write weighting ww to first erase with  
an erase vector e, then add a write vector v: M[i,j] ←  M[i,j]
(1 −  ww[i]e[j]) +  ww[i]v[j]. The functional units that determine and 
apply the weightings are called read and write heads. The operation of 
the heads is illustrated in Fig. 1 and summarized below; see Methods 
for a formal description.

N
i
=

wM i
[ , ]
⋅

r
i
[ ]

1

Interaction between the heads and the memory
The heads use three distinct forms of differentiable attention. The first is 
content lookup16,17,19–21, in which a key vector emitted by the controller  
is compared to the content of each location in memory according to 
a similarity measure (here, cosine similarity). The similarity scores 
determine a weighting that can be used by the read heads for asso-
ciative recall19 or by the write head to modify an existing vector in 
memory. Importantly, a key that only partially matches the content of 
a memory location can still be used to attend strongly to that location.  

0 0   M O N T H   2 0 1 6   |   V O L   0 0 0   |   N A T U R E   |   1

a Controller

b Read and write heads

c Memory

d Memory usage 
and temporal links

Output

Input

Write vector

Erase vector

Write key

Read key

Read mode
B C F

Read key

Read mode
B C F

Read vectors

W

Write

Read 1

Read 2

N

W

Figure 1 | DNC architecture. a, A recurrent controller network receives 
input from an external data source and produces output. b, c, The 
controller also outputs vectors that parameterize one write head (green) 
and multiple read heads (two in this case, blue and pink). (A reduced 
selection of parameters is shown.) The write head defines a write and 
an erase vector that are used to edit the N ×  W memory matrix, whose 
elements’ magnitudes and signs are indicated by box area and shading, 
respectively. Additionally, a write key is used for content lookup to find 
previously written locations to edit. The write key can contribute to 

defining a weighting that selectively focuses the write operation over the 
rows, or locations, in the memory matrix. The read heads can use gates 
called read modes to switch between content lookup using a read key (‘C’) 
and reading out locations either forwards (‘F’) or backwards (‘B’) in the 
order they were written. d, The usage vector records which locations have 
been used so far, and a temporal link matrix records the order in which 
locations were written; here, we represent the order locations were written 
to using directed arrows.

This enables a form of pattern completion whereby the value recovered 
by reading the memory location includes additional information that 
is not present in the key. In general, key-value retrieval provides a rich 
mechanism for navigating associative data structures in the external 
memory, because the content of one address can effectively encode 
references to other addresses.

A second attention mechanism records transitions between conse-
cutively written locations in an N ×  N temporal link matrix L. L[i, j] 
is close to 1 if i was the next location written after j, and is close to 0 
otherwise. For any weighting w, the operation Lw smoothly shifts the 
focus forwards to the locations written after those emphasized in w, 
whereas L⊤w shifts the focus backwards. This gives a DNC the native 
ability to recover sequences in the order in which it wrote them, even 
when consecutive writes did not occur in adjacent time-steps.

The third form of attention allocates memory for writing. The ‘usage’ 
of each location is represented as a number between 0 and 1, and a 
weighting that picks out unused locations is delivered to the write head. 
As well as automatically increasing with each write to a location, usage 
can be decreased after each read. This allows the controller to reallocate 
memory that is no longer required (see Extended Data Fig. 1). The 
allocation mechanism is independent of the size and contents of the 
memory, meaning that DNCs can be trained to solve a task using one 
size of memory and later upgraded to a larger memory without retrain-
ing (Extended Data Fig. 2). In principle, this would make it possible to 
use an unbounded external memory by automatically increasing the 
number of locations every time the minimum usage of any location 
passes a certain threshold.

The design of the attention mechanisms was motivated largely by 
computational considerations. Content lookup enables the formation 
of associative data structures; temporal links enable sequential retrieval 
of input sequences; and allocation provides the write head with unused 
locations. However, there are interesting parallels between the memory 
mechanisms of a DNC and the functional capabilities of the mamma-
lian hippocampus. DNC memory modification is fast and can be one-
shot, resembling the associative long-term potentiation of hippocampal 
CA3 and CA1 synapses22. The hippocampal dentate gyrus, a region 
known to support neurogenesis23, has been proposed to increase rep-
resentational sparsity, thereby enhancing memory capacity24: usage-
based memory allocation and sparse weightings may provide similar 

2   |   N A T U R E   |   V O L   0 0 0   |   0 0   M O N T H   2 0 1 6

facilities in our model. Human ‘free recall’ experiments demonstrate 
the increased probability of item recall in the same order as first pre-
sented—a hippocampus-dependent phenomenon accounted for by the 
temporal context model25, bearing some similarity to the formation of 
temporal links (Methods).

Synthetic question answering experiments
Our first experiments investigated the capacity of the DNC to perform 
question answering. To compare DNCs to other neural network archi-
tectures, we considered the bAbI dataset26, which includes 20 types of 
synthetically generated questions designed to mimic aspects of textual 
reasoning. The dataset consists of short ‘story’ snippets followed by 
questions with answers that can be inferred from the stories: for exam-
ple, the story “John is in the playground. John picked up the football.” 
followed by the question “Where is the football?” with answer “play-
ground” requires a system to combine two supporting facts, whereas 
“Sheep are afraid of wolves. Gertrude is a sheep. Mice are afraid of 
cats. What is Gertrude afraid of?” (answer, “wolves”) tests its facility at 
basic deduction (and resilience to distractors). We found that a single 
DNC, jointly trained on all 20 question types with 10,000 instances 
each, was able to achieve a mean test error rate of 3.8% with task failure 
(defined as > 5% error) on 2 types of questions, compared to 7.5% mean 
error and 6 failed tasks for the best previous jointly trained result21. We 
also found that DNCs performed much better than both long short-
term memory27 (LSTM; at present the benchmark neural network for 
most sequence processing tasks) and the neural Turing machine16 
(see Extended Data Table 1 for details). Unlike previous results on this 
dataset, the inputs to our model were single word tokens without any 
preprocessing or sentence-level features (see Methods for details).

Graph experiments
Although bAbI is presented in natural language, each declarative sen-
tence involves a limited vocabulary and is generated from a simple triple  
containing an actor, an action and a set of arguments. Such sentences 
could easily be rendered in graphical form: for example “John is in the 
playground” can be diagrammed as two named nodes, ‘Playground’ and 
‘John’, connected by a named edge ‘Contains’. In this sense, the prop-
ositional knowledge in many of the bAbI tasks is equivalent to a set of 
constraints on an underlying graph structure. Indeed, many important 

a Random graph

b London Underground

c Family tree

Ian

Jodie

Alan

Lindsey

Mary

Becky

Tom

Charlotte

Alison

Fergus

Jane

Steve

Jo

Mat

Liam Nina

Alice

Bob

Simon Freya

Maternal great uncle

Natalie

Underground input:
(OxfordCircus, TottenhamCtRd, Central)
(TottenhamCtRd, OxfordCircus, Central)
(BakerSt, Marylebone, Circle)
(BakerSt, Marylebone, Bakerloo)
(BakerSt, OxfordCircus, Bakerloo)

…

(LeicesterSq, CharingCross, Northern)
(TottenhamCtRd, LeicesterSq, Northern)
(OxfordCircus, PiccadillyCircus, Bakerloo)
(OxfordCircus, NottingHillGate, Central)
(OxfordCircus, Euston, Victoria)

84 edges in total

Traversal

Shortest-path

Traversal question:
(BondSt, _, Central), 
(_, _, Circle), (_, _, Circle), 
(_, _, Circle), (_, _, Circle), 
(_, _, Jubilee), (_, _, Jubilee), 

Shortest-path question:
(Moorgate, PiccadillyCircus, _)

Answer: 
(BondSt, NottingHillGate, Central)
(NottingHillGate, GloucesterRd, Circle)

…

(Westminster, GreenPark, Jubilee)
(GreenPark, BondSt, Jubilee)

Answer: 
(Moorgate, Bank, Northern)
(Bank, Holborn, Central)
(Holborn, LeicesterSq, Piccadilly)
(LeicesterSq, PiccadillyCircus, Piccadilly)

Family tree input:
(Charlotte, Alan, Father)
(Simon, Steve, Father)
(Steve , Simon, Son1)
(Nina, Alison, Mother)
(Lindsey, Fergus, Son1)

…

(Bob, Jane, Mother)
(Natalie, Alice, Mother)
(Mary, Ian, Father)
(Jane, Alice, Daughter1)
(Mat, Charlotte, Mother)

54 edges in total

Inference question:
(Freya, _, MaternalGreatUncle)

Answer: 
(Freya, Fergus, MaternalGreatUncle)

Figure 2 | Graph tasks. a, An example of a randomly generated graph used 
for training. b, Zone 1 interchange stations of the London Underground 
map, used as a generalization test for the traversal and shortest-path tasks. 
Random seven-step traversals (an example of which is shown on the left) 
were tested, yielding an average accuracy of 98.8%. Testing on all possible 
four-step shortest paths (example shown on the right) gave an average 
accuracy of 55.3%. c, The family tree that was used as a generalization 
test for the inference task; four-step relations such as the one shown in 

blue (from Freya to Fergus, her maternal great uncle) were tested, giving 
an average accuracy of 81.8%. The symbol sequences processed by the 
network during the test examples are shown beneath the graphs. The 
input is an unordered list of (‘from node’, ‘to node’, ‘edge’) triple vectors 
that describes the graph. For each task, the question is a sequence of 
triples with missing elements (denoted ‘_’) and the answer is a sequence of 
completed triples.

tasks faced by machine learning involve graph data, including parse 
trees, social networks, knowledge graphs and molecular structures. 
We therefore turn next to a set of synthetic reasoning experiments on 
randomly generated graphs.

Unlike bAbI, the edges in our graphs were presented explicitly, with 
each input vector specifying a triple consisting of two node labels and 
an edge label. We generated training graphs with random labelling and 
connectivity and defined three kinds of query: ‘traversal’, ‘shortest path’ 
and ‘inference’ (Fig. 2). After training with curriculum learning28,29 
using graphs and queries with gradually increasing complexity, the 
networks were tested (with no retraining) on two specific graphs as a 
test of generalization to realistic data: a symbolic map of the London 
Underground and an invented family tree.

For the traversal task (Fig. 2b), the network was instructed to report 
the node arrived at after leaving a start node and following a path of 
edges generated by a random walk. For the shortest-path task (Fig. 2b),  
a random start and end node were given as the query, and the net-
work was asked to return a sequence of triples corresponding to a  
minimum-length path between them. Because we considered paths of 
up to length five, this is a harder version of the path-finding task in the 
bAbI dataset, which has a maximum length of two. For the inference 
task (Fig. 2c), we predefined 400 ‘relation’ labels that stood as abbre-
viations for sequences of up to five connected edge labels. A query 
consisted of an incomplete triple specifying a start node and a relation 
label, and the required answer was the final node after following the 
relation sequence. Because the relation sequences were never presented 
to the network, they had to be inferred from the queries and targets.

As a benchmark we again compared DNCs with LSTM. In this case, 
the best LSTM network we found in an extensive hyper-parameter 
search failed to complete the first level of its training curriculum of even 
the easiest task (traversal), reaching an average of only 37% accuracy 
after almost two million training examples; DNCs reached an average of 
98.8% accuracy on the final lesson of the same curriculum after around 
one million training examples.

Figure 3 illustrates a DNC’s use of memory allocation, content lookup 
and temporal linkage to store and traverse the London Underground 
map. Visualization of a DNC trained on shortest-path suggests that it 
progressively explored the links radiating out from the start and end 
nodes until a connecting path was found (Supplementary Video 1).

Block puzzle experiments
Next we wanted to investigate the ability of DNCs to exploit their 
memory for logical planning tasks. To do this, we created a block 
puzzle game inspired by Winograd’s SHRDLU30—a classic artificial 
intelligence demonstration of an environment with movable objects 
and a rule-based agent that executed user instructions. Unlike the 
previous experiments, for which the networks were trained with 
supervised learning, we applied a form of reinforcement learning 
in which a sequence of instructions describing a goal is coupled to 
a reward function that evaluates whether the goal is satisfied—a 
set-up that resembles an animal training protocol with a symbolic  
task cue31.

Our environment, which we term Mini-SHRDLU, contains a set of 
numbered blocks on a grid board. An agent, given a view of the board 
as input, can move the top block from a column and deposit it on top 
of a stack in another column. At every episode, we generated a start 
board configuration and several possible goals. Each goal, identified by 
a single-letter label, was composed of several individual constraints on 
adjacent block pairs that were transmitted one constraint per time-step 
(goal ‘T’ is “block 6 below 2; block 4 left of 1; …”) (Fig. 4b, c). After all 
of the goals were presented, a single goal label was chosen at random, 
and the agent was cued to satisfy that goal.

The DNC used its memory to store the instructions by iteratively writing 
goals to locations (Fig. 4a); it could then carry out any chosen goal (Fig. 4c,  
Supplementary Video 2). We observed that, at the time a goal was writ-
ten, but many steps before execution was required, the first action could 
be decoded from memory (Fig. 4d). This indicates that the DNC had 
written its decision to memory before acting upon it; thus, remarkably, 

0 0   M O N T H   2 0 1 6   |   V O L   0 0 0   |   N A T U R E   |   3

s
n
o
i
t
a
c
o

l
 
y
r
o
m
e
m
d
e
d
o
c
e
D

 

Oxford Circus>Tottenham Court Rd
Tottenham Court Rd>Oxford Circus
Green Park>Oxford Circus
Victoria>Green Park
Oxford Circus>Green Park
Green Park>Victoria
Green Park>Piccadilly Circus
Piccadilly Circus>Leicester Sq
Piccadilly Circus>Green Park
Leicester Sq>Piccadilly Circus
Piccadilly Circus>Oxford Circus
Charing Cross>Piccadilly Circus
Piccadilly Circus>Charing Cross
Oxford Circus>Piccadilly Circus
Leicester Sq>Tottenham Court Rd
Charing Cross>Leicester Sq
Leicester Sq>Charing Cross
Tottenham Court Rd>Leicester Sq
Victoria>___ Victoria N
___>___ Victoria N
___>___ Central E
___>___ North S
___>___ Piccadilly W
___>___ Bakerloo N
___>___ Central E

Write head
Read head 1
Read head 2

Backward
Content
Forward

Backward
Content
Forward

a Read and write weightings

c London Underground map

Graph de(cid:31)nition

Query

Answer

d Read key

e Location content

b Read mode

Decode

Decode

From

To

Line

From

To

Line

0

5

10

15

Time

20

25

30

 

s
s
o
r
C
g
n
i
r
a
h
C

 

k
r
a
P
n
e
e
r
G

q
S
 
r
e
t
s
e
c
e
L

i

s
u
c
r
i

 

C
d
r
o
f
x
O

s
u
c
r
i

C
 
y

l
l
i

d
a
c
c
P

i

d
R

 

 
t
r
u
o
C
m
a
h
n
e
t
t
o
T

a
i
r
o
t
c
V

i

 

s
s
o
r
C
g
n
i
r
a
h
C

 

k
r
a
P
n
e
e
r
G

q
S
 
r
e
t
s
e
c
e
L

i

s
u
c
r
i

 

C
d
r
o
f
x
O

s
u
c
r
i

C
 
y

l
l
i

d
a
c
c
P

i

d
R

 

 
t
r
u
o
C
m
a
h
n
e
t
t
o
T

a
i
r
o
t
c
V

i

 

 

N
o
o
l
r
e
k
a
B

S
o
o
l
r
e
k
a
B

E

W

 
l
a
r
t
n
e
C

 
l
a
r
t
n
e
C

 

 

N
h
t
r
o
N

S
h
t
r
o
N

N
 
a
i
r
o
t
c
V

S
 
a
i
r
o
t
c
V

i

i

W
 
y

E
 
y

l
l
i

d
a
c
c
P

i

l
l
i

d
a
c
c
P

i

 

s
s
o
r
C
g
n
i
r
a
h
C

 

k
r
a
P
n
e
e
r
G

q
S
 
r
e
t
s
e
c
e
L

i

s
u
c
r
i

 

C
d
r
o
f
x
O

s
u
c
r
i

C
 
y

l
l
i

d
a
c
c
P

i

d
R

 

 
t
r
u
o
C
m
a
h
n
e
t
t
o
T

a
i
r
o
t
c
V

i

 

s
s
o
r
C
g
n
i
r
a
h
C

 

k
r
a
P
n
e
e
r
G

q
S
 
r
e
t
s
e
c
e
L

i

s
u
c
r
i

 

C
d
r
o
f
x
O

a
i
r
o
t
c
V

i

s
u
c
r
i

C
 
y

l
l
i

d
a
c
c
P

i

d
R

 

 
t
r
u
o
C
m
a
h
n
e
t
t
o
T

 

 

N
h
t
r
o
N

S
h
t
r
o
N

 

 

N
o
o
l
r
e
k
a
B

S
o
o
l
r
e
k
a
B

E

W

 
l
a
r
t
n
e
C

 
l
a
r
t
n
e
C

N
 
a
i
r
o
t
c
V

S
 
a
i
r
o
t
c
V

i

i

E
 
y

W
 
y

l
l
i

d
a
c
c
P

i

l
l
i

d
a
c
c
P

i

Figure 3 | Traversal on the London Underground. a, During the graph 
definition phase, the network writes each triple in the map to a separate 
memory location, as shown by the write weightings (green). During the 
query phase, the start station (Victoria) and lines to be traversed are 
recorded. The triple stored in each location can be recovered by a logistic 
regression decoder, as shown on the vertical axis. b, The read mode 
distribution during the answer phase reveals that read head 1 (pink) 

follows temporal links forwards to retrieve the instructions in order, 
whereas read head 2 (blue) uses content lookup to find the stations along 
the path. The degree of coloration indicates how strongly each mode is 
used. c, The region of the map used. d, The final content key used by read 
head 2 is decoded as a triple with no destination. e, The memory location 
returned by the key contains the complete triple, allowing the network to 
infer the destination (Tottenham Court Rd).

DNC learned to make a plan. As with the graph tasks, learning  
followed a curriculum that gradually increased the number of blocks on 
the board and constraints in a goal as well as the number of goals and 

the minimum number of actions needed to find a solution (Methods). 
Again, the DNC performed substantially better than LSTM (see Fig. 5 
and Extended Data Fig. 3).

a Weightings

Decode

c Board states

s
n
o
i
t
a
c
o
L

G
T
T
T
T
C
C
C
C
A
A
#
#
#
#
#

Write head

Read head 1

Read head 2

3
b
2
G

:

1
r
6
G

:

2
b
6
T

:

Time

l

1
4
T

:

4
b
5
T

:

3
r
6
T

:

1
b
2
T

:

l

2
5
T

:

2
a
1
C

:

6
r
5
C

:

1
a
3
C

:

6
a
4
C

:

l

6
1
A

:

?
T

3
a
5
A

:

3
r
4
A

:

Action

y
c
a
r
u
c
c
a
 

b Goal T constraints

6b2

4l1

5b4

6r3

2b1

5l2

n
o
i
t
a
c
ƒ
s
s
a
C

l

i

d Planned action decodings

e t-SNE location goal labels

0.9

0.7

0.5

0.3

0.1

Location average
Action frequencies

1

2

3

Action number

4

5

Figure 4 | Mini-SHRDLU analysis. a, In a short example episode, the 
network wrote goal-related information to sequences of memory locations. 
(‘G’, ‘T’, ‘C’ and ‘A’ denote goals; the numbers refer to the block number; 
‘b’, ‘a’, ‘l’ and ‘r’ denote ‘below’, ‘above’, ‘left of’ and ‘right of’, respectively.) 
The chosen goal was T (‘T?’), and the read heads focused on the locations 
containing goal T. b, The constraints comprising goal T. c, The policy 
made an optimal sequence of moves to satisfy its constraints. d, On 800 
random episodes, the first five actions that the network took for the chosen 
goal were decoded from memory using logistic regression at the time-step 
after the goal was written (box in a with arrow to c). Decoding accuracy 

4   |   N A T U R E   |   V O L   0 0 0   |   0 0   M O N T H   2 0 1 6

for the first action is 89%, compared to 17% using action frequencies 
alone, indicating that the network had determined a plan at the time of 
writing, many steps before execution. Error bars represent 5–95 percentile 
bootstrapped confidence intervals on validation data. e, Within trials, we 
average the location contents associated with each goal label into single 
vectors. Across trials, we create a dataset of these vectors and perform 
t-SNE (t-distributed stochastic neighbour embedding) dimensionality 
reduction down to two dimensions. This shows that each goal label is 
coded geometrically in the memory locations.

a

25

n
o
s
s
e
L

20

15

10

5

0

adapting network parameters. We aim to further develop DNCs to 
serve as representational engines for one-shot learning35–37, scene 
understanding38, language processing39 and cognitive mapping40, capa-
ble of intuiting the variable structure and scale of the world within a 
single, generic model.

Online Content Methods, along with any additional Extended Data display items and 
Source Data, are available in the online version of the paper; references unique to 
these sections appear only in the online paper.

DNC
LSTM

Received 5 January; accepted 19 September 2016. 
Published online 12 October 2016.

0

1

2
Learning step (106)

3

4

5

b

1.0

n
o
i
t
r
o
p
o
r
P

0.8

0.6

0.4

0.2

0

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25

Perfect
Success
Incomplete

Lesson

Figure 5 | Mini-SHRDLU results. a, 20 replicated training runs with 
different random-number seeds for a DNC and LSTM. Only the DNC was 
able to complete the learning curriculum. b, A single DNC was able to 
solve a large percentage of problems optimally from each previous lesson 
(perfect), with a few episodes solved in extra moves (success), and some 
failures to satisfy all constraints (incomplete).

Discussion
Taken together, the bAbI and graph tasks demonstrate that DNCs are 
able to process and reason about graph-structured data regardless of 
whether the links are implicit or explicit. Moreover, we have seen that 
the structure of the data source is directly reflected in the memory- 
access procedures learned by the controller. The Mini-SHRDLU prob-
lem shows that a systematic use of memory also emerges when a DNC 
learns by reinforcement to act in pursuit of a set of symbolic goals.

The theme connecting these tasks is the need to learn to represent 
and reason about the complex, quasi-regular structure embedded in 
data sequences. In each problem, domain regularities, such as the con-
ventions for representing graphs, are invariant across all sequences 
shown; on the other hand, for any given sequence, a DNC must detect 
and capture novel variability as episodic variables in memory. This 
mixture of large-scale structure and microscopic variability is generic 
to many problems that confront a cognitive agent32–34. For example, in 
visual scenes, stories and action plans, broad regularities bind together 
novel variation in any exemplar. Rooms statistically have chairs in them, 
but the shape and location of a particular chair in a room are variables. 
These variable values can be written to the external memory of a DNC, 
leaving the controller network free to concentrate on learning global 
regularities.

Our experiments focused on relatively small-scale synthetic tasks, 
which have the advantage of being easy to generate and interpret. For 
such problems, memory matrices of up to 512 locations were suffi-
cient. To tackle real-world data we will need to scale up to thousands or  
millions of locations, at which point the memory will be able to store 
more information than can be contained in the weights of the con-
troller. Such systems should be able to continually acquire knowledge 
through exposure to large, naturalistic data sources, even without 

1.  Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep 
convolutional neural networks. In Advances in Neural Information Processing 
Systems Vol. 25 (eds Pereira, F. et al.) 1097–1105 (Curran Associates, 2012).
2.  Graves, A. Generating sequences with recurrent neural networks. Preprint at 

http://arxiv.org/abs/1308.0850 (2013).

3.  Sutskever, I., Vinyals, O. & Le, Q. V. Sequence to sequence learning with neural 

networks. In Advances in Neural Information Processing Systems Vol. 27 (eds 
Ghahramani, Z. et al.) 3104–3112 (Curran Associates, 2014).

4.  Mnih, V. et al. Human-level control through deep reinforcement learning. 

Nature 518, 529–533 (2015).

5.  Gallistel, C. R. & King, A. P. Memory and the Computational Brain: Why Cognitive 

Science Will Transform Neuroscience (John Wiley & Sons, 2011).

6.  Marcus, G. F. The Algebraic Mind: Integrating Connectionism and Cognitive 

Science (MIT Press, 2001).

7.  Kriete, T., Noelle, D. C., Cohen, J. D. & O’Reilly, R. C. Indirection and symbol-like 
processing in the prefrontal cortex and basal ganglia. Proc. Natl Acad. Sci. USA 
110, 16390–16395 (2013).

8.  Hinton, G. E. Learning distributed representations of concepts. In Proc. Eighth 

Annual Conference of the Cognitive Science Society Vol. 1, 1–12 (Lawrence 
Erlbaum Associates, 1986).

9.  Bottou, L. From machine learning to machine reasoning. Mach. Learn. 94, 

133–149 (2014).

10.  Fusi, S., Drew, P. J. & Abbott, L. F. Cascade models of synaptically stored 

memories. Neuron 45, 599–611 (2005).

11.  Ganguli, S., Huh, D. & Sompolinsky, H. Memory traces in dynamical systems. 

Proc. Natl Acad. Sci. USA 105, 18970–18975 (2008).

12.  Kanerva, P. Sparse Distributed Memory (MIT press, 1988).
13.  Amari, S.-i. Characteristics of sparsely encoded associative memory. Neural 

14.  Weston, J., Chopra, S. & Bordes, A. Memory networks. Preprint at http://arxiv.

Netw. 2, 451–457 (1989).

org/abs/1410.3916 (2014).

15.  Vinyals, O., Fortunato, M. & Jaitly, N. Pointer networks. In Advances in Neural 

Information Processing Systems Vol. 28 (eds Cortes, C et al.) 2692–2700 
(Curran Associates, 2015).

16.  Graves, A., Wayne, G. & Danihelka, I. Neural Turing machines. Preprint at 

http://arxiv.org/abs/1410.5401 (2014).

17.  Bahdanau, D., Cho, K. & Bengio, Y. Neural machine translation by jointly learning 

to align and translate. Preprint at http://arxiv.org/abs/1409.0473 (2014).

18.  Gregor, K., Danihelka, I., Graves, A., Rezende, D. J. & Wierstra, D. DRAW: a recurrent 

neural network for image generation. In Proc. 32nd International Conference on 
Machine Learning (eds Bach, F. & Blei, D.) 1462–1471 (JMLR, 2015).

19.  Hintzman, D. L. MINERVA 2: a simulation model of human memory. Behav. 

Res. Methods Instrum. Comput. 16, 96–101 (1984).

20.  Kumar, A. et al. Ask me anything: dynamic memory networks for natural 

language processing. Preprint at http://arxiv.org/abs/1506.07285 (2015).
21.  Sukhbaatar, S. et al. End-to-end memory networks. In Advances in Neural 
Information Processing Systems Vol. 28 (eds Cortes, C et al.) 2431–2439 
(Curran Associates, 2015).

22.  Magee, J. C. & Johnston, D. A synaptically controlled, associative signal for 
Hebbian plasticity in hippocampal neurons. Science 275, 209–213 (1997).

23.  Johnston, S. T., Shtrahman, M., Parylak, S., Gonc¸ alves, J. T. & Gage, F. H. 

Paradox of pattern separation and adult neurogenesis: a dual role for new 
neurons balancing memory resolution and robustness. Neurobiol. Learn. Mem. 
129, 60–68 (2016).

24.  O’Reilly, R. C. & McClelland, J. L. Hippocampal conjunctive encoding, storage, 

and recall: avoiding a trade-off. Hippocampus 4, 661–682 (1994).

25.  Howard, M. W. & Kahana, M. J. A distributed representation of temporal 

context. J. Math. Psychol. 46, 269–299 (2002).

26.  Weston, J., Bordes, A., Chopra, S. & Mikolov, T. Towards AI-complete question 

answering: a set of prerequisite toy tasks. Preprint at http://arxiv.org/
abs/1502.05698 (2015).

27.  Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural Comput. 9, 

1735–1780 (1997).

28.  Bengio, Y., Louradour, J., Collobert, R. & Weston, J. Curriculum learning. In Proc. 
26th International Conference on Machine Learning (eds Bottou, L. & Littman, M.)  
41–48 (ACM, 2009).

29.  Zaremba, W. & Sutskever, I. Learning to execute. Preprint at http://arxiv.org/

abs/1410.4615 (2014).

30.  Winograd, T. Procedures as a Representation for Data in a Computer Program for 

Understanding Natural Language. Report No. MAC-TR-84 (DTIC, MIT Project 
MAC, 1971).

0 0   M O N T H   2 0 1 6   |   V O L   0 0 0   |   N A T U R E   |   5

31.  Epstein, R., Lanza, R. P. & Skinner, B. F. Symbolic communication between two 

pigeons (Columba livia domestica). Science 207, 543–545 (1980).
32.  McClelland, J. L., McNaughton, B. L. & O’Reilly, R. C. Why there are 

complementary learning systems in the hippocampus and neocortex: insights 
from the successes and failures of connectionist models of learning and 
memory. Psychol. Rev. 102, 419–457 (1995).

33.  Kumaran, D., Hassabis, D. & McClelland, J. L. What learning systems do 

intelligent agents need? Complementary learning systems theory updated. 
Trends Cogn. Sci. 20, 512–534 (2016).

34.  McClelland, J. L. & Goddard, N. H. Considerations arising from a 

complementary learning systems perspective on hippocampus and neocortex. 
Hippocampus 6, 654–665 (1996).

35.  Lake, B. M., Salakhutdinov, R. & Tenenbaum, J. B. Human-level concept learning 

through probabilistic program induction. Science 350, 1332–1338 (2015).

36.  Rezende, D. J., Mohamed, S., Danihelka, I., Gregor, K. & Wierstra, D. One-shot 

generalization in deep generative models. In Proc. 33nd International 
Conference on Machine Learning (eds Balcan, M. F. & Weinberger, K. Q.) 
1521–1529 (JMLR, 2016).

37.  Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D. & Lillicrap, T. Meta-learning 

with memory-augmented neural networks. In Proc. 33nd International 
Conference on Machine Learning (eds Balcan, M. F. & Weinberger, K. Q.) 
1842–1850 (JMLR, 2016).

38.  Oliva, A. & Torralba, A. The role of context in object recognition. Trends Cogn. 

1978).

Sci. 11, 520–527 (2007).

39.  Hermann, K. M. et al. Teaching machines to read and comprehend. In Advances 

in Neural Information Processing Systems Vol. 28 (eds Cortes, C. et al.) 
1693–1701 (Curran Associates, 2015).

40.  O’Keefe, J. & Nadel, L. The Hippocampus as a Cognitive Map (Oxford Univ. Press, 

Supplementary Information is available in the online version of the paper.

Acknowledgements We thank D. Silver, M. Botvinick and S. Legg for reviewing 
the paper prior to submission; P. Dayan, D. Wierstra, G. Hinton, J. Dean, N. 
Kalchbrenner, J. Veness, I. Sutskever, V. Mnih, A. Mnih, D. Kumaran, N. de Freitas, 
L. Sifre, R. Pascanu, T. Lillicrap, J. Rae, A. Senior, M. Denil, T. Kocisky, A. Fidjeland, 
K. Gregor, A. Lerchner, C. Fernando, D. Rezende, C. Blundell and N. Heess for 
discussions; J. Besley for legal assistance; the rest of the DeepMind team for 
support and encouragement; and Transport for London for allowing us to 
reproduce portions of the London Underground map.

Author Contributions A.G. and G.W. conceived the project. A.G., G.W., M.R., 
T.H., I.D., S.G. and E.G. implemented networks and tasks. A.G., G.W., M.R., T.H., 
A.G.-B., T.R. and J.A. performed analysis. M.R., T.H., I.D., E.G., K.M.H., C.S., P.B., 
K.K. and D.H. contributed ideas. A.C. prepared graphics. A.G., G.W., M.R., T.H., 
S.G., A.P.B., Y.Z., G.O. and K.K. performed experiments. A.G., G.W., H.K., K.K. 
and D.H. managed the project. A.G., G.W., M.R., T.H., K.K. and D.H. wrote the 
paper.

Author Information Reprints and permissions information is available at  
www.nature.com/reprints. The authors declare no competing financial  
interests. Readers are welcome to comment on the online version of the  
paper. Correspondence and requests for materials should be addressed  
to A.G. (gravesa@google.com), G.W. (gregwayne@google.com),  
D.H. (demishassabis@google.com).

Reviewer Information Nature thanks Y. Bengio, J. McClelland and the  
other anonymous reviewer(s) for their contribution to the peer review of  
this work.

6   |   N A T U R E   |   V O L   0 0 0   |   0 0   M O N T H   2 0 1 6

METHODS
A glossary of symbols and full equations for the DNC model are provided in 
Supplementary Information.
Controller network. At every time-step t the controller network N  receives an 
input vector xt ∈  RX from the dataset or environment and emits an output vector 
yt ∈  RY that parameterizes either a predictive distribution for a target vector zt ∈  RY 
(supervised learning) or an action distribution (reinforcement learning). 
Additionally, the controller receives a set of R read vectors  …−
1 from the 
,
memory matrix Mt−1 ∈  R ×N W at the previous time-step, via the read heads. It then 
emits an interface vector ξt that defines its interactions with the memory at the 
current time-step. For notational convenience, we concatenate the read and input 
vectors to obtain a single controller input vector χ =
1 . Any 
]
neural network can be used for the controller, but we have used the following 
variant of the deep LSTM architecture41:

1
…−
t
1

x r
;
t

R
t
−

R
t
−

,
1

1
t

r

r

r

[

;

;

t

l
i
t
l
f
t
l
s
t
l
o
t
l
h
t

=
=

=

=
=

;

;

1

−

l
l
l
l
1
−
b
h
h
W
]
)
(
[
+
σ
χ
i
i
t
t
t
−
l
l
l
l
1
−
h
b
h
W
]
;
;
(
[
+
σ
χ
f
f
t
t
1
t
l
l
l
l
l
h
i
f s
W
[
tanh(
;
χ
+
s
t
t
t
1
t
−
t
l
l
l
l
1
−
b
h
h
W
[
;
]
;
(
σ
+
χ
o
o
t
t
t
l
l
o
s
tanh( )
t
t

−

1

)

−
)

;

1

h

l
−
t

1
]

+

l
b
s

)

l, st

l,  f t

l and ot

where l is the layer index, σ(x) =  1/(1 +  exp(− x)) is the logistic sigmoid function, 
l are the hidden, input gate, forget gate, state and output gate 
l, it
ht
l
activation vectors, respectively, of layer l at time t.  =h
h
 for 
0
l is the matrix 
all l. The W terms denote learnable weight matrices (for example, Wi
of weights going into the layer-l input gates) and the b terms are learnable biases.
At each time-step, the controller emits an output vector υt and an interface 

 for all t;  = =

0

0

l
0

0
t

s

vector ξt ∈  R(W×R)+3W+5R+3, defined as

t

υ
ξ

t

=
=

1
h
W
[
…
y
t
1
h
W
[
…ξ
t

;
;

L
h
;
t
L
h
;
t

]
]

Assuming the controller network is recurrent, its outputs are a function of the 
complete history (χ1, …, χt) of its inputs up to the current time-step. We can 
therefore encapsulate the operation of the controller as

(

,
υ ξ

t

)

t

=

N

([

χ
1

;

…

;

χ θ

];

t

)

where θ is the set of trainable network weights. It is also possible to use a feedfor-
ward controller, in which case N  is a function of χt only; however, we use only 
recurrent controllers in this paper. Finally, the output vector yt is defined by adding 
υt to a vector obtained by passing the concatenation of the current read vectors 
through the RW ×  Y weight matrix Wr

y
t

υ= +

t

W[
r

r

1
t

;

…

;

r

R
t

]

This arrangement allows the DNC to condition its output decisions on memory 
that has just been read; it would not be possible to pass this information back to 
the controller, and thereby use them to determine υ, without creating a cycle in 
the computation graph.
Interface parameters. Before being used to parameterize the memory interactions, 
the interface vector ξt is subdivided as follows:

ξ

t

= 



r,1
k
t

;

…

;

R

r,
k
t

;

r,1
ˆ
β
t

;

…

;

R

ˆ
β

r,
t

;

w
k
t

;

w
ˆ
β
t

;

ˆ
ˆ
e v f
;
t

;

t

1
t

;

…

;

ˆ
f

R
t

;

ˆ
g

a
t

;

ˆ
g

w
t

;

1
ˆ
π
t

;

… 
R
ˆ
π

t


;

The individual components are then processed with various functions to ensure 
that they lie in the correct domain. The logistic sigmoid function is used to con-
strain to [0, 1]. The ‘oneplus’ function is used to constrain to [1, ∞ ), where

oneplus( ) 1

= +

x

log (1

+

e )x

and the softmax function is used to constrain vectors to SN, the N −  1-dimensional 
unit simplex

S

N

=






α

∈

N

R

:

α
i

∈

[0,1],

N
∑
i
1
=

α
i

=



1



After processing we have the following set of scalars and vectors:

k

•  R read keys 
{

W
R∈
r,
•  R read strengths 
{
β
t

i

i

r,
t

; 1

≤ ≤

i R

=

oneplus(

;
}
ˆ
β

i
)

r,
t

∈ ∞ ≤ ≤i R

) ; 1

[1,

;
}

oneplus(

ˆ
β

w
t

)

;
)
∈ ∞

[1,

∈

[0,1]

W;

≤ ≤

i R

;
}

[0,1]; 1
a
t
∈

∈

)

[0,1]
i
ˆ
π
t

)

;
[0,1]

; and

g
ˆσ=
g(

w
t

)

w
t
π

;

=

the write key 

e
the erase vector 

the write strength 

w
W
R∈kt
w
β
t
ˆσ=
e(
t
the write vector vt ∈  RW;
i
)
∈
t
ˆσ=
g(

the allocation gate 

ˆσ=
f
(
a
t

)

i
t

f

t

the write gate 

g

•  R free gates 
{

• 

• 

• 

• 

• 

• 

•  R read modes 
{

i
t

=

softmax(

∈

S

3

; 1

≤ ≤

i R

.
}

The use and interpretation of these terms will be explored in the following 

sections.
Reading and writing to memory. Selecting locations for reading and writing 
depends on weightings, which are vectors of non-negative numbers whose ele-
ments sum to at most 1. The complete set of allowed weightings over N locations 
is the non-negative orthant of RN with the unit simplex as a boundary (known as 
the ‘corner of the cube’):

Δ =

N






α

∈

N

R

:

α
i

∈

[0,1],

N
∑
i
1
=

α
i

≤



1



For the read operation, R read weightings 
 are used to compute 
}
{
weighted averages of the contents of the locations, thereby defining the read vectors 
1
…r
{ ,
t

 as
}

Δ
N

…

∈

R
t

w

w

r

,

,

r,1
,
t

R

r,
t

r

i
= 
wMt
t

ir,
t

The read vectors are appended to the controller input at the next time-step, giving 
it access to the memory contents. The write operation is mediated by a single write 
, which is used in conjunction with an erase vector et ∈  [0, 1]W 
weighting 
and a write vector vt ∈  RW (both emitted by the controller) to modify the memory 
as follows:

Δ∈wt
N

w

M M

=

t

− (cid:31)
1

E(

t

−

w
w e
t


)
t

+

w
w v
t


t

where (cid:31) denotes element-wise multiplication and E is an N ×  W matrix of ones. 
The computation of the read and write weightings is detailed in the following 
section.
Memory addressing. The system uses a combination of content-based addressing 
and dynamic memory allocation to determine where to write in memory, and a 
combination of content-based addressing and temporal memory linkage to deter-
mine where to read. These mechanisms, all of which are parameterized by the 
interface vectors defined in Methods section ‘Interface parameters’, are described 
in detail below.
Content-based addressing. All content lookup operations on the memory 
M ∈  R ×N W use the following function

C

(

M

,

k

,

β

i
)[ ]

=

exp{
D
exp{
∑
j

k
M i
( ,
[ , ]) }
β
⋅
k
M j
( ,
[ , ]) }
β
D

⋅

where k ∈  RW is a lookup key, β ∈  [1, ∞ ) is a scalar representing key strength and  
D is the cosine similarity:

D u v
( , )

=

u v
⋅
u v

The weighting C(M, k, β) ∈  SN defines a normalized probability distribution over 
the memory locations. In later sections, we will encounter weightings in Δ N that 
may sum to less than one, with the missing weight implicitly assigned to a null 
operation that does not access any of the locations. Content lookup operations are 
performed by both the read and write heads.
Dynamic memory allocation. To allow the controller to free and allocate memory 
as needed, we developed a differentiable analogue of the ‘free list’ memory alloca-
tion scheme42, whereby a list of available memory locations is maintained by add-
ing to and removing addresses from a linked list. Denote by ut ∈  [0, 1]N the 
memory usage vector at time t, and define u0 =  0. Before writing to memory, the 
i, one per read head, that determine whether 
controller emits a set of free gates f t

the most recently read locations can be freed. The memory retention vector  
ψt ∈  [0, 1]N represents by how much each location will not be freed by the free gates, 
and is defined as

i
f
t
i
b
t

=

=

L
L

i
r,
w
t
t
1
−
i
r,

w
t
t
−

1

(
1
The usage vector can then be defined as

R
∏ψ =
t
1
=

i

−

f

w

i
r,
t
−

i
t

)

1

u

t

=

u

(

t

−

1

+

w

w
t
−

1

−

u

t

−

1

w
w
−(cid:31)
t

)
1

(cid:31)

ψ
t

where (cid:31) denotes element-wise multiplication. Intuitively, locations are used if they 
have been retained by the free gates (ψt[i] ≈  1), and were either already in use or 
have just been written to. Every write to a location increases its usage, up to a 
maximum of 1, and usage can only be subsequently decreased (to a minimum of 
0) using the free gates; the elements of ut are therefore bounded in the range [0, 1]. 
Once ut has been determined, the free list φt ∈  ZN is defined by sorting the indices 
of the memory locations in ascending order of usage; φt[1] is therefore the index 
of the least used location. The allocation weighting at ∈  Δ N, which is used to pro-
vide new locations for writing, is

a

t

[

φ

t

j
[ ]]

= −

(1

u

t

[

φ

t

j
[ ]])

j
1
−
∏
i
1
=

u

t

[

φ

t

i
[ ]]

(1)

If all usages are 1, then at =  0 and the controller can no longer allocate memory 
without first freeing used locations. The sort operation induces discontinuities at 
the points at which the sort order changes. We ignore these discontinuities when 
calculating the gradient, as they do not seem to be relevant to learning.
Write weighting. The controller can write to newly allocated locations, or to loca-
tions addressed by content, or it can choose not to write at all. First, a write content 
weighting  ∈ S

 is constructed using the write key kt

w and write strength βt
w:

w
ct

N

c

w
t

=

C

kM(
,
1

−

t

w
t

,

β

w
t

)

w is interpolated with the allocation weighting at defined in equation (1) to deter-
ct
mine a write weighting 

:

w
Δ∈wt
N
w a
= 
w
g g

t
t

w
t

a

t

+ −

(1

g

a w
c
)
t
t




(2)

a
t

w
t

[0,1]

 is the allocation gate governing the interpolation and  ∈g

where  ∈g
 
[0,1]
is the write gate. If the write gate is 0, then nothing is written, regardless of the other 
write parameters; it can therefore be used to protect the memory from unnecessary 
modifications.
Temporal memory linkage. The memory allocation system defined above stores 
no information about the order in which the memory locations are written to. 
However, there are many situations in which retaining this information is useful: 
for example, when a sequence of instructions must be recorded and retrieved in 
order. We therefore use a temporal link matrix Lt ∈  [0, 1]N×N to keep track of con-
secutively modified memory locations (Fig. 1d).

Lt[i, j] represents the degree to which location i was the location written to 
after location j, and each row and column of Lt defines a weighting over locations: 
Lt[i, ·] ∈  Δ N and Lt[·, j] ∈  Δ N for all i, j and t. To define Lt, we require a precedence 
weighting pt ∈  Δ N, where element pt[i] represents the degree to which location  
i was the last one written to. pt is defined by the recurrence relation

p
0

=

p
t

=

0




1

w

w
t

i
[ ]

−

∑
i





p
t

+−
1

w

w
t

w is the write weighting defined in equation (2). Every time a location is 
where wt
modified, the link matrix is updated to remove old links to and from that location. 
New links from the last-written location are then added. We use the following 
recurrence relation to implement this logic:

i j
L i j
[ , ] 0
,
=
∀
0
i
L i i
[ , ] 0
∀
=
t
w
L i j
[ , ]
(1
= −
t

w
t

i
[ ]

−

w

w
t

j L
[ ])

i j
[ , ]
1

t

−

+

w

w
t

i
[ ]

p
t

j
[ ]
1

−

Self-links are excluded (the diagonal of the link matrix is always 0) because it is 
unclear how to follow a transition from a location to itself. The rows and columns 
of Lt represent the weights of the temporal links going into and out from particular 
memory slots, respectively. Given Lt, the backward weighting 
N and forward 
weighting 

N for read head i are defined as

i
Δ∈bt

i
Δ∈f t

 is the ith read weighting from the previous time-step.
1

i
r,
where  −wt
Sparse link matrix. The link matrix is N ×  N and therefore requires O(N2) resources 
in both memory and computation to calculate exactly. Although tolerable for the 
experiments in this paper, this cost rapidly becomes prohibitive as the number of 
locations increases. Fortunately, the link matrix is typically very sparse and can 
be approximated with O(NlogN) computation cost and O(N) memory with no 
discernible loss in performance (see Extended Data Fig. 4 for an example).

For some fixed K, we first calculate sparse vectors  ˆwt

w and  ˆ −pt 1 by sorting wt
w 
and pt−1, setting all but the K highest values to 0, and dividing the remaining K by 
their sum to ensure that they sum to 1. This step has O(NlogN +  K) computational 
cost to account for the sort and O(N) memory cost. We then compute the sparse 
, which requires O(K2) memory and computation. Assuming 
outer product ˆ
the sparse link matrix ˆ −Lt 1 from the previous time-step has at most NK non-zero 
elements, ˆLt can be updated with O(NK) cost using
i j
[ , ]
1

ˆw pt
w
t

ˆ
L i j
[ , ]
t

= −

ˆ
j L
[ ])

i
[ ]

i
[ ]

(1

−

+

−

t

ˆ
w

w
t

ˆ
p
t

j
[ ]
1

−

ˆ
w

w
t

ˆ
w

w
t

and then setting all elements of ˆLt that are less than 1/K to zero. Because each row 
and column of ˆLt sums to at most 1, this operation guarantees that ˆLt has at most 
K non-zero elements per row and column and therefore at most NK non-zero 
elements. Finally, the forward and backward weightings can be calculated with 
O(NK) computation cost and O(N) memory cost as follows:

f

i
t
i
b
t

=

=

ˆ
L
ˆ
L

i
r,
w
t
t
1
−

i
r,
w
t
t
−

1

Because K is a constant that is independent of N (in practice K =  8 appears to be 
sufficient, regardless of memory size), the complete sparse update is O(NlogN) in 
computation and O(N) in memory.
Read weighting. Each read head i computes a content weighting 
read key 

r,
Δ∈ct
N

 using a 

r,
W
R∈kt

:

i

i

i

c

r,
t

= C

kM(
,

t

i
,

r,
t

β

i
)

r,
t

Each read head also receives a read mode vector π ∈ St
i, the forward weighting f t
among the backward weighting bt
ir,
wt
weighting ct
f
[3]

ir, , thereby determining the read weighting 
i
t

[2]

[1]

i
b
t

w

=

+

+

π

π

π

r,
t

r,
t

c

i
t

i
t

i

i

i
t

i

3, which interpolates  
i and the content read  
∈ S

3:

i
t

i
t

i
t

 dominates the read mode, then the weighting reverts to content lookup 
ir, . If π [3]
 dominates, then the read head iterates through memory locations 
 dominates, then the 

If π [2]
using kt
in the order they were written, ignoring the read key. If π [1]
read head iterates in the reverse order.
Comparison with the neural Turing machine. The neural Turing machine16 
(NTM) was the predecessor to the DNC described in this work. It used a similar 
architecture of neural network controller with read–write access to a memory 
matrix, but differed in the access mechanism used to interface with the mem-
ory. In the NTM, content-based addressing was combined with location-based 
addressing to allow the network to iterate through memory locations in order of 
their indices (for example, location n followed by n +  1 and so on). This allowed 
the network to store and retrieve temporal sequences in contiguous blocks of 
memory. However, there were several drawbacks. First, the NTM has no mecha-
nism to ensure that blocks of allocated memory do not overlap and interfere—a 
basic problem of computer memory management. Interference is not an issue for 
the dynamic memory allocation used by DNCs, which provides single free loca-
tions at a time, irrespective of index, and therefore does not require contiguous 
blocks. Second, the NTM has no way of freeing locations that have already been 
written to and, hence, no way of reusing memory when processing long sequences. 
This problem is addressed in DNCs by the free gates used for de-allocation. Third, 
sequential information is preserved only as long as the NTM continues to iterate 
through consecutive locations; as soon as the write head jumps to a different part 
of the memory (using content-based addressing) the order of writes before and 
after the jump cannot be recovered by the read head. The temporal link matrix 
used by DNCs does not suffer from this problem because it tracks the order in 
which writes were made.
bAbI task descriptions. The bAbI dataset26 comprises a set of 20 synthetic 
question answering tasks that are designed to test different aspects of logical 

reasoning. Because the bAbI data are programmatically generated, and the code 
is publicly available, multiple versions of the data can be used. For our exper-
iments we used the en-10k subset of the data available for download from  
http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz. For each 
of the 20 tasks, the data comes partitioned into a training set with 10,000 questions 
and a test set with 1,000 questions21. The bAbI tasks are designed around stories 
that may contain more than one question; we treated each story as a separate 
sequence and presented it to the network in the form of word vectors, one word at 
a time. After removing all numbers, splitting the remaining text into words, and 
converting all words to lower case, there were 156 unique words in the lexicon and 
three punctuation symbols: ‘ . ’ , ‘?’ and ‘-’, the last of which we added to indicate 
points in the input sequence where outputs were required. Each word was therefore 
represented as a size-159 one-hot vector, and the network outputs were size-159 
softmax distributions. The sentences were separated by full stop characters, and 
all questions were delimited by a question mark followed by as many dash charac-
ters as there were words in the answer to that question. For example, a story from 
the ‘Counting and Lists/Sets’ task containing five questions was presented as the 
following input sequence of 69 word tokens:

mary journeyed to the kitchen. mary moved to the bedroom. john went 
back to the hallway. john picked up the milk there. what is john carrying ? 
- john travelled to the garden. john journeyed to the bedroom. what is john 
carrying ? - mary travelled to the bathroom. john took the apple there. what 
is john carrying ? - -

The answers required at the ‘− ’ symbols, grouped by question into braces, are

{milk}, {milk}, {milk apple}

The network was trained to minimize the cross-entropy of the softmax outputs 
with respect to the target words; the outputs during time-steps when no target was 
present were ignored. For each step where a target was present, the most probable 
word in the network’s output distribution was selected as its answer. The network 
was considered to have correctly replied to a question only if it got all of the tar-
get words correct (for example, it had to answer “milk” then “apple” to get the 
final question in the above story right). Following previous work, we evaluated 
our networks using the per-task ‘question error rate’ (the fraction of incorrectly 
answered questions).

For each task, we removed approximately 10% of the stories and added them to 
a validation set. All of the remaining stories were gathered together into a single 
training set from which a random story was drawn for each training sample. No 
distinction was drawn between the different tasks during training, and no explicit 
information was provided to the network to indicate which task the current story 
was drawn from. We performed a grid search over experimental hyper-parameters 
for all three architectures, and kept the two settings that returned (1) the lowest 
average question error rate over the validation set and (2) the single network with 
the lowest validation question error rate. We also used the validation error rate 
as the early stopping criterion during training (although in practice we did not 
observe a substantial increase in validation error due to overfitting for any of the 
networks).

Using word tokens led to much longer sequences than previous work on bAbI, 
for which sentence embeddings were used as input21,26. The distinction is notable in 
that it places greater stress on the long-range memory capacity of the models, and 
in that the word-level approach is easier to generalize to natural language, which 
has far greater variability in sentence length and structure than the bAbI data.

For complete results and hyper-parameters on all the bAbI tasks for DNC, NTM 

and LSTM, see Extended Data Tables 1 and 2.
Graph task descriptions. The graph tasks were supervised learning problems 
with each training example consisting of an input vector sequence and corre-
sponding target vector sequence. Each vector encoded a triple consisting of a 
source label, an edge label and a destination label. All labels were represented 
as numbers between 0 and 999, with each digit represented as a 10-way one-hot 
encoding. We reserved a special ‘blank’ label, represented by the all-zero vector 
for the three digits, to indicate an unspecified label. Each label required 30 input 
elements, and each triple required 90. The sequences were divided into multiple 
phases: first a graph description phase, then a series of query and answer phases; in 
some cases the query and answer were separated by an additional planning phase 
with no input, during which the network was given time to compute the answer. 
During the graph description phase, the triples defining the input graph were 
presented in random order. Target vectors were present during only the answer 
phases. The input vectors had additional binary channels alongside the triples 
to indicate when transitions between the different phases occurred, and when a 
prediction was required of the network (this channel remained active throughout 
the answer phase). In total, the input vectors were size 92 and the target vectors 

were size 90. The graph networks had 90 output units, corresponding to nine 
separate softmax distributions over the ten digits. The log-probability of correctly 
predicting an entire target triple was therefore the sum of the log-probabilities 
of correctly classifying each of the nine digits. Given input sequence x, network 
output sequence y and target sequence z, all of length T, this yields the following 
cross-entropy loss function:

L x z
( , )

= −

T
∑
t
1
=






A t
( )

9
∑
d
0
=

log[Pr(

d
z y
|
t

d
t

)]






d is the target at time t for digit d, yt

d is the softmax distribution over digit 
where zt
d returned by the network at time t, and A(t) is an indicator function whose value 
was 1 during answer phases (that is, when predictions were required of the net-
work) and 0 otherwise.

The network’s predictions were determined by taking the mode of the output 
distribution; the network indicated that it had completed an answer by outputting 
a specially reserved termination pattern. For all tasks apart from shortest-path task, 
performance was evaluated as the fraction of sequences for which all target vectors 
were correctly predicted. The metric used for the shortest-path task is described 
in Methods section ‘Structured prediction’.
Random graph generation. For all graph tasks, the graphs used to train the net-
works were generated by uniformly sampling a set of two-dimensional points 
from a unit square, each point corresponding to a node in the graph. For each 
node, the K nearest neighbours in the square were used as the K outbound con-
nections, with K independently sampled from a uniform range for each node. The 
numerical labels for the nodes were chosen uniformly from the range [0, 999]. 
For the shortest-path and traversal problems, the edge labels were unique per 
outbound node, but non-unique across the graph. This meant that the network 
had to search the graph for a node and edge label simultaneously to pinpoint a 
particular triple, which made following paths much more difficult than if it had 
to search for only an edge. For a graph with N nodes, N unique numbers in the 
range [0, 999] were initially drawn. Then, the outbound edge labels for each node 
were chosen at random from those N numbers. The edge labelling procedure for 
the inference task is described below.
Traversal. A path on the graph was defined on the basis of a random walk from 
a random start node. At the query phase, the first input to the network was an 
incomplete triple with the destination unspecified (source label, edge label, _). The 
input triples for the rest of the query contained only edge labels, with source and 
destination unspecified. During the answer phase, no inputs were presented and 
the target output was the sequence of complete triples along the path. To succeed, 
the network had to infer the destination of each triple, and remember it as the 
implicit source for the next triple.
Shortest path. In the query phase, a single incomplete triple was presented, defining 
the start and end nodes (source, _, destination). Each query was followed by a 
10-time-step planning phase, allowing the network to perform computations and 
to attempt to determine a shortest path. During the answer phase, the network 
emitted a sequence of triples corresponding to a path. Unlike the traversal task, 
it also received input triples during the answer phase, indicating the actions cho-
sen on the previous time-step. This makes the problem a ‘structured prediction’ 
problem, which we explain further in Methods section ‘Structured prediction’. As 
described therein, the input triples were sometimes the network’s own predictions 
from the previous time-step, and during training were sometimes provided by an 
optimal planner recalculating a shortest path to the end node. To ensure that the 
network always moved to a valid node, the output distribution was renormalized 
over the set of possible triples outgoing from the current node. The performance 
measure was the fraction of sequences for which the network found a minimally 
short path.
Inference. We define a ‘relation’ to be a concatenation of two or more edge labels 
that is given a distinct label; the label therefore acts as a kind of alias for the 
sequence. For the inference task, numbers from 0 to 9 indicated single edge labels 
and numbers from 10 to 410 indicated relation labels. The relations were generated 
as unique sequences of single edges of length 2–5, with 100 distinct sequences for 
each length. The sequences and labels were fixed for all networks trained on the 
task. During the query phase, an incomplete triple was presented, consisting of a 
start node and a relation label (start node, relation label, _). This was followed by 
a 10-time-step planning phase. The single target vector during the answer phase 
was the completed triple from the query: (start node, relation label, end node). 
To solve the problem the network had to infer the relation sequence from its label 
and perform an implicit traversal along that sequence during the planning phase 
to reach the destination. The relations were never passed as input the network, so 
it had to infer them from error signals alone.

Structured prediction. The shortest-path task can be considered a structured 
prediction problem43,44, because the output decisions of the network determine 
the sequence of nodes traversed on the graph and, hence, influence the future 
decisions of network and the prediction costs. To bring nomenclature in line with 
the literature on this topic, we refer to the output distribution of the network as a 
policy π(a |  s) over the actions a available to the network in state s. The state incor-
porates both the node currently occupied by the network and the latent state of the 
network itself. The actions are the outgoing edges from the current node (recall 
that the output distribution is renormalized over the allowed triples after each 
move). Following policy π, the induced distribution over states at time-step t is 
denoted ρπ s( )
. We denote the optimal policy as π*(a |  s) with corresponding state 
distribution  ⁎ρ s( )
t

. The conventional supervised loss is

t

J

=

sup

T
∑π
( )
t
1
=
(
⋅ | = − ∑
π

)]

s

E

s

t

ρ

~

⁎
s
( )
t

l

[

π

⁎
(
⋅ |

s

),

t

π

(
⋅ |

s

)]

t

⁎
(

⁎
(
⋅ |

t

t

t

a

s

[

π

π

),

a s
|

)log [ (
π

where 
 is the cross-entropy loss. 
l
π*(a |  st) is a delta function on the action that corresponds to the first step along  
one possible shortest path from the current node to the destination (there may  
be more than one). If the state distributions  ⁎ρ s( )
 are dissimilar, then 
minimizing the supervised loss function does not necessarily transfer to the true 
task objective

 and ρπ s( )

a s
|

)]

t

t

t

task

J

T
∑π
( )
1
=

=

t

E

s

t

π~
ρ
t

s
( )

l

[

π

⁎
(
⋅ |

s

),

t

π

(
⋅ |

s

)]

t

t

where the actions are sampled from the network policy and the states are therefore 
drawn from ρπ s( )
. To counteract this problem, we follow a similar approach to the 
DAGGER algorithm43, which constructs a mixture policy πβ(a |  s) =  βπ*(a |  s) +   
(1 −  β)π(a |  s) with parameter β and induced state distribution ρβ s( )
. To simplify 
the implementation, we used a batch size of 1 and trained by taking a stochastic 
gradient of

t

β

J

T
∑π
( )
1
=

=

t

E

s

t

β~
ρ
t

s
( )

l

[

π

⁎
(
⋅ |

s

),

t

π

(
⋅ |

s

)]

t

where the actions are sampled from the network with probability (1 −  β) and from 
the optimal policy with probability β. To make transitions driven by the network 
output, all possible edges connected to a source node are assigned a probability, 
and the most likely edge is chosen.
Reinforcement learning. Most reinforcement learning problems are sequential 
decision problems: the environment is in state s and each action a issued by the 
agent causes a transition of the environment state on the basis of the environment 
dynamics. We consider episodic problems whereby the agent acts in the envi-
ronment for T steps before the environment is reset and a new episode begins. 
The agent thus acts to create a time series s1, a1, s2, a2, s3, a3, …, sT, aT. A reward 
function that defines the goal of the problem is given as a function of a state and 
an action: r(st, at). The goal of the agent is to maximize the total expected reward 
over an episode. The architecture of the reinforcement learning agent presented 
here contains two DNC networks: a policy network that selects an action and a 
value network that estimates the expected future reward given the policy network 
and current state.

T
= ∑
t
=

The policy specifies a parametric mapping from state observations to a  
probability distribution over actions: a ∼  π(· |  s; θ), where θ denotes the policy 
parameters. The total expected reward of the policy over an episode is 
J
. In the context of Mini-SHRDLU, the policy-network 
]
( )
π
DNC observes the environment states by receiving an observation sequence  
o1, o2, …, ot, one step at a time and conditions its action on the sequence seen so 
far: π(· |  o1, …, ot; θ). The value-network DNC tries to predict the sum of future 
rewards for this policy given the current history of observations: V π(o1, …, ot; φ), 
where φ comprises its parameters.

The learning algorithm for the value network is conceptually simpler than that 
of the policy network. The value network learns by supervised regression to predict 
the sum of future rewards. After a mini-batch of L episodes, the value network 
updates its parameters φ using gradient descent on the loss function

r s a
( ,
t

E

π

)

[

|

1

t

C

( )
φ

=

1
L
2

T

T
L
∑ ∑ ∑
t
l
1
=
= =

1

τ

t

r s a
(
,

l
τ

l
τ

)

−

π
V o
,
(
1

,
…

o
τ

;

φ

)

2

The action distribution of the policy network π(at |  o1, …, ot; θ) is a softmax over 
a discrete set of actions. We use a policy gradient method to optimize the policy 
parameters45,46. After each mini-batch of L episodes, the policy parameter gradient 
direction to ascend J(π) is

∇
θ

J

( )
π

≈

1
L






E

t

T
L
∑ ∑
l
1
1
= =

T

∑



t
≥
τ

∇
θ

log (
π




l
l
a o
,
t
1

,
| …

l
o
t

; )
θ




r s a
(
,

l
τ

l
τ

)

l
s a
,
t

l
t







T
∑
t
≥
τ

r s a
(
,

l
τ

l
τ

)

s

l
t













E

−







T
∑

τ



≥






l
τ

E

r s a
(
,


l ,  known  as  the  

t


l in  
l. Using the value network, we can approximate the advantage using the 

T
The  quantity  ∑
τ
advantage, represents the amount that the value changes from taking action at
state st
temporal difference error

r s a
(
,

l
s a
,
t

−

E

l
τ

l
τ

l
τ






≥

)

)

s

l
t

|

|

t

t

δ

l
t

=

l
r s a
( ,
t

l
t

)

+

l
π
V o
,
(
1

…

,

l
o
t

+

1

;

φ

)

−

l
π
V o
,
(
1

…

,

l
o
t

;

φ

)

We use a slight modification of this expression for the advantage47 to reduce the 
bias in the value networks. The advantage is estimated using a geometric series of 
l , where λ is a parameter that controls a 
temporal difference errors 
τ
bias-variance trade-off between estimates based on the empirical return versus the 
parametric value function. Finally, the policy gradient estimate is

∑τ

λ

δ

≥

−

τ

t

t

∇
θ

J

( )
π

≈

1
L

T
L
∑ ∑
l
1
1
= =

t

∇
θ

log[ (
π

l
l
a o
,
t
1

,
| …

l
o
t

; )]
θ

T
∑
t
=
τ

τ

−

t

λ

δ

l
τ

Mini-SHRDLU. The Mini-SHRDLU board consists of an S ×  S grid, each square 
of which is either empty or filled with a numbered block. We report experiments 
with S =  3 and a maximum of 6 blocks on the board, uniquely numbered 1–6. To 
generate a problem instance, we first randomly place the blocks on the board so 
that a block always rests on top of the highest block previously placed in its column. 
A sequence of G goals is generated, each composed of a number of constraints. An 
example of a single goal is: block 1 is below block 4; block 2 is to the right of block 
5; block 3 is above block 4; and so on. Each goal represents a label for a set of con-
straints on the adjacency relations of the blocks. A goal can be ambiguous in that 
it does not describe a unique board configuration. For example, a goal consisting 
of only “block 1 is left of block 2” allows any configuration of the unstated blocks. 
Each goal is chosen by constructing a tree search of all configurations of the board 
that are at minimum D moves away from the starting board, randomly selecting 
one of these configurations, and then sampling a set of constraints on the chosen 
board configuration. Redundant conditions such as “block 1 is left of block 2; block 
2 is right of block 1” are pruned, as are constraints that are already fulfilled by the 
initial state of the board.

The goals are presented sequentially to the policy network during which time 
the policy cannot make any moves on the board. Each constraint in the goal is 
presented in one time-step using a place-coded vector: (first block, adjacency rela-
tion, second block). For example, (100000, 1000, 010000) represents the constraint 
“block 1 is above block 2”. In addition, each constraint is labelled with the goal of 
which it is a part: (goal name, first block, adjacency relation, second block), where 
we have chosen to let the goals be 1 of 26 possible letters designated by one-hot 
encodings; that is, A =  (1, 0, …, 0), Z =  (0, 0, …, 1) and so on. The board is rep-
resented as a set of place-coded representations, one for each square. Therefore, 
(000000, 100000, …) designates that the bottom, left-hand square is empty, block 
1 is in the bottom centre square, and so on. The network also sees a binary flag that 
represents a ‘go cue’. While the go cue is active, a goal is selected from the list of 
goals that have been shown to the network, its label is retransmitted to the network 
for one time-step, and the network can begin to move the blocks on the board. All 
told, the policy observes at each time-step a vector with features (goal name, first 
block, adjacency relation, second block, go cue, board state). Up to 10 goals with  
6 constraints each can be sent to the network before action begins.

Once the go cue arrives, it is possible for the policy network to move a block 
from one column to another or to pass at each turn. We parameterize these actions 
using another one-hot encoding so that, for a 3 ×  3 board, a move can be made 
from any column to any other; with the pass move, there are therefore 7 moves. 
The policy’s outputs define the probability of selecting each one of these actions, 
and a move is sampled at each time-step, which changes the board configuration 
correspondingly. The policy has a fixed number of moves to make progress on 
the board until the episode ends. In this setting, we can determine the minimum 
number of moves L required to solve the problem. We found that the early stages 
of learning benefited from giving the policy a number of extra steps to satisfy the 
instructions, in total allowing L +  Δ L moves with Δ L fixed at 6 in the reported 
experiments. This parameter did not need to be fine-tuned.

The reward function for the policy equalled the number of constraints in the 
chosen goal that are currently satisfied minus a small cost for making invalid moves 
such as picking a block up from a column without any blocks. There was also a 
penalty for achieving the goal configuration, but then undoing it. In addition, the 

policy received an extra reward that promoted higher-entropy output distributions 
and encouraged exploration45.
Curriculum learning. We found that curriculum learning28 was essential for all of 
our experiments apart from those on the bAbi dataset. For each task, we selected 
a set of task parameters governing the complexity of the problem. For example, 
for the traversal task, these parameters were the number of nodes in the graph, 
the number of outgoing edges from each node and the number of steps in the 
traversal. We built a linear sequence of lessons in which the complexity of the 
task increases along at least one task parameter with each new lesson. Consistent 
with the observations of Zaremba and Sutskever29, we found in some tasks that 
performance on earlier lessons degraded if all training exemplars were drawn only 
from the current lesson. We followed their strategy to remedy this effect and drew 
10% of exemplars uniformly from earlier lessons on the traversal, inference and 
Mini-SHRDLU problems. (Shortest-path lessons effectively include earlier lessons 
as proper subsets of the final lessons, so this is unnecessary.) After a predefined 
number of gradient updates, we ran a batch evaluation of the success of the network 
on the current problem distribution. During evaluation in each episode, on the 
graph problems, we deterministically sampled the most probable outputs of the 
network (modal sampling). For the graph problems besides shortest-path, if 90% of 
the episodes were solved optimally, the lesson was completed. (Shortest-path lesson 
completion required 80% of network paths to be no more than one step longer than 
the shortest path.) In the Mini-SHRDLU problem, the lesson was marked complete 
if 85% of the relevant goal constraints were satisfied on average over the batch. The 
curricula for the tasks are presented in Extended Data Tables 3–6.
Network analysis. In Fig. 3a, the y-axis labels are the input triples provided at 
each time-step, written beside the location with strongest write magnitude. The 
locations were re-ordered spatially on the basis of the writing order. Figure 3d, e 
is produced on the basis of the output of a trainer classifier, called the decoder. A 
logistic regression classifier was built from a dataset of 40,000 data points in which 
write vectors were treated as classifier inputs, with the input triples from the same 
time-step taken to be the classifier targets, and treating source, destination and 
edges as independent outputs. The digits of each element were decoded inde-
pendently, so that a total of nine 10-way classifiers were used to decode each triple. 
The classifier was trained with an L2 regularization of the classifier weights with 
coefficient 0.1. Output classes that were irrelevant to the episode were excluded 
from the diagram. Figure 3d was produced by applying the classifier to the content 
lookup key; Fig. 3e was produced by applying the classifier to the contents of the 
memory location with the highest read weighting.

For Fig. 4d, classifiers were trained on a dataset of 800 Mini-SHRDLU epi-
sodes. On each episode, the locations to which the read heads assign more than a 
threshold of 0.01 weighting at the time of the query were considered relevant to the 
selected goal. These locations were noted, and their contents at the time when they 
were last written (determined by the same numerical threshold) were uniformly 
averaged into a single vector. The vectors therefore encapsulate an average of the 
locations containing the goal directly after the goal has been written to memory, 
but potentially many (up to about 60) time-steps before the first action occurs. The 
vectors were used as inputs to train the classifier to predict the first five actions 
following the query; that is, action 1 occurs at tquery +  0, action 2 occurs at tquery +  1  
and so on. The classifiers use logistic regression with an L2 regularization  
coefficient of 1. The action-frequencies baseline predicts each of the action choices 
on the basis of its frequency at that time-step after the query. Classifier accuracy 
is determined by constructing 100 80%/20% random splits of the episodes into 

training and test data. The error bars represent 5–95 percentile accuracy on the 
test data partitions. In Fig. 4e, two-dimensional t-SNE48 dimensionality reduction 
is performed on those same averaged vectors. Each data point (only half of which 
are shown to reduce crowding) is marked with the relevant goal label.
Optimization. For all experiments, results are reported on the basis of statis-
tics gathered from 20 randomly initialized networks that share the same set of 
hyper-parameters. The hyper-parameters were selected from large grid searches, 
and are listed for each experiment in Extended Data Table 2. All networks were 
trained using a single machine version of Downpour SGD49. Each CPU in the 
machine runs a ‘worker’ process with its own copy of the model parameters. The 
workers generate episodes and compute gradients of the loss function with respect 
to the network weights using backpropagation through time50. The gradients are 
combined with a single central instance of the RMSProp optimization algorithm51. 
Once a worker computes gradients for an episode, it acquires a mutual exclusion 
lock guaranteeing that no other process is accessing the optimizer. The gradients 
are used to perform one optimization step, modifying the central master copy of 
the parameters, and the new parameter values are copied back to the worker. The 
optimizer updates a single global copy of its state, which for RMSProp includes a 
moving average of gradient magnitudes. Finally, the mutual exclusion is released, 
allowing a different worker to perform a gradient update. In the backpropaga-
tion-through-time backward pass, the gradient with respect to the LSTM controller 
activations was clipped element-wise to the range [− 10, 10].
Code Availability. A public version of the code will be made available within  
6 months, linked to from our website http://www.deepmind.com.

41.  Graves, A., Mohamed, A.-r. & Hinton, G. Speech recognition with deep recurrent 

neural networks. In IEEE International Conference on Acoustics, Speech and 
Signal Processing (eds Ward, R. et al.) 6645–6649 (Curran Associates, 2013).

42.  Wilson, P. R., Johnstone, M. S., Neely, M. & Boles, D. Dynamic storage allocation: 

a survey and critical review. In Memory Management (ed. Baler, H. G.) 1–116 
(Springer, 1995).

43.  Ross, S., Gordon, G. J. & Bagnell, J. A. A reduction of imitation learning and 

structured prediction to no-regret online learning. In Proc. Fourteenth 
International Conference on Artificial Intelligence and Statistics (eds Gordon, G.  
et al.) 627–635 (JMLR, 2010).

44.  Daumé, H. III, Langford, J. & Marcu, D. Search-based structured prediction. 

Mach. Learn. 75, 297–325 (2009).

45.  Williams, R. J. Simple statistical gradient-following algorithms for connectionist 

reinforcement learning. Mach. Learn. 8, 229–256 (1992).

46.  Sutton, R. S., McAllester, D., Singh, S. P. & Mansour, Y. Policy gradient methods 
for reinforcement learning with function approximation. In Advances in Neural 
Information Processing Systems Vol. 12 (eds Solla, S. A. et al.) 1057–1063 (MIT 
Press, 1999).

47.  Schulman, J., Moritz, P., Levine, S., Jordan, M. & Abbeel, P. High-dimensional 

continuous control using generalized advantage estimation. Preprint at http://
arxiv.org/abs/1506.02438 (2015).

48.  van der Maaten, L. & Hinton, G. Visualizing data using t-SNE. J. Mach. Learn. 

Res. 9, 2579–2605 (2008).

49.  Dean, J. et al. Large scale distributed deep networks. In Advances in Neural 

Information Processing Systems Vol. 25 (eds Pereira, F. et al.) 1223–1231 
(Curran Associates, 2012).

50.  Werbos, P. J. Backpropagation through time: what it does and how to do it. 

Proc. IEEE 78, 1550–1560 (1990).

51.  Tieleman, T. & Hinton, G. RmsProp: divide the gradient by a running average of its 

recent magnitude. Lecture 6.5 of Neural Networks for Machine Learning 
(COURSERA, 2012); available at http://www.cs.toronto.edu/∼ tijmen/csc321/
slides/lecture_slides_lec6.pdf.

Extended Data Figure 1 | Dynamic memory allocation. We trained 
the DNC on a copy problem, in which a series of 10 random sequences 
was presented as input. After each input sequence was presented, it was 
recreated as output. Once the output was generated, that input sequence 
was not needed again and could be erased from memory. We used a DNC 
with a feedforward controller and a memory of 10 locations—insufficient 
to store all 50 input vectors with no overwriting. The goal was to test 

whether the memory allocation system would be used to free and re-use  
locations as needed. As shown by the read and write weightings, the 
same locations are repeatedly used. The free gate is active during the read 
phases, meaning that locations are deallocated immediately after they are 
read from. The allocation gate is active during the write phases, allowing 
the deallocated locations to be re-used.

Extended Data Figure 2 | Altering the memory size of a trained 
network. A DNC trained on the traversal task with 256 memory locations 
was tested while varying the number of memory locations and graph 
triples. The heat map shows the fraction of traversals of length 1–10 
performed perfectly by the network, out of a batch of 100. There is a 
clear correspondence between the number of triples in the graph and 

the number of memory locations required to solve the task, reflecting 
our earlier analysis (Fig. 3) that suggests that DNC writes each triple to a 
separate location in memory. The network appears to exploit all available 
memory, regardless of how much memory it was trained with. This 
supports our claim that memory is independent of processing in a DNC, 
and points to large-scale applications such as knowledge graph processing.

Extended Data Figure 3 | Probability of achieving optimal solution. a, DNC. With 10 goals, the performance of a DNC network with respect to 
satisfying constraints in minimal time as the minimum number of moves to a goal and the number of constraints in a goal are varied. Performance was 
highest with a large number of constraints in each goal. b, The performance of an LSTM on the same test.

Extended Data Figure 4 | Effect of link matrix sparsity on performance. 
We trained the DNC on a copy problem, for which a sequence of length 
1–100 of size-6 random binary vectors was given as input, and an identical 
sequence was then required as output. A feedforward controller was 
used to ensure that the sequences could not be stored in the controller 
state. The faint lines show error curves for 20 randomly initialized runs 
with identical hyper-parameters, with link matrix sparsity switched off 
(pink), sparsity used with K =  5 (green) and with the link matrix disabled 
altogether (blue). The bold lines show the mean curve for each setting. 

The error rate is the fraction of sequences copied with no mistakes out 
of a batch of 100. There does not appear to be any systematic difference 
between no sparsity and K =  5. We observed similar behaviour for values 
of K between 2 and 20 (plots omitted for clarity). The task cannot easily 
be solved without the link matrix because the input sequence has to be 
recovered in the correct order. Note the abrupt drops in error for the 
networks with link matrices: these are the points at which the system 
learns a copy algorithm that generalizes to longer sequences.

Extended Data Table 1 | bAbI best and mean results

To compare with previous results we report error rates for the single best network across all tasks (measured on the validation set) over 20 runs. The lowest error rate for each task is shown in bold. 
Results for MemN2N are from ref. 21; those for DMN are from ref. 20. The mean results are reported with ± s.d. for the error rates over all 20 runs for each task. The lowest mean error rate for each task 
is shown in bold.

Extended Data Table 2 | Hyper-parameter settings for bAbI, graph tasks and Mini-SHRDLU

In bAbI experiments, for all models (LSTM, NTM and DNC) we kept the hyper-parameter settings that (1) gave the lowest average validation error rate and (2) gave the single best validation error rate for 
a single model. For LSTM and NTM the same setting was best for both criteria, but for DNC two different settings were found (DNC1 for criterion 1 and DNC2 for criterion 2).

Extended Data Table 3 | Curriculum results for graph traversal

Parentheses represent ranges: (lower bound, upper bound). ‘Test’ is the accuracy (mean ±  s.d.) on the test problem (here, the London Underground map) after the completion of each intermediate 
lesson. Evaluation of lesson completion occurs after every group of 100 batches has been processed on the main worker thread. The completion threshold is met if 90% of modal samples (most likely 
output of the network) are correct. ‘Final’ (mean ±  s.d.) is the accuracy on the final lesson of the curriculum after the completion of each intermediate lesson.

Extended Data Table 4 | Curriculum results for inference

Parentheses represent ranges: (lower bound, upper bound). Evaluation of lesson completion occurs after every group of 100 batches has been processed on the main worker thread. The completion 
threshold is met if 90% of modal samples (most likely output of the network) are correct. ‘Test’ and ‘Final’ as in Extended Data Table 3.

Extended Data Table 5 | Curriculum results for shortest-path task

Parentheses represent ranges: (lower bound, upper bound). Evaluation of lesson completion occurs after every group of 2,000 batches (size 1) has been processed on main worker thread. A path is 
defined as ‘correct’ if it is a shortest path. The completion threshold is met if 80% of modal samples (most likely output of the network) are correct on a new group of 50 episodes. ‘Test’ and ‘Final’ as 
in Extended Data Table 3.

Extended Data Table 6 | Curriculum for Mini-SHRDLU

Parentheses represent ranges: (lower bound, upper bound). Evaluation of lesson completion occurs after every group of 400 batches has been processed on the main worker thread. The completion 
threshold is met if 85% of constraints are satisfied at episode termination on average over 160 episodes. (The final lesson has no termination.)

