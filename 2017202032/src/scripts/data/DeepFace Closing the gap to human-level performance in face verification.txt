DeepFace: Closing the Gap to Human-Level Performance in Face Veriﬁcation

Yaniv Taigman

Ming Yang

Marc’Aurelio Ranzato

Lior Wolf

Facebook AI Research
Menlo Park, CA, USA

{yaniv, mingyang, ranzato}@fb.com

Tel Aviv University

Tel Aviv, Israel

wolf@cs.tau.ac.il

Abstract

In modern face recognition, the conventional pipeline
consists of four stages: detect ⇒ align ⇒ represent ⇒ clas-
sify. We revisit both the alignment step and the representa-
tion step by employing explicit 3D face modeling in order to
apply a piecewise afﬁne transformation, and derive a face
representation from a nine-layer deep neural network. This
deep network involves more than 120 million parameters
using several locally connected layers without weight shar-
ing, rather than the standard convolutional layers. Thus
we trained it on the largest facial dataset to-date, an iden-
tity labeled dataset of four million facial images belong-
ing to more than 4,000 identities. The learned representa-
tions coupling the accurate model-based alignment with the
large facial database generalize remarkably well to faces in
unconstrained environments, even with a simple classiﬁer.
Our method reaches an accuracy of 97.35% on the Labeled
Faces in the Wild (LFW) dataset, reducing the error of the
current state of the art by more than 27%, closely approach-
ing human-level performance.

1. Introduction

Face recognition in unconstrained images is at the fore-
front of the algorithmic perception revolution. The social
and cultural implications of face recognition technologies
are far reaching, yet the current performance gap in this do-
main between machines and the human visual system serves
as a buffer from having to deal with these implications.

We present a system (DeepFace) that has closed the ma-
jority of the remaining gap in the most popular benchmark
in unconstrained face recognition, and is now at the brink
of human level accuracy. It is trained on a large dataset of
faces acquired from a population vastly different than the
one used to construct the evaluation benchmarks, and it is
able to outperform existing systems with only very minimal
adaptation. Moreover, the system produces an extremely
compact face representation, in sheer contrast to the shift

1

toward tens of thousands of appearance features in other re-
cent systems [5, 7, 2].

The proposed system differs from the majority of con-
tributions in the ﬁeld in that it uses the deep learning (DL)
framework [3, 21] in lieu of well engineered features. DL is
especially suitable for dealing with large training sets, with
many recent successes in diverse domains such as vision,
speech and language modeling. Speciﬁcally with faces, the
success of the learned net in capturing facial appearance in
a robust manner is highly dependent on a very rapid 3D
alignment step. The network architecture is based on the
assumption that once the alignment is completed, the loca-
tion of each facial region is ﬁxed at the pixel level.
It is
therefore possible to learn from the raw pixel RGB values,
without any need to apply several layers of convolutions as
is done in many other networks [19, 21].

In summary, we make the following contributions : (i)
The development of an effective deep neural net (DNN) ar-
chitecture and learning method that leverage a very large
labeled dataset of faces in order to obtain a face representa-
tion that generalizes well to other datasets; (ii) An effective
facial alignment system based on explicit 3D modeling of
faces; and (iii) Advance the state of the art signiﬁcantly in
(1) the Labeled Faces in the Wild benchmark (LFW) [18],
reaching near human-performance; and (2) the YouTube
Faces dataset (YTF) [30], decreasing the error rate there by
more than 50%.
1.1. Related Work
Big data and deep learning
In recent years, a large num-
ber of photos have been crawled by search engines, and up-
loaded to social networks, which include a variety of un-
constrained material, such as objects, faces and scenes.

This large volume of data and the increase in compu-
tational resources have enabled the use of more powerful
statistical models. These models have drastically improved
the robustness of vision systems to several important vari-
ations, such as non-rigid deformations, clutter, occlusion
and illumination, all problems that are at the core of many
computer vision applications. While conventional machine

Recently,

learning methods such as Support Vector Machines, Prin-
cipal Component Analysis and Linear Discriminant Analy-
sis, have limited capacity to leverage large volumes of data,
deep neural networks have shown better scaling properties.
there has been a surge of interest in neu-
In particular, deep and large net-
ral networks [19, 21].
works have exhibited impressive results once:
(1) they
have been applied to large amounts of training data and (2)
scalable computation resources such as thousands of CPU
cores [11] and/or GPU’s [19] have become available. Most
notably, Krizhevsky et al. [19] showed that very large and
deep convolutional networks [21] trained by standard back-
propagation [25] can achieve excellent recognition accuracy
when trained on a large dataset.

Face recognition state of the art Face recognition er-
ror rates have decreased over the last twenty years by three
orders of magnitude [12] when recognizing frontal faces in
still images taken in consistently controlled (constrained)
environments. Many vendors deploy sophisticated systems
for the application of border-control and smart biometric
identiﬁcation. However, these systems have shown to be
sensitive to various factors, such as lighting, expression, oc-
clusion and aging, that substantially deteriorate their perfor-
mance in recognizing people in such unconstrained settings.
Most current face veriﬁcation methods use hand-crafted
features. Moreover,
these features are often combined
to improve performance, even in the earliest LFW con-
tributions. The systems that currently lead the perfor-
mance charts employ tens of thousands of image descrip-
tors [5, 7, 2].
In contrast, our method is applied directly
to RGB pixel values, producing a very compact yet sparse
descriptor.

Deep neural nets have also been applied in the past to
face detection [24], face alignment [27] and face veriﬁca-
tion [8, 16]. In the unconstrained domain, Huang et al. [16]
used as input LBP features and they showed improvement
when combining with traditional methods. In our method
we use raw images as our underlying representation, and
to emphasize the contribution of our work, we avoid com-
bining our features with engineered descriptors. We also
provide a new architecture, that pushes further the limit of
what is achievable with these networks by incorporating 3D
alignment, customizing the architecture for aligned inputs,
scaling the network by almost two order of magnitudes and
demonstrating a simple knowledge transfer method once the
network has been trained on a very large labeled dataset.

Metric learning methods are used heavily in face ver-
iﬁcation, often coupled with task-speciﬁc objectives [26,
29, 6]. Currently, the most successful system that uses a
large data set of labeled faces [5] employs a clever transfer
learning technique which adapts a Joint Bayesian model [6]
learned on a dataset containing 99,773 images from 2,995
different subjects, to the LFW image domain. Here, in order

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

Figure 1. Alignment pipeline. (a) The detected face, with 6 initial ﬁdu-
cial points. (b) The induced 2D-aligned crop. (c) 67 ﬁducial points on
the 2D-aligned crop with their corresponding Delaunay triangulation, we
added triangles on the contour to avoid discontinuities. (d) The reference
3D shape transformed to the 2D-aligned crop image-plane. (e) Triangle
visibility w.r.t. to the ﬁtted 3D-2D camera; darker triangles are less visible.
(f) The 67 ﬁducial points induced by the 3D model that are used to direct
the piece-wise afﬁne warpping. (g) The ﬁnal frontalized crop. (h) A new
view generated by the 3D model (not used in this paper).

to demonstrate the effectiveness of the features, we keep the
distance learning step trivial.

2. Face Alignment

Existing aligned versions of several face databases (e.g.
LFW-a [29]) help to improve recognition algorithms by pro-
viding a normalized input
[26]. However, aligning faces
in the unconstrained scenario is still considered a difﬁcult
problem that has to account for many factors such as pose
(due to the non-planarity of the face) and non-rigid expres-
sions, which are hard to decouple from the identity-bearing
facial morphology. Recent methods have shown successful
ways that compensate for these difﬁculties by using sophis-
ticated alignment techniques. These methods can use one
or more from the following: (1) employing an analytical
3D model of the face [28, 32, 14], (2) searching for sim-
ilar ﬁducial-points conﬁgurations from an external dataset
to infer from [4], and (3) unsupervised methods that ﬁnd a
similarity transformation for the pixels [17, 15].

While alignment is widely employed, no complete phys-
ically correct solution is currently present in the context of
unconstrained face veriﬁcation. 3D models have fallen out
of favor in recent years, especially in unconstrained envi-
ronments. However, since faces are 3D objects, done cor-
rectly, we believe that it is the right way. In this paper, we
describe a system that includes analytical 3D modeling of
the face based on ﬁducial points, that is used to warp a de-
tected facial crop to a 3D frontal mode (frontalization).

Similar to much of the recent alignment literature, our
alignment is based on using ﬁducial point detectors to direct
the alignment process. We use a relatively simple ﬁducial

point detector, but apply it in several iterations to reﬁne its
output. At each iteration, ﬁducial points are extracted by
a Support Vector Regressor (SVR) trained to predict point
conﬁgurations from an image descriptor. Our image de-
scriptor is based on LBP Histograms [1], but other features
can also be considered. By transforming the image using
the induced similarity matrix T to a new image, we can run
the ﬁducial detector again on a new feature space and reﬁne
the localization.

2d := (si, Ri, ti) where: xj

2D Alignment We start our alignment process by de-
tecting 6 ﬁducial points inside the detection crop, centered
at the center of the eyes, tip of the nose and mouth loca-
tions as illustrated in Fig. 1(a). They are used to approxi-
mately scale, rotate and translate the image into six anchor
locations by ﬁtting T i
anchor :=
si[Ri|ti]∗ xj
source for points j = 1..6 and iterate on the new
warped image until there is no substantial change, even-
tually composing the ﬁnal 2D similarity transformation:
2d. This aggregated transformation
T2d := T 1
generates a 2D aligned crop, as shown in Fig. 1(b). This
alignment method is similar to the one employed in LFW-a,
which has been used frequently to boost recognition accu-
racy. However, similarity transformation fails to compen-
sate for out-of-plane rotation, which is particularly impor-
tant in unconstrained conditions.

2d ∗ ... ∗ T k

3D Alignment In order to align faces undergoing out-
of-plane rotations, we use a generic 3D shape model and
register a 3D afﬁne camera, which are used to warp the 2D-
aligned crop to the image plane of the 3D shape. This gen-
erates the 3D-aligned version of the crop as illustrated in
Fig. 1(g). This is achieved by localizing additional 67 ﬁdu-
cial points x2d in the 2D-aligned crop (see Fig. 1(c)), using
a second SVR. As a 3D generic shape model, we simply
take the average of the 3D scans from the USF Human-ID
database, which were post-processed to be represented as
aligned vertices vi = (xi, yi, zi)n
i=1. We manually place
67 anchor points on the 3D shape, and in this way achieve
full correspondence between the 67 detected ﬁducial points
and their 3D references. An afﬁne 3D-to-2D camera P
is then ﬁtted using the generalized least squares solution
to the linear system x2d = X3d (cid:126)P with a known covari-
ance matrix Σ, that is, (cid:126)P that minimizes the following loss:
loss( (cid:126)P ) = rT Σ−1r where r = (x2d − X3d (cid:126)P ) is the resid-
ual vector and X3d is a (67 ∗ 2)× 8 matrix composed by
stacking the (2×8) matrices [x(cid:62)
3d(i), 1], with
(cid:126)0 denoting a row vector of four zeros, for each reference
ﬁducial point x3d(i). The afﬁne camera P of size 2× 4 is
represented by the vector of 8 unknowns (cid:126)P . The loss can
be minimized using the Cholesky decomposition of Σ, that
transforms the problem into ordinary least squares. Since,
for example, detected points on the contour of the face tend
to be more noisy, as their estimated location is largely in-
ﬂuenced by the depth with respect to the camera angle, we

3d(i), 1,(cid:126)0;(cid:126)0, x(cid:62)

use a (67 ∗ 2)×(67 ∗ 2) covariance matrix Σ given by the
estimated covariances of the ﬁducial point errors.

Frontalization Since full perspective projections and
non-rigid deformations are not modeled, the ﬁtted camera
P is only an approximation. In order to reduce the corrup-
tion of such important identity-bearing factors to the ﬁnal
warping, we add the corresponding residuals in r to the x-y
components of each reference ﬁducial point x3d, we denote

this as (cid:102)x3d. Such a relaxation is plausible for the purpose of

warping the 2D image with smaller distortions to the iden-
tity. Without it, faces would have been warped into the same
shape in 3D, losing important discriminative factors. Fi-
nally, the frontalization is achieved by a piece-wise afﬁne

transformation T from x2d (source) to (cid:102)x3d (target), directed

by the Delaunay triangulation derived from the 67 ﬁducial
points1. Also, invisible triangles w.r.t. to camera P , can be
replaced using image blending with their symmetrical coun-
terparts.

3. Representation

In recent years, the computer vision literature has at-
tracted many research efforts in descriptor engineering.
Such descriptors when applied to face-recognition, mostly
use the same operator to all locations in the facial im-
age. Recently, as more data has become available, learning-
based methods have started to outperform engineered fea-
tures, because they can discover and optimize features for
the speciﬁc task at hand [19]. Here, we learn a generic rep-
resentation of facial images through a large deep network.
DNN Architecture and Training We train our DNN
on a multi-class face recognition task, namely to classify
the identity of a face image. The overall architecture is
shown in Fig. 2. A 3D-aligned 3-channels (RGB) face im-
age of size 152 by 152 pixels is given to a convolutional
layer (C1) with 32 ﬁlters of size 11x11x3 (we denote this
by 32x11x11x3@152x152). The resulting 32 feature maps
are then fed to a max-pooling layer (M2) which takes the
max over 3x3 spatial neighborhoods with a stride of 2, sep-
arately for each channel. This is followed by another con-
volutional layer (C3) that has 16 ﬁlters of size 9x9x16. The
purpose of these three layers is to extract low-level features,
like simple edges and texture. Max-pooling layers make the
output of convolution networks more robust to local trans-
lations. When applied to aligned facial images, they make
the network more robust to small registration errors. How-
ever, several levels of pooling would cause the network to
lose information about the precise position of detailed facial
structure and micro-textures. Hence, we apply max-pooling
only to the ﬁrst convolutional layer. We interpret these ﬁrst
layers as a front-end adaptive pre-processing stage. While
they are responsible for most of the computation, they hold

1T2d can be used here to avoid going through the 2D lossy warping.

Figure 2. Outline of the DeepFace architecture. A front-end of a single convolution-pooling-convolution ﬁltering on the rectiﬁed input, followed by three
locally-connected layers and two fully-connected layers. Colors illustrate feature maps produced at each layer. The net includes more than 120 million
parameters, where more than 95% come from the local and fully connected layers.

very few parameters. These layers merely expand the input
into a set of simple local features.

The subsequent layers (L4, L5 and L6) are instead lo-
cally connected [13, 16], like a convolutional layer they ap-
ply a ﬁlter bank, but every location in the feature map learns
a different set of ﬁlters. Since different regions of an aligned
image have different local statistics, the spatial stationarity
assumption of convolution cannot hold. For example, ar-
eas between the eyes and the eyebrows exhibit very differ-
ent appearance and have much higher discrimination ability
compared to areas between the nose and the mouth. In other
words, we customize the architecture of the DNN by lever-
aging the fact that our input images are aligned. The use
of local layers does not affect the computational burden of
feature extraction, but does affect the number of parameters
subject to training. Only because we have a large labeled
dataset, we can afford three large locally connected layers.
The use of locally connected layers (without weight shar-
ing) can also be justiﬁed by the fact that each output unit of
a locally connected layer is affected by a very large patch of
the input. For instance, the output of L6 is inﬂuenced by a
74x74x3 patch at the input, and there is hardly any statisti-
cal sharing between such large patches in aligned faces.

Finally, the top two layers (F7 and F8) are fully con-
nected: each output unit is connected to all inputs. These
layers are able to capture correlations between features cap-
tured in distant parts of the face images, e.g., position and
shape of eyes and position and shape of mouth. The output
of the ﬁrst fully connected layer (F7) in the network will be
used as our raw face representation feature vector through-
out this paper.
In terms of representation, this is in con-
trast to the existing LBP-based representations proposed in
the literature, that normally pool very local descriptors (by
computing histograms) and use this as input to a classiﬁer.
The output of the last fully-connected layer is fed to a
K-way softmax (where K is the number of classes) which
produces a distribution over the class labels. If we denote
by ok the k-th output of the network on a given input, the
probability assigned to the k-th class is the output of the

softmax function: pk = exp(ok)/(cid:80)

h exp(oh).

The goal of training is to maximize the probability of
the correct class (face id). We achieve this by minimiz-
ing the cross-entropy loss for each training sample.
If k
is the index of the true label for a given input, the loss is:
L = − log pk. The loss is minimized over the parameters
by computing the gradient of L w.r.t. the parameters and
by updating the parameters using stochastic gradient de-
scent (SGD). The gradients are computed by standard back-
propagation of the error [25, 21]. One interesting property
of the features produced by this network is that they are very
sparse. On average, 75% of the feature components in the
topmost layers are exactly zero. This is mainly due to the
use of the ReLU [10] activation function: max(0, x). This
soft-thresholding non-linearity is applied after every con-
volution, locally connected and fully connected layer (ex-
cept the last one), making the whole cascade produce highly
non-linear and sparse features. Sparsity is also encouraged
by the use of a regularization method called dropout [19]
which sets random feature components to 0 during training.
We have applied dropout only to the ﬁrst fully-connected
layer. Due to the large training set, we did not observe sig-
niﬁcant overﬁtting during training2.

Given an image I, the representation G(I) is then com-
puted using the described feed-forward network. Any feed-
forward neural network with L layers, can be seen as a com-
φ. In our case, the representation is:
position of functions gl
G(I) = gF7
φ (...gC1
φ (T (I, θT ))...)) with the net’s pa-
rameters φ = {C1, ..., F7} and θT = {x2d, (cid:126)P , (cid:126)r} as de-
scribed in Section 2.

φ (gL6

Normaliaztion As a ﬁnal stage we normalize the fea-
tures to be between zero and one in order to reduce the sen-
sitivity to illumination changes: Each component of the fea-
ture vector is divided by its largest value across the training
set. This is then followed by L2-normalization: f (I) :=
¯G(I)/|| ¯G(I)||2 where ¯G(I)i = G(I)i/ max(Gi, ) 3.
Since we employ ReLU activations, our system is not in-
variant to re-scaling of the image intensities. Without bi-

2See the supplementary material for more details.
3 = 0.05 in order to avoid division by a small number.

ases in the DNN, perfect equivariance would have been
achieved.

4. Veriﬁcation Metric

Verifying whether two input instances belong to the same
class (identity) or not has been extensively researched in the
domain of unconstrained face-recognition, with supervised
methods showing a clear performance advantage over unsu-
pervised ones. By training on the target-domain’s training
set, one is able to ﬁne-tune a feature vector (or classiﬁer)
to perform better within the particular distribution of the
dataset. For instance, LFW has about 75% males, celebri-
ties that were photographed by mostly professional photog-
raphers. As demonstrated in [5], training and testing within
different domain distributions hurt performance consider-
ably and requires further tuning to the representation (or
classiﬁer) in order to improve their generalization and per-
formance. However, ﬁtting a model to a relatively small
dataset reduces its generalization to other datasets. In this
work, we aim at learning an unsupervised metric that gener-
alizes well to several datasets. Our unsupervised similarity
is simply the inner product between the two normalized fea-
ture vectors. We have also experimented with a supervised
metric, the χ2 similarity and the Siamese network.

4.1. Weighted χ2 distance

χ2(f1, f2) = (cid:80)

The normalized DeepFace feature vector in our method
contains several similarities to histogram-based features,
such as LBP [1] : (1) It contains non-negative values, (2)
it is very sparse, and (3) its values are between [0, 1].
Hence, similarly to [1], we use the weighted-χ2 similarity:
i wi(f1[i] − f2[i])2/(f1[i] + f2[i]) where
f1 and f2 are the DeepFace representations. The weight
parameters are learned using a linear SVM, applied to vec-
tors of the elements (f1[i] − f2[i])2/(f1[i] + f2[i]) .
4.2. Siamese network

We have also tested an end-to-end metric learning ap-
proach, known as Siamese network [8]: once learned, the
face recognition network (without the top layer) is repli-
cated twice (one for each input image) and the features are
used to directly predict whether the two input images be-
long to the same person. This is accomplished by: a) taking
the absolute difference between the features, followed by b)
a top fully connected layer that maps into a single logistic
unit (same/not same). The network has roughly the same
number of parameters as the original one, since much of it
is shared between the two replicas, but requires twice the
computation. Notice that in order to prevent overﬁtting on
the face veriﬁcation task, we enable training for only the
two topmost layers. The Siamese network’s induced dis-
i αi|f1[i] − f2[i]|, where αi are

tance is: d(f1, f2) = (cid:80)

Figure 3. The ROC curves on the LFW dataset. Best viewed in color.

trainable parameters. The parameters of the Siamese net-
work are trained by standard cross entropy loss and back-
propagation of the error.

5. Experiments

We evaluate the proposed DeepFace system, by learning
the face representation on a very large-scale labeled face
dataset collected online. In this section, we ﬁrst introduce
the datasets used in the experiments, then present the de-
tailed evaluation and comparison with the state-of-the-art,
as well as some insights and ﬁndings about learning and
transferring the deep face representations.

5.1. Datasets

The proposed face representation is learned from a large
collection of photos from Facebook, referred to as the So-
cial Face Classiﬁcation (SFC) dataset. The representa-
tions are then applied to the Labeled Faces in the Wild
database (LFW), which is the de facto benchmark dataset
for face veriﬁcation in unconstrained environments, and the
YouTube Faces (YTF) dataset, which is modeled similarly
to the LFW but focuses on video clips.

The SFC dataset includes 4.4 million labeled faces from
4,030 people each with 800 to 1200 faces, where the most
recent 5% of face images of each identity are left out for
testing. This is done according to the images’ time-stamp
in order to simulate continuous identiﬁcation through aging.
The large number of images per person provides a unique
opportunity for learning the invariance needed for the core
problem of face recognition. We have validated using sev-
eral automatic methods, that the identities used for train-
ing do not intersect with any of the identities in the below-
mentioned datasets, by checking their name labels.

The LFW dataset [18] consists of 13,323 web photos of
5,749 celebrities which are divided into 6,000 face pairs in
10 splits. Performance is measured by mean recognition ac-
curacy using A) the restricted protocol, in which only same
and not same labels are available in training; B) the unre-
stricted protocol, where additional training pairs are acces-
sible in training; and C) an unsupervised setting in which
no training whatsoever is performed on LFW images.

The YTF dataset [30] collects 3,425 YouTube videos
of 1,595 subjects (a subset of the celebrities in the LFW).
These videos are divided into 5,000 video pairs and 10 splits
and used to evaluate the video-level face veriﬁcation.

The face identities in SFC were labeled by humans,
which typically incorporate about 3% errors. Social face
photos have even larger variations in image quality, light-
ing, and expressions than the web images of celebrities in
the LFW and YTF which were normally taken by profes-
sional photographers rather than smartphones4.

5.2. Training on the SFC

We ﬁrst train the deep neural network on the SFC as a
multi-class classiﬁcation problem using a GPU-based en-
gine, implementing the standard back-propagation on feed-
forward nets by stochastic gradient descent (SGD) with mo-
mentum (set to 0.9). Our mini-batch size is 128, and we
have set an equal learning rate for all trainable layers to
0.01, which was manually decreased, each time by an or-
der of magnitude once the validation error stopped decreas-
ing, to a ﬁnal rate of 0.0001. We initialized the weights
in each layer from a zero-mean Gaussian distribution with
σ = 0.01, and biases are set to 0.5. We trained the network
for roughly 15 sweeps (epochs) over the whole data which
took 3 days. As described in Sec. 3, the responses of the
fully connected layer F7 are extracted to serve as the face
representation.

We evaluated different design choices of DNN in terms
of the classiﬁcation error on 5% data of SFC as the test
set. This validated the necessity of using a large-scale face
dataset and a deep architecture. First, we vary the train/test
dataset size by using a subset of the persons in the SFC.
Subsets of sizes 1.5K, 3K and 4K persons (1.5M, 3.3M, and
4.4M faces, respectively) are used. Using the architecture
in Fig. 2, we trained three networks, denoted by DF-1.5K,
DF-3.3K, and DF-4.4K. Table 1 (left column) shows that
the classiﬁcation error grows only modestly from 7.0% on
1.5K persons to 7.2% when classifying 3K persons, which
indicates that the capacity of the network can well accom-
modate the scale of 3M training images. The error rate rises
to 8.7% for 4K persons with 4.4M images, showing the net-
work scales comfortably to more persons. We’ve also varied
the global number of samples in SFC to 10%, 20%, 50%,

4See the supplementary material for more details about SFC.

leaving the number of identities in place, denoted by DF-
10%, DF-20%, DF-50% in the middle column of Table 1.
We observed the test errors rise up to 20.7%, because of
overﬁtting on the reduced training set. Since performance
does not saturate at 4M images, this shows that the network
would beneﬁt from even larger datasets.

We also vary the depth of the networks by chopping off
the C3 layer, the two local L4 and L5 layers, or all these 3
layers, referred respectively as DF-sub1, DF-sub2, and DF-
sub3. For example, only four trainable layers remain in DF-
sub3 which is a considerably shallower structure compared
to the 9 layers of the proposed network in Fig. 2. In training
such networks with 4.4M faces, the classiﬁcation errors stop
decreasing after a few epochs and remains at a level higher
than that of the deep network, as can be seen in Table 1
(right column). This veriﬁes the necessity of network depth
when training on a large face dataset.

5.3. Results on the LFW dataset

The vision community has made signiﬁcant progress
on face veriﬁcation in unconstrained environments in re-
cent years. The mean recognition accuracy on LFW [18]
marches steadily towards the human performance of over
97.5% [20]. Given some very hard cases due to aging ef-
fects, large lighting and face pose variations in LFW, any
improvement over the state-of-the-art is very remarkable
and the system has to be composed by highly optimized
modules. There is a strong diminishing return effect and any
progress now requires a substantial effort to reduce the num-
ber of errors of state-of-the-art methods. DeepFace couples
large feedforward-based models with ﬁne 3D alignment.
Regarding the importance of each component: 1) Without
frontalization: when using only the 2D alignment, the ob-
tained accuracy is “only” 94.3%. Without alignment at all,
i.e., using the center crop of face detection, the accuracy is
87.9% as parts of the facial region may fall out of the crop.
2) Without learning: when using frontalization only, and a
naive LBP/SVM combination, the accuracy is 91.4% which
is already notable given the simplicity of such a classiﬁer.

All the LFW images are processed in the same pipeline
that was used to train on the SFC dataset, denoted as
DeepFace-single. To evaluate the discriminative capability
of the face representation in isolation, we follow the unsu-
pervised setting to directly compare the inner product of a
pair of normalized features. Quite remarkably, this achieves
a mean accuracy of 95.92% which is almost on par with
the best performance to date, achieved by supervised trans-
fer learning [5]. Next, we learn a kernel SVM (with C=1)
on top of the χ2-distance vector (Sec. 4.1) following the
restricted protocol, i.e., where only the 5,400 pair labels
per split are available for the SVM training. This achieves
an accuracy 97.00%, reducing signiﬁcantly the error of the
state-of-the-art [7, 5], see Table 3.

Network
DF-1.5K
DF-3.3K
DF-4.4K

Error

Network

Network
Error
7.00% DF-10% 20.7% DF-sub1
7.22% DF-20% 15.1% DF-sub2
8.74% DF-50% 10.9% DF-sub3

Error
11.2%
12.6%
13.5%

Table 1. Comparison of the classiﬁcation errors on the SFC w.r.t.
training dataset size and network depth. See Sec. 5.2 for details.
Error (SFC) Accuracy ± SE (LFW)

Network
DeepFace-align2D
DeepFace-gradient
DeepFace-Siamese

9.5%
8.9%
NA

0.9430 ±0.0043
0.9582 ±0.0037
0.9617 ±0.0038

Table 2. The performance of various individual DeepFace net-
works and the Siamese network.

Ensembles of DNNs Next, we combine multiple net-
works trained by feeding different types of inputs to the
DNN: 1) The network DeepFace-single described above
based on 3D aligned RGB inputs; 2) The gray-level im-
age plus image gradient magnitude and orientation; and 3)
the 2D-aligned RGB images. We combine those distances
using a non-linear SVM (with C=1) with a simple sum
of power CPD-kernels: KCombined := Ksingle + Kgradient +
Kalign2d, where K(x, y) := −||x − y||2, and following the
restricted protocol, achieve an accuracy 97.15%.

The unrestricted protocol provides the operator with
knowledge about the identities in the training sets, hence
enabling the generation of many more training pairs to be
added to the training set. We further experiment with train-
ing a Siamese Network (Sec. 4.2) to learn a veriﬁcation met-
ric by ﬁne-tuning the Siamese’s (shared) pre-trained feature
extractor. Following this procedure, we have observed sub-
stantial overﬁtting to the training data. The training pairs
generated using the LFW training data are redundant as
they are generated out of roughly 9K photos, which are
insufﬁcient to reliably estimate more than 120M parame-
ters. To address these issues, we have collected an ad-
ditional dataset following the same procedure as with the
SFC, containing an additional new 100K identities, each
with only 30 samples to generate same and not-same pairs
from. We then trained the Siamese Network on it followed
by 2 training epochs on the LFW unrestricted training splits
to correct for some of the data set dependent biases. The
slightly-reﬁned representation is handled similarly as be-
fore. Combining it into the above-mentioned ensemble,
i.e., KCombined += KSiamese, yields the accuracy 97.25%, un-
der the unrestricted protocol. By adding four additional
DeepFace-single networks to the ensemble, each trained
from scratch with different random seeds, i.e., KCombined +=

(cid:80) KDeepFace-Single, the obtained accuracy is 97.35%. The

performances of the individual networks, before combina-
tion, are presented in Table 2.

The comparisons with the recent state-of-the-art meth-

Method
Joint Bayesian [6]
Tom-vs-Pete [4]
High-dim LBP [7]
TL Joint Bayesian [5]
DeepFace-single
DeepFace-single
DeepFace-ensemble
DeepFace-ensemble
Human, cropped

Accuracy ± SE
0.9242 ±0.0108
0.9330 ±0.0128
0.9517 ±0.0113
0.9633 ±0.0108
0.9592 ±0.0029
0.9700 ±0.0028
0.9715 ±0.0027
0.9735 ±0.0025
0.9753

Protocol
restricted
restricted
restricted
restricted

unsupervised

restricted
restricted
unrestricted

Table 3. Comparison with the state-of-the-art on the LFW dataset.

Method
MBGS+SVM- [31]
APEM+FUSION [22]
STFRD+PMML [9]
VSOF+OSS [23]
DeepFace-single

Accuracy (%) AUC EER
78.9 ±1.9
21.2
79.1 ±1.5
21.4
79.5 ±2.5
19.9
79.7 ±1.8
20.0
91.4 ±1.1
8.6

86.9
86.6
88.6
89.4
96.3

Table 4. Comparison with the state-of-the-art on the YTF dataset.

ods in terms of the mean accuracy and ROC curves are pre-
sented in Table 3 and Fig. 3, including human performance
on the cropped faces. The proposed DeepFace method ad-
vances the state-of-the-art, closely approaching human per-
formance in face veriﬁcation.
5.4. Results on the YTF dataset

We further validate DeepFace on the recent video-level
face veriﬁcation dataset. The image quality of YouTube
video frames is generally worse than that of web photos,
mainly due to motion blur or viewing distance. We em-
ploy the DeepFace-single representation directly by creat-
ing, for every pair of training videos, 50 pairs of frames,
one from each video, and label these as same or not-same
in accordance with the video training pair. Then a weighted
χ2 model is learned as in Sec. 4.1. Given a test-pair, we
sample 100 random pairs of frames, one from each video,
and use the mean value of the learned weighed similarity.

The comparison with recent methods is shown in Ta-
ble 4 and Fig. 4. We report an accuracy of 91.4% which
reduces the error of the previous best methods by more than
50%. Note that there are about 100 wrong labels for video
pairs, recently updated to the YTF webpage. After these are
corrected, DeepFace-single actually reaches 92.5%. This
experiment veriﬁes again that the DeepFace method easily
generalizes to a new target domain.
5.5. Computational efﬁciency

We have efﬁciently implemented a CPU-based feedfor-
ward operator, which exploits both the CPU’s Single In-
struction Multiple Data (SIMD) instructions and its cache
by leveraging the locality of ﬂoating-point computations

[5] X. Cao, D. Wipf, F. Wen, G. Duan, and J. Sun. A practical transfer
learning algorithm for face veriﬁcation. In ICCV, 2013. 1, 2, 5, 6, 7
[6] D. Chen, X. Cao, L. Wang, F. Wen, and J. Sun. Bayesian face revis-

ited: A joint formulation. In ECCV, 2012. 2, 7

[7] D. Chen, X. Cao, F. Wen, and J. Sun. Blessing of dimensionality:
High-dimensional feature and its efﬁcient compression for face veri-
ﬁcation. In CVPR, 2013. 1, 2, 6, 7

[8] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity met-
ric discriminatively, with application to face veriﬁcation. In CVPR,
2005. 2, 5

[9] Z. Cui, W. Li, D. Xu, S. Shan, and X. Chen. Fusing robust face
region descriptors via multiple metric learning for face recognition
in the wild. In CVPR, 2013. 7

[10] G. E. Dahl, T. N. Sainath, and G. E. Hinton. Improving deep neu-
ral networks for LVCSR using rectiﬁed linear units and dropout. In
ICASSP, 2013. 4

[11] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao,
M. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Ng. Large scale
distributed deep networks. In NIPS, 2012. 2

[12] P. J. P. et al. An introduction to the good, the bad, & the ugly face

recognition challenge problem. In FG, 2011. 2

[13] K. Gregor and Y. LeCun. Emergence of complex-like cells in a tem-
poral product network with local receptive ﬁelds. arXiv:1006.0448,
2010. 4

[14] T. Hassner. Viewing real-world faces in 3D. In International Con-

ference on Computer Vision (ICCV), Dec. 2013. 2

[15] G. B. Huang, V. Jain, and E. G. Learned-Miller. Unsupervised joint

alignment of complex images. In ICCV, 2007. 2

[16] G. B. Huang, H. Lee, and E. Learned-Miller. Learning hierarchical
representations for face veriﬁcation with convolutional deep belief
networks. In CVPR, 2012. 2, 4

[17] G. B. Huang, M. A. Mattar, H. Lee, and E. G. Learned-Miller. Learn-

ing to align from scratch. In NIPS, pages 773–781, 2012. 2

[18] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-miller. Labeled
faces in the wild: A database for studying face recognition in un-
constrained environments. In ECCV Workshop on Faces in Real-life
Images, 2008. 1, 6

[19] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classiﬁcation
with deep convolutional neural networks. In ANIPS, 2012. 1, 2, 3, 4
[20] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar. Attribute

and simile classiﬁers for face veriﬁcation. In ICCV, 2009. 6

[21] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient based
learning applied to document recognition. Proc. IEEE, 1998. 1, 2, 4
[22] H. Li, G. Hua, Z. Lin, J. Brandt, and J. Yang. Probabilistic elastic

matching for pose variant face veriﬁcation. In CVPR, 2013. 7

[23] H. Mendez-Vazquez, Y. Martinez-Diaz, and Z. Chai. Volume struc-
tured ordinal features with background similarity measure for video
face recognition. In Int’l Conf. on Biometrics, 2013. 7

[24] M. Osadchy, Y. LeCun, and M. Miller. Synergistic face detection and

pose estimation with energy-based models. JMLR, 2007. 2

[25] D. Rumelhart, G. Hinton, and R. Williams. Learning representations

by back-propagating errors. Nature, 1986. 2, 4

[26] K. Simonyan, O. M. Parkhi, A. Vedaldi, and A. Zisserman. Fisher

vector faces in the wild. In BMVC, 2013. 2

[27] Y. Sun, X. Wang, and X. Tang. Deep convolutional network cascade

for facial point detection. In CVPR, 2013. 2

[28] Y. Taigman and L. Wolf.

faces to
overcome performance barriers in unconstrained face recognition.
arXiv:1108.1122, 2011. 2

Leveraging billions of

[29] Y. Taigman, L. Wolf, and T. Hassner. Multiple one-shots for utilizing

class label information. In BMVC, 2009. 2

[30] L. Wolf, T. Hassner, and I. Maoz. Face recognition in unconstrained

videos with matched background similarity. In CVPR, 2011. 1, 6

[31] L. Wolf and N. Levy. The SVM-minus similarity score for video face

recognition. In CVPR, 2013. 7

[32] D. Yi, Z. Lei, and S. Z. Li. Towards pose robust face recognition. In

CVPR, 2013. 2

Figure 4. The ROC curves on the YTF dataset. Best viewed in color.

across the kernels and the image. Using a single core In-
tel 2.2GHz CPU, the operator takes 0.18 seconds to extract
features from the raw input pixels. Efﬁcient warping tech-
niques were implemented for alignment; alignment alone
takes about 0.05 seconds. Overall, the DeepFace runs at
0.33 seconds per image, accounting for image decoding,
face detection and alignment, the feedforward network, and
the ﬁnal classiﬁcation output.
6. Conclusion

An ideal face classiﬁer would recognize faces in accu-
racy that is only matched by humans. The underlying face
descriptor would need to be invariant to pose, illumination,
expression, and image quality. It should also be general, in
the sense that it could be applied to various populations with
little modiﬁcations, if any at all. In addition, short descrip-
tors are preferable, and if possible, sparse features. Cer-
tainly, rapid computation time is also a concern. We believe
that this work, which departs from the recent trend of using
more features and employing a more powerful metric learn-
ing technique, has addressed this challenge, closing the vast
majority of this performance gap. Our work demonstrates
that coupling a 3D model-based alignment with large capac-
ity feedforward models can effectively learn from many ex-
amples to overcome the drawbacks and limitations of previ-
ous methods. The ability to present a marked improvement
in face recognition, attests to the potential of such coupling
to become signiﬁcant in other vision domains as well.
References
[1] T. Ahonen, A. Hadid, and M. Pietik¨ainen. Face description with local

binary patterns: Application to face recognition. PAMI, 2006. 3, 5

[2] O. Barkan, J. Weill, L. Wolf, and H. Aronowitz. Fast high dimen-

sional vector multiplication face recognition. In ICCV, 2013. 1, 2

[3] Y. Bengio. Learning deep architectures for AI. Foundations and

Trends in Machine Learning, 2009. 1

[4] T. Berg and P. N. Belhumeur. Tom-vs-pete classiﬁers and identity-

preserving alignment for face veriﬁcation. In BMVC, 2012. 2, 7

