4
1
0
2

 

n
u
J
 

5

 
 
]
E
N
.
s
c
[
 
 

5
v
0
5
8
0

.

8
0
3
1
:
v
i
X
r
a

Generating Sequences With
Recurrent Neural Networks

Alex Graves

Department of Computer Science

University of Toronto

graves@cs.toronto.edu

Abstract

This paper shows how Long Short-term Memory recurrent neural net-
works can be used to generate complex sequences with long-range struc-
ture, simply by predicting one data point at a time. The approach is
demonstrated for text (where the data are discrete) and online handwrit-
ing (where the data are real-valued). It is then extended to handwriting
synthesis by allowing the network to condition its predictions on a text
sequence. The resulting system is able to generate highly realistic cursive
handwriting in a wide variety of styles.

1

Introduction

Recurrent neural networks (RNNs) are a rich class of dynamic models that have
been used to generate sequences in domains as diverse as music [6, 4], text [30]
and motion capture data [29]. RNNs can be trained for sequence generation by
processing real data sequences one step at a time and predicting what comes
next. Assuming the predictions are probabilistic, novel sequences can be gener-
ated from a trained network by iteratively sampling from the network’s output
distribution, then feeding in the sample as input at the next step.
In other
words by making the network treat its inventions as if they were real, much like
a person dreaming. Although the network itself is deterministic, the stochas-
ticity injected by picking samples induces a distribution over sequences. This
distribution is conditional, since the internal state of the network, and hence its
predictive distribution, depends on the previous inputs.

RNNs are ‘fuzzy’ in the sense that they do not use exact templates from
the training data to make predictions, but rather—like other neural networks—
use their internal representation to perform a high-dimensional interpolation
between training examples. This distinguishes them from n-gram models and
compression algorithms such as Prediction by Partial Matching [5], whose pre-
dictive distributions are determined by counting exact matches between the
recent history and the training set. The result—which is immediately appar-

1

ent from the samples in this paper—is that RNNs (unlike template-based al-
gorithms) synthesise and reconstitute the training data in a complex way, and
rarely generate the same thing twice. Furthermore, fuzzy predictions do not suf-
fer from the curse of dimensionality, and are therefore much better at modelling
real-valued or multivariate data than exact matches.

In principle a large enough RNN should be suﬃcient to generate sequences
of arbitrary complexity.
In practice however, standard RNNs are unable to
store information about past inputs for very long [15]. As well as diminishing
their ability to model long-range structure, this ‘amnesia’ makes them prone to
instability when generating sequences. The problem (common to all conditional
generative models) is that if the network’s predictions are only based on the last
few inputs, and these inputs were themselves predicted by the network, it has
little opportunity to recover from past mistakes. Having a longer memory has
a stabilising eﬀect, because even if the network cannot make sense of its recent
history, it can look further back in the past to formulate its predictions. The
problem of instability is especially acute with real-valued data, where it is easy
for the predictions to stray from the manifold on which the training data lies.
One remedy that has been proposed for conditional models is to inject noise into
the predictions before feeding them back into the model [31], thereby increasing
the model’s robustness to surprising inputs. However we believe that a better
memory is a more profound and eﬀective solution.

Long Short-term Memory (LSTM) [16] is an RNN architecture designed to
be better at storing and accessing information than standard RNNs. LSTM has
recently given state-of-the-art results in a variety of sequence processing tasks,
including speech and handwriting recognition [10, 12]. The main goal of this
paper is to demonstrate that LSTM can use its memory to generate complex,
realistic sequences containing long-range structure.

Section 2 deﬁnes a ‘deep’ RNN composed of stacked LSTM layers, and ex-
plains how it can be trained for next-step prediction and hence sequence gener-
ation. Section 3 applies the prediction network to text from the Penn Treebank
and Hutter Prize Wikipedia datasets. The network’s performance is compet-
itive with state-of-the-art language models, and it works almost as well when
predicting one character at a time as when predicting one word at a time. The
highlight of the section is a generated sample of Wikipedia text, which showcases
the network’s ability to model long-range dependencies. Section 4 demonstrates
how the prediction network can be applied to real-valued data through the use
of a mixture density output layer, and provides experimental results on the IAM
Online Handwriting Database. It also presents generated handwriting samples
proving the network’s ability to learn letters and short words direct from pen
traces, and to model global features of handwriting style. Section 5 introduces
an extension to the prediction network that allows it to condition its outputs on
a short annotation sequence whose alignment with the predictions is unknown.
This makes it suitable for handwriting synthesis, where a human user inputs
a text and the algorithm generates a handwritten version of it. The synthesis
network is trained on the IAM database, then used to generate cursive hand-
writing samples, some of which cannot be distinguished from real data by the

2

Figure 1: Deep recurrent neural network prediction architecture. The
circles represent network layers, the solid lines represent weighted connections
and the dashed lines represent predictions.

naked eye. A method for biasing the samples towards higher probability (and
greater legibility) is described, along with a technique for ‘priming’ the sam-
ples on real data and thereby mimicking a particular writer’s style. Finally,
concluding remarks and directions for future work are given in Section 6.

2 Prediction Network

Fig. 1 illustrates the basic recurrent neural network prediction architecture used
in this paper. An input vector sequence x = (x1, . . . , xT ) is passed through
weighted connections to a stack of N recurrently connected hidden layers to
compute ﬁrst the hidden vector sequences hn = (hn
T ) and then the
output vector sequence y = (y1, . . . , yT ). Each output vector yt is used to
parameterise a predictive distribution Pr(xt+1|yt) over the possible next inputs
xt+1. The ﬁrst element x1 of every input sequence is always a null vector whose
entries are all zero; the network therefore emits a prediction for x2, the ﬁrst
real input, with no prior information. The network is ‘deep’ in both space
and time, in the sense that every piece of information passing either vertically
or horizontally through the computation graph will be acted on by multiple
successive weight matrices and nonlinearities.

1 , . . . , hn

Note the ‘skip connections’ from the inputs to all hidden layers, and from
all hidden layers to the outputs. These make it easier to train deep networks,

3

by reducing the number of processing steps between the bottom of the network
and the top, and thereby mitigating the ‘vanishing gradient’ problem [1]. In
the special case that N = 1 the architecture reduces to an ordinary, single layer
next step prediction RNN.

The hidden layer activations are computed by iterating the following equa-

tions from t = 1 to T and from n = 2 to N :

t = H(cid:0)Wih1xt + Wh1h1 h1
t = H(cid:0)Wihn xt + Whn−1hnhn−1

t−1 + b1
h

h1
hn

(cid:1)

t + Whnhn hn

t−1 + bn
h

(cid:1)

(1)

(2)

where the W terms denote weight matrices (e.g. Wihn is the weight matrix
connecting the inputs to the nth hidden layer, Wh1h1 is the recurrent connection
at the ﬁrst hidden layer, and so on), the b terms denote bias vectors (e.g. by is
output bias vector) and H is the hidden layer function.

Given the hidden sequences, the output sequence is computed as follows:

Whnyhn
t

(3)

N(cid:88)

n=1

ˆyt = by +
yt = Y(ˆyt)

T(cid:89)

t=1

L(x) = − T(cid:88)

(4)
where Y is the output layer function. The complete network therefore deﬁnes
a function, parameterised by the weight matrices, from input histories x1:t to
output vectors yt.
The output vectors yt are used to parameterise the predictive distribution
Pr(xt+1|yt) for the next input. The form of Pr(xt+1|yt) must be chosen carefully
to match the input data. In particular, ﬁnding a good predictive distribution
for high-dimensional, real-valued data (usually referred to as density modelling),
can be very challenging.

The probability given by the network to the input sequence x is

Pr(x) =

Pr(xt+1|yt)

(5)

and the sequence loss L(x) used to train the network is the negative logarithm
of Pr(x):

log Pr(xt+1|yt)

(6)

t=1

The partial derivatives of the loss with respect to the network weights can be
eﬃciently calculated with backpropagation through time [33] applied to the
computation graph shown in Fig. 1, and the network can then be trained with
gradient descent.

2.1 Long Short-Term Memory
In most RNNs the hidden layer function H is an elementwise application of a
sigmoid function. However we have found that the Long Short-Term Memory

4

Figure 2: Long Short-term Memory Cell

(LSTM) architecture [16], which uses purpose-built memory cells to store infor-
mation, is better at ﬁnding and exploiting long range dependencies in the data.
Fig. 2 illustrates a single LSTM memory cell. For the version of LSTM used in
this paper [7] H is implemented by the following composite function:

it = σ (Wxixt + Whiht−1 + Wcict−1 + bi)
ft = σ (Wxf xt + Whf ht−1 + Wcf ct−1 + bf )
ct = ftct−1 + it tanh (Wxcxt + Whcht−1 + bc)
ot = σ (Wxoxt + Whoht−1 + Wcoct + bo)
ht = ot tanh(ct)

(7)

(8)

(9)

(10)

(11)

where σ is the logistic sigmoid function, and i, f , o and c are respectively the
input gate, forget gate, output gate, cell and cell input activation vectors, all of
which are the same size as the hidden vector h. The weight matrix subscripts
have the obvious meaning, for example Whi is the hidden-input gate matrix,
Wxo is the input-output gate matrix etc. The weight matrices from the cell
to gate vectors (e.g. Wci) are diagonal, so element m in each gate vector only
receives input from element m of the cell vector. The bias terms (which are
added to i, f , c and o) have been omitted for clarity.

The original LSTM algorithm used a custom designed approximate gradi-
ent calculation that allowed the weights to be updated after every timestep [16].
However the full gradient can instead be calculated with backpropagation through
time [11], the method used in this paper. One diﬃculty when training LSTM
with the full gradient is that the derivatives sometimes become excessively large,

5

leading to numerical problems. To prevent this, all the experiments in this pa-
per clipped the derivative of the loss with respect to the network inputs to the
LSTM layers (before the sigmoid and tanh functions are applied) to lie within
a predeﬁned range1.

3 Text Prediction

Text data is discrete, and is typically presented to neural networks using ‘one-
hot’ input vectors. That is, if there are K text classes in total, and class k is fed
in at time t, then xt is a length K vector whose entries are all zero except for
the kth, which is one. Pr(xt+1|yt) is therefore a multinomial distribution, which
can be naturally parameterised by a softmax function at the output layer:

Pr(xt+1 = k|yt) = yk

t =

Substituting into Eq. (6) we see that

(cid:1)

t

exp(cid:0)ˆyk
(cid:1)
k(cid:48)=1 exp(cid:0)ˆyk(cid:48)
(cid:80)K
L(x) = − T(cid:88)

log yxt+1

t

t

=⇒ ∂L(x)

∂ ˆyk
t

t=1

= yk

t − δk,xt+1

(12)

(13)

(14)

The only thing that remains to be decided is which set of classes to use. In
most cases, text prediction (usually referred to as language modelling) is per-
formed at the word level. K is therefore the number of words in the dictionary.
This can be problematic for realistic tasks, where the number of words (in-
cluding variant conjugations, proper names, etc.) often exceeds 100,000. As
well as requiring many parameters to model, having so many classes demands a
huge amount of training data to adequately cover the possible contexts for the
words. In the case of softmax models, a further diﬃculty is the high computa-
tional cost of evaluating all the exponentials during training (although several
methods have been to devised make training large softmax layers more eﬃcient,
including tree-based models [25, 23], low rank approximations [27] and stochas-
tic derivatives [26]). Furthermore, word-level models are not applicable to text
data containing non-word strings, such as multi-digit numbers or web addresses.
Character-level language modelling with neural networks has recently been
considered [30, 24], and found to give slightly worse performance than equiv-
alent word-level models. Nonetheless, predicting one character at a time is
more interesting from the perspective of sequence generation, because it allows
the network to invent novel words and strings. In general, the experiments in
this paper aim to predict at the ﬁnest granularity found in the data, so as to
maximise the generative ﬂexibility of the network.

1In fact this technique was used in all my previous papers on LSTM, and in my publicly

available LSTM code, but I forgot to mention it anywhere—mea culpa.

6

3.1 Penn Treebank Experiments

The ﬁrst set of text prediction experiments focused on the Penn Treebank por-
tion of the Wall Street Journal corpus [22]. This was a preliminary study whose
main purpose was to gauge the predictive power of the network, rather than to
generate interesting sequences.

Although a relatively small text corpus (a little over a million words in total),
the Penn Treebank data is widely used as a language modelling benchmark. The
training set contains 930,000 words, the validation set contains 74,000 words and
the test set contains 82,000 words. The vocabulary is limited to 10,000 words,
with all other words mapped to a special ‘unknown word’ token. The end-of-
sentence token was included in the input sequences, and was counted in the
sequence loss. The start-of-sentence marker was ignored, because its role is
already fulﬁlled by the null vectors that begin the sequences (c.f. Section 2).

The experiments compared the performance of word and character-level
LSTM predictors on the Penn corpus. In both cases, the network architecture
was a single hidden layer with 1000 LSTM units. For the character-level network
the input and output layers were size 49, giving approximately 4.3M weights in
total, while the word-level network had 10,000 inputs and outputs and around
54M weights. The comparison is therefore somewhat unfair, as the word-level
network had many more parameters. However, as the dataset is small, both net-
works were easily able to overﬁt the training data, and it is not clear whether the
character-level network would have beneﬁted from more weights. All networks
were trained with stochastic gradient descent, using a learn rate of 0.0001 and a
momentum of 0.99. The LSTM derivates were clipped in the range [−1, 1] (c.f.
Section 2.1).

Neural networks are usually evaluated on test data with ﬁxed weights. For
prediction problems however, where the inputs are the targets, it is legitimate
to allow the network to adapt its weights as it is being evaluated (so long as
it only sees the test data once). Mikolov refers to this as dynamic evaluation.
Dynamic evaluation allows for a fairer comparison with compression algorithms,
for which there is no division between training and test sets, as all data is only
predicted once.

Since both networks overﬁt the training data, we also experiment with two
types of regularisation: weight noise [18] with a std. deviation of 0.075 applied
to the network weights at the start of each training sequence, and adaptive
weight noise [8], where the variance of the noise is learned along with the weights
using a Minimum description Length (or equivalently, variational inference) loss
function. When weight noise was used, the network was initialised with the
ﬁnal weights of the unregularised network. Similarly, when adaptive weight
noise was used, the weights were initialised with those of the network trained
with weight noise. We have found that retraining with iteratively increased
regularisation is considerably faster than training from random weights with
regularisation. Adaptive weight noise was found to be prohibitively slow for
the word-level network, so it was regularised with ﬁxed-variance weight noise
only. One advantage of adaptive weight is that early stopping is not needed

7

Table 1: Penn Treebank Test Set Results.
‘Error’ is next-step classiﬁcation error rate, for either characters or words.

‘BPC’ is bits-per-character.

Input
Char
char
char
char
char
char
word
word
word
word

Regularisation

Dynamic BPC Perplexity

Error (%)

Epochs

none
none

weight noise
weight noise

adapt. wt. noise
adapt. wt. noise

none
none

weight noise
weight noise

no
yes
no
yes
no
yes
no
yes
no
yes

1.32
1.29
1.27
1.24
1.26
1.24
1.27
1.25
1.25
1.23

167
148
140
124
133
122
138
126
126
117

28.5
28.0
27.4
26.9
27.4
26.9
77.8
76.9
76.9
76.2

9
9
25
25
26
26
11
11
14
14

(the network can safely be stopped at the point of minimum total ‘description
length’ on the training data). However, to keep the comparison fair, the same
training, validation and test sets were used for all experiments.
The results are presented with two equivalent metrics: bits-per-character
(BPC), which is the average value of − log2 Pr(xt+1|yt) over the whole test set;
and perplexity which is two to the power of the average number of bits per word
(the average word length on the test set is about 5.6 characters, so perplexity ≈
25.6BP C). Perplexity is the usual performance measure for language modelling.
Table 1 shows that the word-level RNN performed better than the character-
level network, but the gap appeared to close when regularisation is used. Overall
the results compare favourably with those collected in Tomas Mikolov’s the-
sis [23]. For example, he records a perplexity of 141 for a 5-gram with Keyser-
Ney smoothing, 141.8 for a word level feedforward neural network, 131.1 for the
state-of-the-art compression algorithm PAQ8 and 123.2 for a dynamically eval-
uated word-level RNN. However by combining multiple RNNs, a 5-gram and a
cache model in an ensemble, he was able to achieve a perplexity of 89.4. Inter-
estingly, the beneﬁt of dynamic evaluation was far more pronounced here than
in Mikolov’s thesis (he records a perplexity improvement from 124.7 to 123.2
with word-level RNNs). This suggests that LSTM is better at rapidly adapting
to new data than ordinary RNNs.

3.2 Wikipedia Experiments

In 2006 Marcus Hutter, Jim Bowery and Matt Mahoney organised the following
challenge, commonly known as Hutter prize [17]: to compress the ﬁrst 100
million bytes of the complete English Wikipedia data (as it was at a certain
time on March 3rd 2006) to as small a ﬁle as possible. The ﬁle had to include
not only the compressed data, but also the code implementing the compression
algorithm.
Its size can therefore be considered a measure of the minimum
description length [13] of the data using a two part coding scheme.

Wikipedia data is interesting from a sequence generation perspective because

8

it contains not only a huge range of dictionary words, but also many character
sequences that would not be included in text corpora traditionally used for
language modelling. For example foreign words (including letters from non-
Latin alphabets such as Arabic and Chinese), indented XML tags used to deﬁne
meta-data, website addresses, and markup used to indicate page formatting such
as headings, bullet points etc. An extract from the Hutter prize dataset is shown
in Figs. 3 and 4.

The ﬁrst 96M bytes in the data were evenly split into sequences of 100 bytes
and used to train the network, with the remaining 4M were used for validation.
The data contains a total of 205 one-byte unicode symbols. The total number
of characters is much higher, since many characters (especially those from non-
Latin languages) are deﬁned as multi-symbol sequences. In keeping with the
principle of modelling the smallest meaningful units in the data, the network
predicted a single byte at a time, and therefore had size 205 input and output
layers.

Wikipedia contains long-range regularities, such as the topic of an article,
which can span many thousand words. To make it possible for the network to
capture these, its internal state (that is, the output activations ht of the hidden
layers, and the activations ct of the LSTM cells within the layers) were only reset
every 100 sequences. Furthermore the order of the sequences was not shuﬄed
during training, as it usually is for neural networks. The network was therefore
able to access information from up to 10K characters in the past when making
predictions. The error terms were only backpropagated to the start of each 100
byte sequence, meaning that the gradient calculation was approximate. This
form of truncated backpropagation has been considered before for RNN lan-
guage modelling [23], and found to speed up training (by reducing the sequence
length and hence increasing the frequency of stochastic weight updates) without
aﬀecting the network’s ability to learn long-range dependencies.

A much larger network was used for this data than the Penn data (reﬂecting
the greater size and complexity of the training set) with seven hidden layers of
700 LSTM cells, giving approximately 21.3M weights. The network was trained
with stochastic gradient descent, using a learn rate of 0.0001 and a momentum
of 0.9.
It took four training epochs to converge. The LSTM derivates were
clipped in the range [−1, 1].

As with the Penn data, we tested the network on the validation data with
and without dynamic evaluation (where the weights are updated as the data
is predicted). As can be seen from Table 2 performance was much better with
dynamic evaluation. This is probably because of the long range coherence of
Wikipedia data; for example, certain words are much more frequent in some
articles than others, and being able to adapt to this during evaluation is ad-
vantageous. It may seem surprising that the dynamic results on the validation
set were substantially better than on the training set. However this is easily
explained by two factors: ﬁrstly, the network underﬁt the training data, and
secondly some portions of the data are much more diﬃcult than others (for
example, plain text is harder to predict than XML tags).

To put the results in context, the current winner of the Hutter Prize (a

9

Table 2: Wikipedia Results (bits-per-character)

Train Validation (static) Validation (dynamic)
1.42

1.67

1.33

variant of the PAQ-8 compression algorithm [20]) achieves 1.28 BPC on the same
data (including the code required to implement the algorithm), mainstream
compressors such as zip generally get more than 2, and a character level RNN
applied to a text-only version of the data (i.e. with all the XML, markup tags
etc. removed) achieved 1.54 on held-out data, which improved to 1.47 when the
RNN was combined with a maximum entropy model [24].

A four page sample generated by the prediction network is shown in Figs. 5
to 8. The sample shows that the network has learned a lot of structure from
the data, at a wide range of diﬀerent scales. Most obviously, it has learned a
large vocabulary of dictionary words, along with a subword model that enables
it to invent feasible-looking words and names: for example “Lochroom River”,
“Mughal Ralvaldens”, “submandration”, “swalloped”. It has also learned basic
punctuation, with commas, full stops and paragraph breaks occurring at roughly
the right rhythm in the text blocks.

Being able to correctly open and close quotation marks and parentheses is
a clear indicator of a language model’s memory, because the closure cannot be
predicted from the intervening text, and hence cannot be modelled with short-
range context [30]. The sample shows that the network is able to balance not
only parentheses and quotes, but also formatting marks such as the equals signs
used to denote headings, and even nested XML tags and indentation.

The network generates non-Latin characters such as Cyrillic, Chinese and
Arabic, and seems to have learned a rudimentary model for languages other
than English (e.g. it generates “es:Geotnia slago” for the Spanish ‘version’ of an
article, and “nl:Rodenbaueri” for the Dutch one) It also generates convincing
looking internet addresses (none of which appear to be real).

The network generates distinct, large-scale regions, such as XML headers,
bullet-point lists and article text. Comparison with Figs. 3 and 4 suggests that
these regions are a fairly accurate reﬂection of the constitution of the real data
(although the generated versions tend to be somewhat shorter and more jumbled
together). This is signiﬁcant because each region may span hundreds or even
thousands of timesteps. The fact that the network is able to remain coherent
over such large intervals (even putting the regions in an approximately correct
order, such as having headers at the start of articles and bullet-pointed ‘see also’
lists at the end) is testament to its long-range memory.

As with all text generated by language models, the sample does not make
sense beyond the level of short phrases. The realism could perhaps be improved
with a larger network and/or more data. However, it seems futile to expect
meaningful language from a machine that has never been exposed to the sensory

10

world to which language refers.

Lastly, the network’s adaptation to recent sequences during training (which
allows it to beneﬁt from dynamic evaluation) can be clearly observed in the
extract. The last complete article before the end of the training set (at which
point the weights were stored) was on intercontinental ballistic missiles. The
inﬂuence of this article on the network’s language model can be seen from the
profusion of missile-related terms. Other recent topics include ‘Individual An-
archism’, the Italian writer Italo Calvino and the International Organization
for Standardization (ISO), all of which make themselves felt in the network’s
vocabulary.

11

Figure 3: Real Wikipedia data

12

Figure 4: Real Wikipedia data (cotd.)

13

Figure 5: Generated Wikipedia data.

14

Figure 6: Generated Wikipedia data (cotd.)

15

Figure 7: Generated Wikipedia data (cotd.)

16

Figure 8: Generated Wikipedia data (cotd.)

17

4 Handwriting Prediction

To test whether the prediction network could also be used to generate convincing
real-valued sequences, we applied it to online handwriting data (online in this
context means that the writing is recorded as a sequence of pen-tip locations,
as opposed to oﬄine handwriting, where only the page images are available).
Online handwriting is an attractive choice for sequence generation due to its
low dimensionality (two real numbers per data point) and ease of visualisation.
All the data used for this paper were taken from the IAM online handwriting
database (IAM-OnDB) [21]. IAM-OnDB consists of handwritten lines collected
from 221 diﬀerent writers using a ‘smart whiteboard’. The writers were asked to
write forms from the Lancaster-Oslo-Bergen text corpus [19], and the position
of their pen was tracked using an infra-red device in the corner of the board.
Samples from the training data are shown in Fig. 9. The original input data
consists of the x and y pen co-ordinates and the points in the sequence when
the pen is lifted oﬀ the whiteboard. Recording errors in the x, y data was
corrected by interpolating to ﬁll in for missing readings, and removing steps
whose length exceeded a certain threshold. Beyond that, no preprocessing was
used and the network was trained to predict the x, y co-ordinates and the end-
of-stroke markers one point at a time. This contrasts with most approaches to
handwriting recognition and synthesis, which rely on sophisticated preprocessing
and feature-extraction techniques. We eschewed such techniques because they
tend to reduce the variation in the data (e.g. by normalising the character size,
slant, skew and so-on) which we wanted the network to model. Predicting the
pen traces one point at a time gives the network maximum ﬂexibility to invent
novel handwriting, but also requires a lot of memory, with the average letter
occupying more than 25 timesteps and the average line occupying around 700.
Predicting delayed strokes (such as dots for ‘i’s or crosses for ‘t’s that are added
after the rest of the word has been written) is especially demanding.

IAM-OnDB is divided into a training set, two validation sets and a test
set, containing respectively 5364, 1438, 1518 and 3859 handwritten lines taken
from 775, 192, 216 and 544 forms. For our experiments, each line was treated
as a separate sequence (meaning that possible dependencies between successive
lines were ignored). In order to maximise the amount of training data, we used
the training set, test set and the larger of the validation sets for training and
the smaller validation set for early-stopping. The lack of independent test set
means that the recorded results may be somewhat overﬁt on the validation set;
however the validation results are of secondary importance, since no benchmark
results exist and the main goal was to generate convincing-looking handwriting.
The principal challenge in applying the prediction network to online hand-
writing data was determining a predictive distribution suitable for real-valued
inputs. The following section describes how this was done.

18

Figure 9: Training samples from the IAM online handwriting database.
Notice the wide range of writing styles, the variation in line angle and character
sizes, and the writing and recording errors, such as the scribbled out letters in
the ﬁrst line and the repeated word in the ﬁnal line.

4.1 Mixture Density Outputs

The idea of mixture density networks [2, 3] is to use the outputs of a neural
network to parameterise a mixture distribution. A subset of the outputs are
used to deﬁne the mixture weights, while the remaining outputs are used to
parameterise the individual mixture components. The mixture weight outputs
are normalised with a softmax function to ensure they form a valid discrete dis-
tribution, and the other outputs are passed through suitable functions to keep
their values within meaningful range (for example the exponential function is
typically applied to outputs used as scale parameters, which must be positive).
Mixture density network are trained by maximising the log probability den-
sity of the targets under the induced distributions. Note that the densities are
normalised (up to a ﬁxed constant) and are therefore straightforward to diﬀer-
entiate and pick unbiased sample from, in contrast with restricted Boltzmann
machines [14] and other undirected models.

Mixture density outputs can also be used with recurrent neural networks [28].
In this case the output distribution is conditioned not only on the current input,
but on the history of previous inputs. Intuitively, the number of components is
the number of choices the network has for the next output given the inputs so
far.

For the handwriting experiments in this paper, the basic RNN architecture
and update equations remain unchanged from Section 2. Each input vector xt
consists of a real-valued pair x1, x2 that deﬁnes the pen oﬀset from the previous

19

input, along with a binary x3 that has value 1 if the vector ends a stroke (that
is, if the pen was lifted oﬀ the board before the next vector was recorded) and
value 0 otherwise. A mixture of bivariate Gaussians was used to predict x1
and x2, while a Bernoulli distribution was used for x3. Each output vector yt
therefore consists of the end of stroke probability e, along with a set of means
µj, standard deviations σj, correlations ρj and mixture weights πj for the M
mixture components. That is

(cid:16)

xt ∈ R × R × {0, 1}
t , σj
yt =

et,{πj

t , µj

t}M

j=1

t , ρj

(cid:17)

Note that the mean and standard deviation are two dimensional vectors, whereas
the component weight, correlation and end-of-stroke probability are scalar. The
vectors yt are obtained from the network outputs ˆyt, where

(cid:17)

N(cid:88)

ˆet,{ ˆwj

t , ˆµj

t , ˆσj

t , ˆρj

t}M

j=1

= by +

Whnyhn
t

(17)

ˆyt =

as follows:

n=1

(cid:17)

=⇒ et ∈ (0, 1)

=⇒ πj

t ∈ (0, 1),

(cid:88)

j

πj
t = 1

(cid:16)

1

M(cid:88)

et =

πj
t =

(cid:17)
(cid:16)

(cid:16)

(cid:17)

1 + exp (ˆet)
ˆπj
t

exp

j(cid:48)=1 exp

ˆπj(cid:48)

t

(cid:80)M
(cid:16)

t

t = ˆµj
µj
σj
t = exp
t = tanh(ˆρj
ρj
t )

ˆσj
t

(15)

(16)

(18)

(19)

(20)

(21)

(23)

(24)

(25)

=⇒ µj
=⇒ σj
=⇒ ρj

t ∈ R
t > 0
t ∈ (−1, 1)

(cid:40)

(22)
The probability density Pr(xt+1|yt) of the next input xt+1 given the output
vector yt is deﬁned as follows:

Pr(xt+1|yt) =

t N (xt+1|µj
πj

t , σj

t , ρj
t )

et
1 − et

if (xt+1)3 = 1
otherwise

j=1

N (x|µ, σ, ρ) =

2πσ1σ2

Z =

(x1 − µ1)2

σ2
1

+

(x2 − µ2)2

σ2
2

(cid:21)

(cid:20) −Z

1

exp

(cid:112)1 − ρ2
− 2ρ(x1 − µ1)(x2 − µ2)

2(1 − ρ2)

σ1σ2

where

with

20

This can be substituted into Eq. (6) to determine the sequence loss (up to
a constant that depends only on the quantisation of the data and does not
inﬂuence network training):

T(cid:88)

(cid:88)

L(x) =

− log

tN (xt+1|µj
πj

t , σj

t , ρj
t )

t=1

j

(cid:40)

 −

log et
log(1 − et)

if (xt+1)3 = 1
otherwise

(26)

The derivative of the loss with respect to the end-of-stroke outputs is straight-
forward:

∂L(x)
∂ˆet

= (xt+1)3 − et

(27)

The derivatives with respect to the mixture density outputs can be found by
ﬁrst deﬁning the component responsibilities γj
t :
tN (xt+1|µj
t , ρj
t , σj
t )
t(cid:80)M
ˆγj
j(cid:48)=1 ˆγj(cid:48)

ˆγj
t = πj

γj
t =

(29)

(28)

t

Then observing that

∂L(x)
∂ ˆπj
t
∂L(x)
t , ˆρj
t , ˆσj
t )

∂(ˆµj

where

∂ log N (x|µ, σ, ρ)

∂ ˆµ1

∂ log N (x|µ, σ, ρ)

∂ ˆµ2

∂ log N (x|µ, σ, ρ)

∂ ˆσ1

∂ log N (x|µ, σ, ρ)

∂ ˆσ2

∂ log N (x|µ, σ, ρ)

∂ ˆρ

=

=

=

=

=

= πj

t − γj

t

= −γj

t

∂ log N (xt+1|µj
t , σj
t , ˆρj
t )

t , ˆσj

∂(ˆµj

t , ρj
t )

(cid:19)
(cid:19)

(cid:18) x1 − µ1
(cid:18) x2 − µ2

C
σ1
C
σ2
σ2
C(x1 − µ1)

σ1

σ1

C(x2 − µ2)

σ2

− ρ(x2 − µ2)
− ρ(x1 − µ1)
(cid:18) x1 − µ1
(cid:18) x2 − µ2

σ1

σ1

σ2

− ρ(x2 − µ2)
− ρ(x1 − µ1)
σ1
+ ρ (1 − CZ)

σ2

σ2
(x1 − µ1)(x2 − µ2)

σ1σ2

with Z deﬁned as in Eq. (25) and

C =

1

1 − ρ2

(30)

(31)

(32)

(33)

(34)

(35)

(36)

(37)

(cid:19)
(cid:19)

− 1

− 1

Fig. 10 illustrates the operation of a mixture density output layer applied to

online handwriting prediction.

21

Figure 10: Mixture density outputs for handwriting prediction. The
top heatmap shows the sequence of probability distributions for the predicted
pen locations as the word ‘under’ is written. The densities for successive
predictions are added together, giving high values where the distributions
overlap.

Two types of prediction are visible from the density map:
the small
blobs that spell out the letters are the predictions as the strokes are being
written, the three large blobs are the predictions at the ends of the strokes for
the ﬁrst point in the next stroke. The end-of-stroke predictions have much
higher variance because the pen position was not recorded when it was oﬀ the
whiteboard, and hence there may be a large distance between the end of one
stroke and the start of the next.

The bottom heatmap shows the mixture component weights during the
same sequence. The stroke ends are also visible here, with the most active
components switching oﬀ in three places, and other components switching on:
evidently end-of-stroke predictions use a diﬀerent set of mixture components
from in-stroke predictions.

22

4.2 Experiments

Each point in the data sequences consisted of three numbers: the x and y oﬀset
from the previous point, and the binary end-of-stroke feature. The network
input layer was therefore size 3. The co-ordinate oﬀsets were normalised to
mean 0, std. dev. 1 over the training set. 20 mixture components were used
to model the oﬀsets, giving a total of 120 mixture parameters per timestep
(20 weights, 40 means, 40 standard deviations and 20 correlations). A further
parameter was used to model the end-of-stroke probability, giving an output
layer of size 121. Two network architectures were compared for the hidden
layers: one with three hidden layers, each consisting of 400 LSTM cells, and one
with a single hidden layer of 900 LSTM cells. Both networks had around 3.4M
weights. The three layer network was retrained with adaptive weight noise [8],
with all std. devs. initialised to 0.075. Training with ﬁxed variance weight noise
proved ineﬀective, probably because it prevented the mixture density layer from
using precisely speciﬁed weights.

The networks were trained with rmsprop, a form of stochastic gradient de-
scent where the gradients are divided by a running average of their recent mag-
nitude [32]. Deﬁne i = ∂L(x)
where wi is network weight i. The weight update
equations were:

∂wi

ni = ℵni + (1 − ℵ)2
gi = ℵgi + (1 − ℵ)i
∆i = (cid:105)∆i − ג
i

(cid:112)ni − g2

i

i + (cid:107)

wi = wi + ∆i

with the following parameters:

(38)

(39)

(40)

(41)

(42)

(43)

(44)

ℵ = 0.95
(cid:105) = 0.9
ג = 0.0001
(cid:107) = 0.0001

(45)
The output derivatives ∂L(x)
were clipped in the range [−100, 100], and the
LSTM derivates were clipped in the range [−10, 10]. Clipping the output gradi-
ents proved vital for numerical stability; even so, the networks sometimes had
numerical problems late on in training, after they had started overﬁtting on the
training data.

∂ ˆyt

Table 3 shows that the three layer network had an average per-sequence loss
15.3 nats lower than the one layer net. However the sum-squared-error was
slightly lower for the single layer network.
the use of adaptive weight noise
reduced the loss by another 16.7 nats relative to the unregularised three layer
network, but did not signiﬁcantly change the sum-squared error. The adaptive
weight noise network appeared to generate the best samples.

23

Table 3: Handwriting Prediction Results. All results recorded on the val-
idation set. ‘Log-Loss’ is the mean value of L(x) (in nats). ‘SSE’ is the mean
sum-squared-error per data point.

Network Regularisation
1 layer
3 layer
3 layer

none
none
adaptive weight noise

Log-Loss SSE
0.40
-1025.7
0.41
-1041.0
-1057.7
0.41

4.3 Samples

Fig. 11 shows handwriting samples generated by the prediction network. The
network has clearly learned to model strokes, letters and even short words (es-
pecially common ones such as ‘of’ and ‘the’). It also appears to have learned a
basic character level language models, since the words it invents (‘eald’, ‘bryoes’,
‘lenrest’) look somewhat plausible in English. Given that the average character
occupies more than 25 timesteps, this again demonstrates the network’s ability
to generate coherent long-range structures.

5 Handwriting Synthesis

Handwriting synthesis is the generation of handwriting for a given text. Clearly
the prediction networks we have described so far are unable to do this, since
there is no way to constrain which letters the network writes. This section de-
scribes an augmentation that allows a prediction network to generate data se-
quences conditioned on some high-level annotation sequence (a character string,
in the case of handwriting synthesis). The resulting sequences are suﬃciently
convincing that they often cannot be distinguished from real handwriting. Fur-
thermore, this realism is achieved without sacriﬁcing the diversity in writing
style demonstrated in the previous section.

The main challenge in conditioning the predictions on the text is that the two
sequences are of very diﬀerent lengths (the pen trace being on average twenty
ﬁve times as long as the text), and the alignment between them is unknown until
the data is generated. This is because the number of co-ordinates used to write
each character varies greatly according to style, size, pen speed etc. One neural
network model able to make sequential predictions based on two sequences of
diﬀerent length and unknown alignment is the RNN transducer [9]. However
preliminary experiments on handwriting synthesis with RNN transducers were
not encouraging. A possible explanation is that the transducer uses two sepa-
rate RNNs to process the two sequences, then combines their outputs to make
decisions, when it is usually more desirable to make all the information avail-
able to single network. This work proposes an alternative model, where a ‘soft
window’ is convolved with the text string and fed in as an extra input to the
prediction network. The parameters of the window are output by the network

24

Figure 11: Online handwriting samples generated by the prediction
network. All samples are 700 timesteps long.

25

at the same time as it makes the predictions, so that it dynamically determines
an alignment between the text and the pen locations. Put simply, it learns to
decide which character to write next.

5.1 Synthesis Network

Fig. 12 illustrates the network architecture used for handwriting synthesis. As
with the prediction network, the hidden layers are stacked on top of each other,
each feeding up to the layer above, and there are skip connections from the
inputs to all hidden layers and from all hidden layers to the outputs. The
diﬀerence is the added input from the character sequence, mediated by the
window layer.
Given a length U character sequence c and a length T data sequence x, the
soft window wt into c at timestep t (1 ≤ t ≤ T ) is deﬁned by the following
discrete convolution with a mixture of K Gaussian functions

(cid:16)−βk

t

t − u(cid:1)2(cid:17)
(cid:0)κk

K(cid:88)
U(cid:88)

k=1

φ(t, u) =

wt =

αk

t exp

φ(t, u)cu

(46)

(47)

u=1

where φ(t, u) is the window weight of cu at timestep t. Intuitively, the κt param-
eters control the location of the window, the βt parameters control the width of
the window and the αt parameters control the importance of the window within
the mixture. The size of the soft window vectors is the same as the size of the
character vectors cu (assuming a one-hot encoding, this will be the number of
characters in the alphabet). Note that the window mixture is not normalised
and hence does not determine a probability distribution; however the window
weight φ(t, u) can be loosely interpreted as the network’s belief that it is writ-
ing character cu at time t. Fig. 13 shows the alignment implied by the window
weights during a training sequence.

The size 3K vector p of window parameters is determined as follows by the

outputs of the ﬁrst hidden layer of the network:

(ˆαt, ˆβt, ˆκt) = Wh1ph1
αt = exp (ˆαt)

t + bp

(cid:17)

(cid:16) ˆβt

βt = exp

κt = κt−1 + exp (ˆκt)

(48)

(49)

(50)

(51)

Note that the location parameters κt are deﬁned as oﬀsets from the previous
locations ct−1, and that the size of the oﬀset is constrained to be greater than
zero. Intuitively, this means that network learns how far to slide each window
at each step, rather than an absolute location. Using oﬀsets was essential to
getting the network to align the text with the pen trace.

26

Figure 12: Synthesis Network Architecture Circles represent layers, solid
lines represent connections and dashed lines represent predictions. The topology
is similar to the prediction network in Fig. 1, except that extra input from the
character sequence c, is presented to the hidden layers via the window layer
(with a delay in the connection to the ﬁrst hidden layer to avoid a cycle in the
graph).

27

Figure 13: Window weights during a handwriting synthesis sequence
Each point on the map shows the value of φ(t, u), where t indexes the pen trace
along the horizontal axis and u indexes the text character along the vertical axis.
The bright line is the alignment chosen by the network between the characters
and the writing. Notice that the line spreads out at the boundaries between
characters; this means the network receives information about next and previous
letters as it makes transitions, which helps guide its predictions.

28

The wt vectors are passed to the second and third hidden layers at time t,
and the ﬁrst hidden layer at time t+1 (to avoid creating a cycle in the processing
graph). The update equations for the hidden layers are

t = H(cid:0)Wih1xt + Wh1h1h1
t = H(cid:0)Wihnxt + Whn−1hn hn−1

h1
hn

t−1 + Wwh1wt−1 + b1
h

t + Whnhn hn

t−1 + Wwhn wt + bn
h

(cid:1)

(cid:1)

The equations for the output layer remain unchanged from Eqs. (17) to (22).
The sequence loss is

L(x) = − log Pr(x|c)

where

Pr(x|c) =

Pr (xt+1|yt)

T(cid:89)

t=1

Note that yt is now a function of c as well as x1:t.
The loss derivatives with respect to the outputs ˆet, ˆπt, ˆµt, ˆσt, ˆρt remain un-
changed from Eqs. (27), (30) and (31). Given the loss derivative ∂L(x)
with
respect to the size W window vector wt, obtained by backpropagating the out-
put derivatives through the computation graph in Fig. 12, the derivatives with
respect to the window parameters are as follows:

∂wt

(52)

(53)

(54)

(55)

(56)

(57)

(58)

(59)

(60)

(cid:16)−βk

t

t − u(cid:1)2(cid:17) W(cid:88)
(cid:0)κk

∂L(x)
∂wj
t

cj
u

j=1

(k, t, u) def= αk

t exp

U(cid:88)

∂L(x)
∂ ˆαk
t
∂L(x)
∂ ˆβk
t
∂L(x)
∂κk
t
∂L(x)
∂ˆκk
t

=

(k, t, u)

u=1

= −βk

t

U(cid:88)

u=1

=

∂L(x)
∂κk

= exp(cid:0)ˆκk

t+1

t

U(cid:88)
(cid:1) ∂L(x)

+ 2βk
t

u=1

∂κk
t

(k, t, u)(κk

t − u)2

(k, t, u)(u − κk
t )

Fig. 14 illustrates the operation of a mixture density output layer applied to

handwriting synthesis.

5.2 Experiments

The synthesis network was applied to the same input data as the handwriting
prediction network in the previous section. The character-level transcriptions
from the IAM-OnDB were now used to deﬁne the character sequences c. The full
transcriptions contain 80 distinct characters (capital letters, lower case letters,
digits, and punctuation). However we used only a subset of 57, with all the

29

Figure 14: Mixture density outputs for handwriting synthesis. The top
heatmap shows the predictive distributions for the pen locations, the bottom
heatmap shows the mixture component weights. Comparison with Fig. 10 indi-
cates that the synthesis network makes more precise predictions (with smaller
density blobs) than the prediction-only network, especially at the ends of strokes,
where the synthesis network has the advantage of knowing which letter comes
next.

30

Table 4: Handwriting Synthesis Results. All results recorded on the val-
idation set.
‘SSE’ is the mean
sum-squared-error per data point.

‘Log-Loss’ is the mean value of L(x) in nats.

Regularisation
none
adaptive weight noise

Log-Loss SSE
0.23
-1096.9
-1128.2
0.23

digits and most of the punctuation characters replaced with a generic ‘non-
letter’ label2.

The network architecture was as similar as possible to the best prediction
network: three hidden layers of 400 LSTM cells each, 20 bivariate Gaussian
mixture components at the output layer and a size 3 input layer. The character
sequence was encoded with one-hot vectors, and hence the window vectors were
size 57. A mixture of 10 Gaussian functions was used for the window parameters,
requiring a size 30 parameter vector. The total number of weights was increased
to approximately 3.7M.

The network was trained with rmsprop, using the same parameters as in
the previous section. The network was retrained with adaptive weight noise,
initial standard deviation 0.075, and the output and LSTM gradients were again
clipped in the range [−100, 100] and [−10, 10] respectively.

Table 4 shows that adaptive weight noise gave a considerable improvement
in log-loss (around 31.3 nats) but no signiﬁcant change in sum-squared error.
The regularised network appears to generate slightly more realistic sequences,
although the diﬀerence is hard to discern by eye. Both networks performed
considerably better than the best prediction network. In particular the sum-
squared-error was reduced by 44%. This is likely due in large part to the im-
proved predictions at the ends of strokes, where the error is largest.

5.3 Unbiased Sampling
Given c, an unbiased sample can be picked from Pr(x|c) by iteratively drawing
xt+1 from Pr (xt+1|yt), just as for the prediction network. The only diﬀerence is
that we must also decide when the synthesis network has ﬁnished writing the text
and should stop making any future decisions. To do this, we use the following
heuristic: as soon as φ(t, U + 1) > φ(t, u) ∀ 1 ≤ u ≤ U the current input xt is
deﬁned as the end of the sequence and sampling ends. Examples of unbiased
synthesis samples are shown in Fig. 15. These and all subsequent ﬁgures were
generated using the synthesis network retrained with adaptive weight noise.
Notice how stylistic traits, such as character size, slant, cursiveness etc. vary

2This was an oversight; however it led to the interesting result that when the text contains
a non-letter, the network must select a digits or punctuation mark to generate. Sometimes
the character can be be inferred from the context (e.g. the apostrophe in “can’t”); otherwise
it is chosen at random.

31

widely between the samples, but remain more-or-less consistent within them.
This suggests that the network identiﬁes the traits early on in the sequence,
then remembers them until the end. By looking through enough samples for a
given text, it appears to be possible to ﬁnd virtually any combination of stylistic
traits, which suggests that the network models them independently both from
each other and from the text.

‘Blind taste tests’ carried out by the author during presentations suggest
that at least some unbiased samples cannot be distinguished from real hand-
writing by the human eye. Nonetheless the network does make mistakes we
would not expect a human writer to make, often involving missing, confused
or garbled letters3; this suggests that the network sometimes has trouble de-
termining the alignment between the characters and the trace. The number of
mistakes increases markedly when less common words or phrases are included
in the character sequence. Presumably this is because the network learns an
implicit character-level language model from the training set that gets confused
when rare or unknown transitions occur.

5.4 Biased Sampling

One problem with unbiased samples is that they tend to be diﬃcult to read
(partly because real handwriting is diﬃcult to read, and partly because the
network is an imperfect model). Intuitively, we would expect the network to
give higher probability to good handwriting because it tends to be smoother
and more predictable than bad handwriting. If this is true, we should aim to
output more probable elements of Pr(x|c) if we want the samples to be easier to
read. A principled search for high probability samples could lead to a diﬃcult
inference problem, as the probability of every output depends on all previous
outputs. However a simple heuristic, where the sampler is biased towards more
probable predictions at each step independently, generally gives good results.
Deﬁne the probability bias b as a real number greater than or equal to zero.
Before drawing a sample from Pr(xt+1|yt), each standard deviation σj
t in the
Gaussian mixture is recalculated from Eq. (21) to

(cid:16)

(cid:17)
t − b
ˆσj

σj
t = exp

and each mixture weight is recalculated from Eq. (19) to

(cid:16)

(cid:17)

(cid:16)

ˆπj
t (1 + b)
ˆπj(cid:48)
t (1 + b)

(cid:17)

πj
t =

exp

(cid:80)M

j(cid:48)=1 exp

(61)

(62)

This artiﬁcially reduces the variance in both the choice of component from the
mixture, and in the distribution of the component itself. When b = 0 unbiased
sampling is recovered, and as b → ∞ the variance in the sampling disappears
3We expect humans to make mistakes like misspelling ‘temperament’ as ‘temperement’, as

the second writer in Fig. 15 seems to have done.

32

Figure 15: Real and generated handwriting. The top line in each block is
real, the rest are unbiased samples from the synthesis network. The two texts
are from the validation set and were not seen during training.

33

and the network always outputs the mode of the most probable component in
the mixture (which is not necessarily the mode of the mixture, but at least a
reasonable approximation). Fig. 16 shows the eﬀect of progressively increasing
the bias, and Fig. 17 shows samples generated with a low bias for the same texts
as Fig. 15.

5.5 Primed Sampling

Another reason to constrain the sampling would be to generate handwriting
in the style of a particular writer (rather than in a randomly selected style).
The easiest way to do this would be to retrain it on that writer only. But
even without retraining, it is possible to mimic a particular style by ‘priming’
the network with a real sequence, then generating an extension with the real
sequence still in the network’s memory. This can be achieved for a real x, c and
a synthesis character string s by setting the character sequence to c(cid:48) = c + s
and clamping the data inputs to x for the ﬁrst T timesteps, then sampling
as usual until the sequence ends. Examples of primed samples are shown in
Figs. 18 and 19. The fact that priming works proves that the network is able to
remember stylistic features identiﬁed earlier on in the sequence. This technique
appears to work better for sequences in the training data than those the network
has never seen.

Primed sampling and reduced variance sampling can also be combined. As
shown in Figs. 20 and 21 this tends to produce samples in a ‘cleaned up’ version
of the priming style, with overall stylistic traits such as slant and cursiveness
retained, but the strokes appearing smoother and more regular. A possible
application would be the artiﬁcial enhancement of poor handwriting.

6 Conclusions and Future Work

This paper has demonstrated the ability of Long Short-Term Memory recur-
rent neural networks to generate both discrete and real-valued sequences with
complex, long-range structure using next-step prediction. It has also introduced
a novel convolutional mechanism that allows a recurrent network to condition
its predictions on an auxiliary annotation sequence, and used this approach to
synthesise diverse and realistic samples of online handwriting. Furthermore, it
has shown how these samples can be biased towards greater legibility, and how
they can be modelled on the style of a particular writer.

Several directions for future work suggest themselves. One is the applica-
tion of the network to speech synthesis, which is likely to be more challenging
than handwriting synthesis due to the greater dimensionality of the data points.
Another is to gain a better insight into the internal representation of the data,
and to use this to manipulate the sample distribution directly. It would also
be interesting to develop a mechanism to automatically extract high-level an-
notations from sequence data. In the case of handwriting, this could allow for

34

Figure 16: Samples biased towards higher probability. The probability
biases b are shown at the left. As the bias increases the diversity decreases and
the samples tend towards a kind of ‘average handwriting’ which is extremely
regular and easy to read (easier, in fact, than most of the real handwriting in the
training set). Note that even when the variance disappears, the same letter is
not written the same way at diﬀerent points in a sequence (for examples the ‘e’s
in “exactly the same”, the ‘l’s in “until they all look”), because the predictions
are still inﬂuenced by the previous outputs. If you look closely you can see that
the last three lines are not quite exactly the same.

35

Figure 17: A slight bias. The top line in each block is real. The rest are
samples from the synthesis network with a probability bias of 0.15, which seems
to give a good balance between diversity and legibility.

36

Figure 18: Samples primed with real sequences. The priming sequences
(drawn from the training set) are shown at the top of each block. None of the
lines in the sampled text exist in the training set. The samples were selected
for legibility.

37

Figure 19: Samples primed with real sequences (cotd).

38

Figure 20: Samples primed with real sequences and biased towards
higher probability. The priming sequences are at the top of the blocks. The
probability bias was 1. None of the lines in the sampled text exist in the training
set.

39

Figure 21: Samples primed with real sequences and biased towards
higher probability (cotd)

40

more nuanced annotations than just text, for example stylistic features, diﬀerent
forms of the same letter, information about stroke order and so on.

Acknowledgements

Thanks to Yichuan Tang, Ilya Sutskever, Navdeep Jaitly, Geoﬀrey Hinton and
other colleagues at the University of Toronto for numerous useful comments
and suggestions. This work was supported by a Global Scholarship from the
Canadian Institute for Advanced Research.

References

[1] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies
with gradient descent is diﬃcult. IEEE Transactions on Neural Networks,
5(2):157–166, March 1994.

[2] C. Bishop. Mixture density networks. Technical report, 1994.

[3] C. Bishop. Neural Networks for Pattern Recognition. Oxford University

Press, Inc., 1995.

[4] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Modeling tempo-
ral dependencies in high-dimensional sequences: Application to polyphonic
music generation and transcription. In Proceedings of the Twenty-nine In-
ternational Conference on Machine Learning (ICML’12), 2012.

[5] J. G. Cleary, Ian, and I. H. Witten. Data compression using adaptive cod-
ing and partial string matching. IEEE Transactions on Communications,
32:396–402, 1984.

[6] D. Eck and J. Schmidhuber. A ﬁrst look at music composition using lstm
recurrent neural networks. Technical report, IDSIA USI-SUPSI Instituto
Dalle Molle.

[7] F. Gers, N. Schraudolph, and J. Schmidhuber. Learning precise timing
with LSTM recurrent networks. Journal of Machine Learning Research,
3:115–143, 2002.

[8] A. Graves. Practical variational inference for neural networks. In Advances
in Neural Information Processing Systems, volume 24, pages 2348–2356.
2011.

[9] A. Graves. Sequence transduction with recurrent neural networks. In ICML

Representation Learning Worksop, 2012.

[10] A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep

recurrent neural networks. In Proc. ICASSP, 2013.

41

[11] A. Graves and J. Schmidhuber. Framewise phoneme classiﬁcation with bidi-
rectional LSTM and other neural network architectures. Neural Networks,
18:602–610, 2005.

[12] A. Graves and J. Schmidhuber. Oﬄine handwriting recognition with multi-
dimensional recurrent neural networks. In Advances in Neural Information
Processing Systems, volume 21, 2008.

[13] P. D. Gr¨unwald. The Minimum Description Length Principle (Adaptive

Computation and Machine Learning). The MIT Press, 2007.

[14] G. Hinton. A Practical Guide to Training Restricted Boltzmann Machines.

Technical report, 2010.

[15] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient Flow
in Recurrent Nets: the Diﬃculty of Learning Long-term Dependencies.
In S. C. Kremer and J. F. Kolen, editors, A Field Guide to Dynamical
Recurrent Neural Networks. 2001.

[16] S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. Neural

Computation, 9(8):1735–1780, 1997.

[17] M. Hutter. The Human Knowledge Compression Contest, 2012.

[18] K.-C. Jim, C. Giles, and B. Horne. An analysis of noise in recurrent neural
networks: convergence and generalization. Neural Networks, IEEE Trans-
actions on, 7(6):1424 –1438, 1996.

[19] S. Johansson, R. Atwell, R. Garside, and G. Leech. The tagged LOB corpus

user’s manual; Norwegian Computing Centre for the Humanities, 1986.

[20] B. Knoll and N. de Freitas. A machine learning perspective on predictive

coding with paq. CoRR, abs/1108.3298, 2011.

[21] M. Liwicki and H. Bunke.

IAM-OnDB - an on-line English sentence
database acquired from handwritten text on a whiteboard. In Proc. 8th
Int. Conf. on Document Analysis and Recognition, volume 2, pages 956–
961, 2005.

[22] M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. Building a large
annotated corpus of english: The penn treebank. COMPUTATIONAL
LINGUISTICS, 19(2):313–330, 1993.

[23] T. Mikolov. Statistical Language Models based on Neural Networks. PhD

thesis, Brno University of Technology, 2012.

[24] T. Mikolov, I. Sutskever, A. Deoras, H. Le, S. Kombrink, and J. Cernocky.
Subword language modeling with neural networks. Technical report, Un-
published Manuscript, 2012.

42

[25] A. Mnih and G. Hinton. A Scalable Hierarchical Distributed Language
Model. In Advances in Neural Information Processing Systems, volume 21,
2008.

[26] A. Mnih and Y. W. Teh. A fast and simple algorithm for training neural
In Proceedings of the 29th International

probabilistic language models.
Conference on Machine Learning, pages 1751–1758, 2012.

[27] T. N. Sainath, A. Mohamed, B. Kingsbury, and B. Ramabhadran. Low-
rank matrix factorization for deep neural network training with high-
dimensional output targets. In Proc. ICASSP, 2013.

[28] M. Schuster. Better generative models for sequential data problems: Bidi-
rectional recurrent mixture density networks. pages 589–595. The MIT
Press, 1999.

[29] I. Sutskever, G. E. Hinton, and G. W. Taylor. The recurrent temporal

restricted boltzmann machine. pages 1601–1608, 2008.

[30] I. Sutskever, J. Martens, and G. Hinton. Generating text with recurrent

neural networks. In ICML, 2011.

[31] G. W. Taylor and G. E. Hinton. Factored conditional restricted boltzmann
machines for modeling motion style. In Proc. 26th Annual International
Conference on Machine Learning, pages 1025–1032, 2009.

[32] T. Tieleman and G. Hinton. Lecture 6.5 - rmsprop: Divide the gradient by

a running average of its recent magnitude, 2012.

[33] R. Williams and D. Zipser. Gradient-based learning algorithms for recur-
rent networks and their computational complexity. In Back-propagation:
Theory, Architectures and Applications, pages 433–486. 1995.

43

