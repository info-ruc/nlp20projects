Generative Visual Manipulation
on the Natural Image Manifold

Jun-Yan Zhu1, Philipp Kr¨ahenb¨uhl1, Eli Shechtman2, and Alexei A. Efros1

University of California, Berkeley1

Adobe Research2

Abstract. Realistic image manipulation is challenging because it re-
quires modifying the image appearance in a user-controlled way, while
preserving the realism of the result. Unless the user has considerable
artistic skill, it is easy to “fall oﬀ” the manifold of natural images while
editing. In this paper, we propose to learn the natural image manifold di-
rectly from data using a generative adversarial neural network. We then
deﬁne a class of image editing operations, and constrain their output
to lie on that learned manifold at all times. The model automatically
adjusts the output keeping all edits as realistic as possible. All our ma-
nipulations are expressed in terms of constrained optimization and are
applied in near-real time. We evaluate our algorithm on the task of real-
istic photo manipulation of shape and color. The presented method can
further be used for changing one image to look like the other, as well as
generating novel imagery from scratch based on user’s scribbles1.

1 Introduction

8
1
0
2
 
c
e
D
6
1

 

 
 
]

V
C
.
s
c
[
 
 

3
v
2
5
5
3
0

.

9
0
6
1
:
v
i
X
r
a

Today, visual communication is sadly one-sided. We all perceive information in
the visual form (through photographs, paintings, sculpture, etc.), but only a
chosen few are talented enough to eﬀectively express themselves visually. This
imbalance manifests itself even in the most mundane tasks. Consider an online
shopping scenario: a user looking for shoes has found a pair that mostly suits
her but she would like them to be a little taller, or wider, or in a diﬀerent color.
How can she communicate her preference to the shopping website? If the user is
also an artist, then a few minutes with an image editing program will allow her
to transform the shoe into what she wants, and then use image-based search to
ﬁnd it. However, for most of us, even a simple image manipulation in Photoshop
presents insurmountable diﬃculties. One reason is the lack of “safety wheels”
in image editing: any less-than-perfect edit immediately makes the image look
completely unrealistic. To put another way, classic visual manipulation paradigm
does not prevent the user from “falling oﬀ” the manifold of natural images.

Understanding and modeling the natural image manifold has been a long-
standing open research problem. But in the last two years, there has been rapid
advancement, fueled largely by the development of the generative adversarial

1 The supplemental video, code, and models are available at our website.

2

Jun-Yan Zhu, Philipp Kr¨ahenb¨uhl, Eli Shechtman, Alexei A. Efros

Fig. 1. We use generative adversarial networks (GAN) [1,2] to perform image editing
on the natural image manifold. We ﬁrst project an original photo (a) onto a low-
dimensional latent vector representation (b) by regenerating it using GAN. We then
modify the color and shape of the generated image (d) using various brush tools (c)
(for example, dragging the top of the shoe). Finally, we apply the same amount of
geometric and color changes to the original photo to achieve the ﬁnal result (e). See
interactive image editing demo on Youtube.

networks [1]. In particular, several recent papers [1,2,3,4,5] have shown visually
impressive results sampling random images drawn from the natural image mani-
fold. However, two reasons prevent these advances from being useful in practical
applications at this time. First, the generated images, while good, are still not
quite photo-realistic (plus there are practical issues in making them high resolu-
tion). Second, these generative models are set up to produce images by sampling
a latent vector-space, typically at random. So, these methods are not able to
create and manipulate visual content in a user-controlled fashion.

In this paper, we use the generative adversarial neural network to learn the
manifold of natural images, but we do not employ it for image generation. In-
stead, we use it as a constraint on the output of various image manipulation
operations, to make sure the results lie on the learned manifold at all times.
This idea enables us to reformulate several editing operations, speciﬁcally color
and shape manipulations, in a natural and data-driven way. The model auto-
matically adjusts the output keeping all edits as realistic as possible (Figure 1).

We show three applications based on our system: (1) Manipulating an exist-
ing photo based on an underlying generative model to achieve a diﬀerent look
(shape and color); (2) “Generative transformation” of one image to look more
like another; (3) Generate a new image from scratch based on user’s scribbles
and warping UI.

All manipulations are performed straightforwardly through gradient-based
optimization, resulting in a simple and fast image editing tool. We hope that
this work inspires further research in data-driven generative image editing, and
thus release the code and data at our website.

Generative Visual Manipulation on the Natural Image Manifold

3

2 Prior Work

Image editing and user interaction: Image editing is a well-established area
in computer graphics where an input image is manipulated to achieve a certain
goal speciﬁed by the user. Examples of basic editing include changing the color
properties of an image either globally [6] or locally [7]. More advanced editing
methods such as image warping [8,9] or structured image editing [10] intelligently
reshuﬄe the pixels in an image following user’s edits. While achieving impressive
results in the hands of an expert, when these types of methods fail, they produce
results that look nothing like a real image. Common artifacts include unrealistic
colors, exaggerated stretching, obvious repetitions, and over-smoothing. This is
because they rely on low-level principles (e.g., the similarity of color, gradients
or patches) and do not capture higher-level information about natural images.
Image morphing: There are several techniques for producing a smooth vi-
sual transition between two input images. Traditional morphing methods [11]
combine an intensity blend with a geometric warp that requires a dense corre-
spondence. In Regenerative Morphing [12] the output sequence is regenerated
from small patches sampled from the source images. Thus, each frame is con-
strained to look similar to the two sources. Exploring Photobios [13] presented
an alternative way to transition between images, by ﬁnding the shortest path in
a large image collection based on pairwise image distances. Here we extend this
idea and produce a morph that is both close to the two sources and stays on, or
close to, the natural image manifold.
Natural image statistics: Generative models of local image statistics have
long been used as a prior for image restoration problems such as image denois-
ing and deblurring. A common strategy is to learn local ﬁlter or patch models,
such as Principal Components, Independent Components, Mixture of Gaussians
or wavelet bases [14,15,16]. Some methods attempt to capture full-image likeli-
hoods [17] through dense patch overlap, though the basic building block is still
small patches that do not capture global image structures and long-range rela-
tions. Zhu et al. [18] recently showed that discriminative deep neural networks
learn a much stronger prior that captures both low-level statistics, as well as
higher order semantic or color-balance clues. This deep prior can be directly
used for a limited set of editing operations (e.g. compositing). However, it does
not extend to the diversity of editing operations considered in this work.
Neural generative models: There is a large body of work on neural network
based models for image generation. Early classes of probabilistic models of im-
ages include restricted Boltzmann machines (e.g., [19]) and their deep variants
[20], auto-encoders [19,21] and more recently, stochastic neural networks [22,3,23]
and deterministic networks [24]. Generative adversarial networks (GAN), pro-
posed by Goodfellow et al. [1], learn a generative network jointly with a second
discriminative adversarial network in a mini-max objective. The discriminator
tries to distinguish between the generated samples and natural image samples,
while the generator tries to fool the discriminator producing highly realistic
looking images. Unfortunately, in practice, GAN does not yield a stable training

4

Jun-Yan Zhu, Philipp Kr¨ahenb¨uhl, Eli Shechtman, Alexei A. Efros

Fig. 2. GAN as a manifold approximation. (a) Randomly generated examples from a
GAN, trained on the shirts dataset; (b) random jittering: each row shows a random
sample from a GAN (the ﬁrst one at the left), and its variants produced by adding
Gaussian noise to z in the latent space; (c) interpolation: each row shows two randomly
generated images (ﬁrst and last), and their smooth interpolations in the latent space.

objective, so several modiﬁcations have been proposed recently, such as a multi-
scale generation [4] and a convolution-deconvolution architecture with batch
normalization [2]. While the above methods attempt to generate an image start-
ing from a random vector, they do not provide tools to change the generation
process with intuitive user controls. In this paper, we remedy this by learning a
generative model that can be easily controlled via a few intuitive user edits.

3 Learning the Natural Image Manifold

Let us assume that all natural images lie on an ideal low-dimensional manifold
M with a distance function S(x1, x2) that measures the perceptual similarity
between two images x1, x2 ∈ M. Directly modeling this ideal manifold M is ex-
tremely challenging, as it involves training a generative model in a highly struc-
tured and complex million dimensional space. Following the recent success of
deep generative networks in generating natural looking images, we approximate
the image manifold by learning a model using generative adversarial networks
(GAN) [1,2] from a large-scale image collection. In addition to the high-quality
results, GAN has a few other useful properties for our task we will discuss next.
Generative Adversarial Networks: A GAN model consists of two neural net-
works: (1) a generative network G(z; θg) that generates an image x ∈ RH×W×C
given a random vector z ∈ Z, where Z denotes a d-dimensional latent space,
and (2) a discriminative network D(x; θd) that predicts a probability of a photo
being real (D = 1) or generated (D = 0). For simplicity, we denote G(z; θG)
and D(x; θD) as G(z) and D(x) in later sections. One common choice of Z is
a multivariate uniform distribution U nif [−1, 1]d. D and G are learned using
a min-max objective [1]. GAN works well when trained on images of a certain
class. We formally deﬁne ˜M = {G(z)|z ∈ Z} and use it as an approximation to
the ideal manifold M (i.e ˜M ≈ M). We also approximate the distance function
of two generated images as an Euclidean distance between their corresponding
latent vectors, i.e., S(G(z1), G(z2)) ≈ (cid:107)z1 − z2(cid:107)2.
GAN as a manifold approximation: We use GAN to approximate an ideal
manifold for two reasons: ﬁrst, it produces high-quality samples (see Figure 2 (a)

Generative Visual Manipulation on the Natural Image Manifold

5

for example). Though lacking visual details sometimes, the model can synthesize
appealing samples with a plausible overall structure. Second, the Euclidean dis-
tance in the latent space often corresponds to a perceptually meaningful visual
similarity (see Figure 2 (b) for examples). Therefore, we argue that GAN is a
powerful generative model for modeling the image manifold.
Traversing the manifold: Given two images on the manifold G(z0), G(zN )) ∈

˜M, one would like to seek a sequence of N + 1 images(cid:2)G(z0), G(z1), . . . G(zN )(cid:3)
minimize(cid:80)N−1
S(G(z1), G(z2)) ≈ (cid:107)z1 − z2(cid:107)2 , so a simple linear interpolation(cid:2)(1− t
(cid:3)N

with a smooth transition. This is often done by constructing an image graph with
images as nodes, and pairwise distance function as the edge, and computing a
shortest path between the starting image and end image [13]. In our case, we
t=0 S(G(zt), G(zt+1)) where S is the distance function. In our case
N )· z0 + t
N ·
zN
t=0 is the shortest path. Figure 2 (c) shows a smooth and meaningful image
sequence generated by interpolating between two points in the latent space. We
will now use this approximation of the manifold of natural images for realistic
photo editing.

4 Approach

Figure 1 illustrates the overview of our approach. Given a real photo, we ﬁrst
project it onto our approximation of the image manifold by ﬁnding the closest
latent feature vector z of the GAN to the original image. Then, we present a real-
time method for gradually and smoothly updating the latent vector z so that it
generates the desired image that both satisﬁes the user’s edits (e.g., a scribble or
a warp; more details in Section 5) and stays close to the natural image manifold.
Unfortunately, in this transformation, the generative model usually looses some
of the important low-level details of the input image. Therefore, we propose
a dense correspondence method that estimates both per-pixel color and shape
changes from the edits applied to the generative model. We then transfer these
changes to the original photo using an edge-aware interpolation technique and
produce the ﬁnal manipulated result.

4.1 Projecting an Image onto the Manifold
A real photo xR lies, by deﬁnition, on the ideal image manifold M. However for
an approximate manifold ˜M, our goal here is to ﬁnd a generated image x∗ ∈ ˜M
close to xR in some distance metric L(x1, x2) as

x∗ = arg min
x∈ ˜M

L(x, xR).

For the GAN manifold ˜M we can rewrite the above equation as follows:

z∗ = arg min

z∈˜Z

L(G(z), xR).

(1)

(2)

6

Jun-Yan Zhu, Philipp Kr¨ahenb¨uhl, Eli Shechtman, Alexei A. Efros

Our goal is to reconstruct the original photo xR using the generative model G
by minimizing the reconstruction error, where L(x1, x2) = (cid:107)C(x1) − C(x2)(cid:107)2 in
some diﬀerentiable feature space C. If C(x) = x, then the reconstruction error is
simply pixel-wise Euclidean error. Previous work [5,25] suggests that using deep
neural network activations leads to a reconstruction of perceptually meaningful
details. We found that a weighted combination of raw pixels and conv4 features
(×0.002) extracted from AlexNet [26] trained on ImageNet [27] to perform best.
Projection via optimization: As both the feature extractor C and the gen-
erative model G are diﬀerentiable, we can directly optimize the above objective
using L-BFGS-B [28]. However, the cascade of C(G(z)) makes the problem highly
non-convex, and as a result, the reconstruction quality strongly relies on a good
initialization of z. We can start from multiple random initializations and output
the solution with the minimal cost. However, the number of random initial-
izations required to obtain a stable reconstruction is prohibitively large (more
than 100), which makes real-time processing impossible. We instead train a deep
neural network to minimize equation 2 directly.
Projection via a feedforward network: We train a feedforward neural net-
work P (x; θP ) that directly predicts the latent vector z from a x. The training
objective for the predictive model P is written as follows:

(cid:88)

n

θ∗
P = arg min

θP

L(G(P (xR

n ; θP )), xR

n ),

(3)

where xR
n denotes the n-th image in the dataset. The architecture of the model
P is equivalent to the discriminator D of the adversarial networks, and only
varies in the ﬁnal number of network outputs. Objective 3 is reminiscent of an
auto-encoder pipeline, with a encoder P and decoder G. However, the decoder G
is ﬁxed throughout the training. While the optimization problem 2 is the same
as the learning objective 3, the learning-based approach often performs better
and does not fall into local optima. We attribute this behavior to the regularity
in the projection problem and the limited capacity of the network P . Projections
of similar images will share similar network parameters and produce a similar
result. In some sense, the loss for one image provides information for many more
images that share a similar appearance [29]. However, the learned inversion is
not always perfect, and can often be improved further by a few additional steps
of optimization.
A hybrid method: The hybrid method takes advantage of both approaches
above. Given a real photo xR, we ﬁrst predict P (xR; θP ) and then use it as the
initialization for the optimization objective (Equation 2). So the learned pre-
dictive model serves as a fast bottom-up initialization method for a non-convex
optimization problem. Figure 3 shows a comparison of these three methods. See
Section 7.4 for a more quantitative evaluation.

4.2 Manipulating the Latent Vector
0 projected onto the manifold ˜M as x0 = G(z0) via the projec-
With the image xR
tion methods just described, we can start modifying the image on that manifold.

Generative Visual Manipulation on the Natural Image Manifold

7

Fig. 3. Projecting real photos onto the image manifold using GAN. Top row: origi-
nal photos (from handbag dataset); 2nd row: reconstruction using optimization-based
method; 3rd row: reconstruction via learned deep encoder P ; bottom row: reconstruc-
tion using the hybrid method (ours). We show the reconstruction loss below each image.

We update the initial projection x0 by simultaneously matching the user inten-
tions while staying on the manifold, close to the original image x0.

Each editing operation is formulated as a constraint fg(x) = vg on a local
part of the output image x. The editing operations g include color, shape and
warping constraints, and are further described in Section 5.1. Given an initial
projection x0, we ﬁnd a new image x ∈ M close to x0 trying to satisfy as many
constraints as possible

x∗ = arg min
x∈M

(cid:110)(cid:88)
(cid:124)

g

(cid:107)fg(x) − vg(cid:107)2

+ λs · S(x, x0)

(cid:124)

(cid:123)(cid:122)

manifold

smoothness

(cid:125)

(cid:123)(cid:122)

data term

(cid:111)

,

(cid:125)

(4)

where the data term measures deviation from the constraint and the smoothness
term enforces moving in small steps on the manifold, so that the image content
is not altered too much. We set λs = 5 in our experiments.

The above equation simpliﬁes to the following on the approximate GAN

manifold ˜M:

z∗ = arg min

z∈Z

(cid:110)(cid:88)
(cid:124)

g

(cid:107)fg(G(z)) − vg(cid:107)2

+ λs · (cid:107)z − z0(cid:107)2

+ED

.

(5)

(cid:123)(cid:122)

data term

(cid:124)

(cid:125)

(cid:123)(cid:122)

(cid:125)

manifold

smoothness

(cid:111)

Here the last term ED = λD · log(1 − D(G(z))) optionally captures the visual
realism of the generated output as judged by the GAN discriminator D. This
constraint further pushes the image towards the manifold of natural images and
slightly improves the visual quality of the result. By default, we turn oﬀ this
term to increase frame rates.
Gradient descent update: For most constraints Equation 5 is non-convex.
We solve it using gradient descent, which allows us to provide the user with

8

Jun-Yan Zhu, Philipp Kr¨ahenb¨uhl, Eli Shechtman, Alexei A. Efros

Fig. 4. Updating latent vector given user edits. (a) Evolving user constraint vg (black
color strokes) at each update step; (b) intermediate results at each update step (G(z0)
at leftmost, and G(z1) at rightmost); (c) a smooth linear interpolation in latent space
between G(z0) and G(z1).

a real-time feedback as she manipulates the image. As a result, the objective
5 evolves in real-time as well. For computational reasons, we only perform a
few gradient descent updates after changing the constraints vg. Each update
step takes 50 − 100 ms, which ensures interactive feedback. Figure 4 shows one
example of the update of z. Given an initial red shoe as shown in Figure 4, the
user gradually scribbles a black color stroke (i.e., speciﬁes a region is black) on
the shoe image (Figure 4 a). Then our update method smoothly changes the
image appearance (Figure 4 b) by adding more and more of the user constraints.
Once the ﬁnal result G(z1) is computed, a user can see the interpolation sequence
between the initial point z0 and z1 (Figure 4 c), and select any intermediate result
as the new starting point. Please see the supplemental video for more details.

While this editing framework allows us to modify any generated image on the
approximate natural image manifold ˜M, it does not directly provide us with a
way to alter the original high-resolution image xR
0 . In the next section, we show
how edits on the approximate manifold can be transferred to the original image.

4.3 Edit Transfer

Give the original photo xR
0 (e.g., a black shoe) and its projection on the manifold
G(z0), and a user modiﬁcation G(z1) by our method (e.g., the generated red
shoe). The generated image G(z1) captures the roughly change we want, albeit
the quality is degraded w.r.t the original image.

Can we instead adjust the original photo and produce a more photo-realistic
result xR
1 that exhibits the changes in the generated image? A straightforward
0 + (G(z1) − G(z0)).
way is to transfer directly the pixel changes (i.e., xR
We tried this approach, and it introduced new artifacts due to the misalignment

1 = xR

Generative Visual Manipulation on the Natural Image Manifold

9

Fig. 5. Edit transfer via Motion+Color Flow. Following user edits on the left shoe
G(z0) we obtain an interpolation sequence in the generated latent space G(z) (top
right). We then compute the motion and color ﬂows (right middle and bottom) between
neighboring images in G(z). These ﬂows are concatenated and, as a validation, can be
applied on G(z0) to obtain a close reconstruction of G(z) (left middle). The bottom left
row shows how the edit is transferred to the original shoe using the same concatenated
ﬂow, to obtain a sequence of edited shoes.

of the two images. To address this issue, we develop a dense correspondence
algorithm to estimate both the geometric and color changes induced by the
editing process.

any number of intermediate frames(cid:2)G((1− t

Speciﬁcally, given two generated images G(z0) and G(z1), we can generate
t=0, where consecutive

N ·z1)(cid:3)N

N )·z0+ t

frames only exhibit minor visual variations.
Motion+Color ﬂow algorithm: We then estimate the color and geomet-
ric changes by generalizing the brightness constancy assumption in traditional
optical ﬂow methods [30,31]. This results in the following motion+color ﬂow
objective2:

(cid:107)I(x, y, t)−A·I(x+u, y+v, t+1)(cid:107)2

+ σs((cid:107)∇u(cid:107)2+(cid:107)∇v(cid:107)2)

+ σc(cid:107)∇A(cid:107)2

dxdy, (6)

(cid:123)(cid:122)

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:90)(cid:90)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:124)

data term

spatial reg

color reg

N ) · z0 + t

where I(x, y, t) denotes the RGB values (r, g, b, 1)T of pixel (x, y) in the generated
image G((1 − t
N · z1). (u, v) is the ﬂow vector with respect to the
change of t, and A denotes a 3 × 4 color aﬃne transformation matrix. The data
term relaxes the color constancy assumption by introducing a locally aﬃne color
transfer model A [32] while the spatial and color regularization terms encourage
smoothness in both the motion and color change. We solve the objective by
iteratively estimating the ﬂow (u, v) using a traditional optical ﬂow algorithm,
and computing the color change A by solving a system of linear equations [32].
We iterate 3 times. We produce 8 intermediate frames (i.e., N = 7).

We estimate the changes between nearby frames, and concatenate these
changes frame by frame to obtain long-range changes between any two frames
along the interpolation sequence z0 → z1. Figure 5 shows a warping sequence
after we apply the ﬂow to the initial projection G(z0).

2 For simplicity, we omit the pixel subscript (x, y) for all the variables.

10

Jun-Yan Zhu, Philipp Kr¨ahenb¨uhl, Eli Shechtman, Alexei A. Efros

Transfer edits to the original photo: After estimating the color and shape
changes in the generated image sequence, we apply them to the original photo
and produce an interesting transition sequence of photo-realistic images as shown
in Figure 5. As the resolution of the ﬂow and color ﬁelds are limited to the
resolution of the generated image (64 × 64), we upsample those edits using a
guided image ﬁlter [33].

5 User Interface

The user interface consists of the main window showing the current edited photo,
a display showing thumbnails of all the candidate results, and a slider bar to ex-
plore the interpolation sequence between the original photo and the ﬁnal result.
Please see our supplemental video for more details.
Candidate results: Given the objective (Equation 5) derived with the user
guidance, we generate multiple diﬀerent results by initializing z as random per-
turbations of z0. We generate 64 examples and show the best 9 results sorted by
the objective cost (Equation 5).
Relative edits: Once a user ﬁnishes one edit, she can drag a slider to see all the
intermediate results interpolated between the original and the ﬁnal manipulated
photo. We call this “relative edits” as it allows a user to explore more alternatives
with a single edit. Similar to relative attributes [34], a user can express ideas like
changing the handle of the handbag to be more red, or making the heel of the
shoes slightly higher, without committing to a speciﬁc ﬁnal state.

5.1 Editing constraints

Our system provides three constraints for editing the photo in diﬀerent aspects:
coloring, sketching and warping. We express all constraints as brush tools. In the
following, we explain the usage of each brush and the corresponding constraints.
Coloring brush: The coloring brush allows the user to change the color of a
speciﬁc region. The user selects a color from a palette and can adjust the brush
size. For each pixel marked with this brush we constrain the color fg(I) = Ip = vg
of a pixel p to the selected values vg.
Sketching brush: The sketching brush allows the user to outline the shape or
add ﬁne details. We constrain fg(I) = HOG(I)p a diﬀerentiable HOG descrip-
tor [35] at a certain location p in the image to be close to the user stroke (i.e.
vg = HOG(stroke)p). We chose the HOG feature extractor because it is binned,
which makes it robust to sketching inaccuracies.
Warping brush: The warping brush allows the user to modify the shape more
explicitly. The user ﬁrst selects a local region (a window with adjustable size),
and then drag it to another location. We then place both a color and sketching
constraint on the displaced pixels encouraging the target patch to mimic the
appearance of the dragged region.

Figure 8 shows a few examples where we use the coloring and sketching
brushed for interactive image generation. Figure 1 shows the result of the warping

Generative Visual Manipulation on the Natural Image Manifold

11

brush that was used to pull the top line of the shoe up. Figure 6 shows a few
more examples.

6 Implementation Details

Network architecture: We follow the same architecture of deep convolutional
generative adversarial networks (DCGAN) [2]. DCGAN mainly builds on multi-
ple convolution, deconvolution and ReLU layers, and eases the min-max training
via batch normalization [36]. We train the generator G to produce a 64 × 64 × 3
image given a 100-dimensional random vector. Notice that our method can also
use other generative models (e.g. variational auto-encoder [3] or future improve-
ments in this area) to approximate the natural image manifold.
Computational time: We run our system on a Titan X GPU. Each update
of the vector z takes 50 ∼ 100 milliseconds, which allows the real-time image
editing and generation. Once an edit is ﬁnished, it takes 5 ∼ 10 seconds for our
edit transfer method to produce high-resolution ﬁnal result.

7 Results

We ﬁrst introduce the statistics of our dataset. We then show three main ap-
plications: realistic image manipulation, generative image transformation, and
generating a photo from scratch using our brush tools. Finally, we evaluate our
image reconstruction methods and perform a human perception study to under-
stand the realism of generated results. Please refer to the supplementary material
for more results and comparisons.

Datasets: We experiment with multiple photo collections from various sources
as follows: “shoes” dataset [37], which has 50K shoes collected from Zappos.com
(the shoes are roughly centered, but not well aligned, and roughly facing left,
with frontal to side view); “church outdoor” dataset (126K images) from the
LSUN challenge [38]; “outdoor natural” images (150K) from the MIT Places
dataset [39]; and two query-based product collections downloaded from Amazon,
including “handbags” (138K) and “shirts” (137K). The downloaded handbags
and shirts are roughly centered but no further alignment has been performed.

7.1 Image Manipulation

Our main application is photo-realistic image manipulation using the brush in-
teractions described in Section 5.1. See Figure 6 for a few examples where the
brush edits are depicted on the left (dashed line for the sketch tool, color scribble
for the color brush and a red square and an arrow for the warp tool). See the
supplementary video for more interactive manipulation demos.

12

Jun-Yan Zhu, Philipp Kr¨ahenb¨uhl, Eli Shechtman, Alexei A. Efros

Fig. 6. Image manipulation examples: for each example, we show the original photo
and user edits on the left. The top row on the right shows the generated sequence and
the bottom row shows the edit transfer sequence on the original image.

7.2 Generative Image Transformation

An interesting outcome of the editing process is the sequence of intermediate
generated images that can be seen as a new kind of image morphing [11,40,12].
We call it “generative transformation”. We use this sequence to transform the
shape and color of one image to look like another image automatically, i.e., with-
out any user edits. This manipulation is done by applying the motion+color ﬂow
on either of the sources. Figure 7 shows a few “generative transform” examples.

7.3

Interactive Image Generation

Another byproduct of our method is that if there is no image to begin with and
all we have are the user brush strokes, the method would generate a natural
image that best satisﬁes the user constraints. This could be useful for dataset
exploration and browsing. The diﬀerence with previous sketch-to-image retrieval
methods [41] or AverageExplorer [42] is that due to potentially contradicting user
constraints, the result may look very diﬀerent than any single image from the
dataset or an average of such images, and more of a realistic hybrid image [43].
See some examples in Figure 8.

7.4 Evaluation

Image reconstruction evaluation: We evaluate three image reconstruction
methods described in Section 4.1: optimization-based, network-based and our

Generative Visual Manipulation on the Natural Image Manifold

13

Fig. 7. Generative image transformation. In both rows, the source on the left is trans-
formed to have the shape and color (or just shape in the 2nd example) of the one on
the right.

Fig. 8. Interactive image generation. The user uses the brush tools to generate an image
from scratch (top row) and then keeps adding more scribbles to reﬁne the result (2nd
and 3rd rows). In the last row, we show the most similar real images to the generated
images. (dashed line for the sketch tool, and color scribble for the color brush)

Shoes Church Outdoor Outdoor Natural Handbags Shirts
Optimization-based 0.155
0.284
0.299
0.210
Network-based
0.302
0.265
Hybrid (ours)
0.140
0.242 0.184
Table 1. Average per-dataset image reconstruction error measured by L(x, xR).

0.319
0.338
0.250

0.176
0.198
0.145

14

Jun-Yan Zhu, Philipp Kr¨ahenb¨uhl, Eli Shechtman, Alexei A. Efros

hybrid approach that combines the last two. We run these on 500 test images
per category, and evaluate them by the reconstruction error L(x, xR) deﬁned
in Equation 1. Table 1 shows the mean reconstruction error of these three
methods on 5 diﬀerent datasets. We can see the optimization-based and neu-
ral network-based methods perform comparably, where their combination yields
better results. See Figure 3 for a qualitative comparison. We include PSNR (in
dB) results in the supplementary material.
Class-speciﬁc model: So far, we have trained the generative model on a
particular class of images. As a comparison, we train a cross-class model on
three datasets altogether (i.e. shoes, handbags, and shirts), and observe that the
model achieves worse reconstruction error compared to class-speciﬁc models (by
∼ 10%). We also have tried to use a class-speciﬁc model to reconstruct images
from a diﬀerent class. The mean cross-category reconstruction errors are much
worse: shoe model used for shoes: 0.140 vs. shoe model for handbags: 0.398, and
for shirts: 0.451. However, we expect a model trained on many categories (e.g.
1, 000) to generalize better to novel objects.
Perception study: We perform a small perception study to compare the pho-
torealism of four types of images: real photos, generated samples produced by
GAN, our method (shape only), and our method (shape+color). We collect
20 annotations for 400 images by asking Amazon Mechanical Turk workers
if the images look realistic or not. Real photos: 91.5%, DCGAN: 14.3%, ours
(shape+color): 25.9%; ours (shape only): 48.7%. DCGAN model alone produces
less photo-realistic images, but when combined with our edit transfer, the realism
signiﬁcantly improves.

8 Discussion and Limitations

We presented a step towards image editing with a direct constraint to stay
close to the manifold of real images. We approximate this manifold using the
state-of-the-art in deep generative models (DCGAN). We show how to make
interactive edits to the generated images and transfer the resulting changes in
shape and color back to the original image. Thus, the quality of the generated
results (low resolution, missing texture and details) and the types of data that
DCGAN applies to (works well on structured datasets such as product images
and worse on more general imagery), limits how far we can get with this editing
approach. However, our method is not tied to a particular generative model
and will improve with the advancement of this ﬁeld. Our current editing brush
tools allow rough changes in color and shape but not texture and more complex
structure changes. We leave these for future work.

Acknowledgments This work was supported, in part, by funding from Adobe,
eBay, and Intel, as well as a hardware grant from NVIDIA. J.-Y. Zhu is supported
by Facebook Graduate Fellowship.

Generative Visual Manipulation on the Natural Image Manifold

15

References

1. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,

Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS. (2014) 2, 3, 4

2. Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with

deep convolutional generative adversarial networks. ICLR (2016) 2, 4, 11

3. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. ICLR (2014) 2, 3,

11

4. Denton, E., Chintala, S., Szlam, A., Fergus, R.: Deep generative image models

using a laplacian pyramid of adversarial networks. In: NIPS. (2015) 2, 4

5. Dosovitskiy, A., Brox, T.: Generating images with perceptual similarity metrics

based on deep networks. In: NIPS. (2016) 2, 6

6. Reinhard, E., Ashikhmin, M., Gooch, B., Shirley, P.: Color transfer between im-

ages. IEEE Computer Graphics and Applications (2001) 3

7. Levin, A., Lischinski, D., Weiss, Y.: Colorization using optimization.

In: SIG-

GRAPH. (2004) 3

8. Alexa, M., Cohen-Or, D., Levin, D.: As-rigid-as-possible shape interpolation. In:

SIGGRAPH. (2000) 3

9. Kr¨ahenb¨uhl, P., Lang, M., Hornung, A., Gross, M.: A system for retargeting of

streaming video. In: SIGGRAPH. (2009) 3

10. Barnes, C., Shechtman, E., Finkelstein, A., Goldman, D.: Patchmatch: A ran-
domized correspondence algorithm for structural image editing. In: SIGGRAPH.
(2009) 3

11. Wolberg, G.: Digital Image Warping. IEEE Computer Society Press (1990) 3, 12
12. Shechtman, E., Rav-Acha, A., Irani, M., Seitz, S.: Regenerative morphing.
In:

CVPR. (2010) 3, 12

13. Kemelmacher-Shlizerman, I., Shechtman, E., Garg, R., Seitz, S.M.: Exploring

photobios. In: SIGGRAPH. (2011) 3, 5

14. Olshausen, B.A., Field, D.J.: Emergence of simple-cell receptive ﬁeld properties

by learning a sparse code for natural images. Nature 381 (1996) 607–609 3

15. Portilla, J., Simoncelli, E.P.: A Parametric Texture Model Based on Joint Statistics

of Complex Wavelet Coeﬃcients. IJCV 40(1) (2000) 49–70 3

16. Zoran, D., Weiss, Y.: From Learning Models of Natural Image Patches to Whole

Image Restoration. In: Proc. ICCV. (2011) 479–486 3

17. Roth, S., Black, M.J.: Fields of Experts: A Framework for Learning Image Priors.

In: CVPR. (2005) 3

18. Zhu, J.Y., Kr¨ahenb¨uhl, P., Shechtman, E., Efros, A.A.: Learning a discriminative

model for the perception of realism in composite images. In: ICCV. (2015) 3

19. Hinton, G.E., Salakhutdinov, R.R.: Reducing the dimensionality of data with

neural networks. Science 313(5786) (2006) 504–507 3

20. Salakhutdinov, R., Hinton, G.E.: Deep boltzmann machines. In: AISTATS. (2009)

3

21. Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.A.: Extracting and composing

robust features with denoising autoencoders. In: ICML. (2008) 3

22. Bengio, Y., Laufer, E., Alain, G., Yosinski, J.: Deep generative stochastic networks

trainable by backprop. In: ICML. (2014) 3

23. Gregor, K., Danihelka, I., Graves, A., Wierstra, D.: Draw: A recurrent neural

network for image generation. ICML (2015) 3

24. Dosovitskiy, A., Tobias Springenberg, J., Brox, T.: Learning to generate chairs

with convolutional neural networks. In: CVPR. (2015) 3

16

Jun-Yan Zhu, Philipp Kr¨ahenb¨uhl, Eli Shechtman, Alexei A. Efros

25. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer

and super-resolution. In: ECCV. (2016) 6

26. Krizhevsky, A., Sutskever, I., Hinton, G.E.:

Imagenet classiﬁcation with deep

convolutional neural networks. In: NIPS. (2012) 1097–1105 6

27. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale

hierarchical image database. In: CVPR. (2009) 6

28. Byrd, R.H., Lu, P., Nocedal, J., Zhu, C.: A limited memory algorithm for bound
constrained optimization. SIAM Journal on Scientiﬁc Computing 16(5) (1995)
1190–1208 6

29. Gershman, S.J., Goodman, N.D.: Amortized inference in probabilistic reasoning.
In: Proceedings of the 36th Annual Conference of the Cognitive Science Society.
(2014) 6

30. Brox, T., Bruhn, A., Papenberg, N., Weickert, J.: High accuracy optical ﬂow

estimation based on a theory for warping. In: ECCV. (2004) 9

31. Bruhn, A., Weickert, J., Schn¨orr, C.: Lucas/kanade meets horn/schunck: Combin-

ing local and global optic ﬂow methods. IJCV 61(3) (2005) 211–231 9

32. Shih, Y., Paris, S., Durand, F., Freeman, W.T.: Data-driven hallucination of dif-
ferent times of day from a single outdoor photo. ACM Transactions on Graphics
(TOG) 32(6) (2013) 200 9

33. He, K., Sun, J., Tang, X.: Guided image ﬁltering. In: ECCV. (2010) 10
34. Parikh, D., Grauman, K.: Relative attributes. In: ICCV. (2011) 10
35. Dalal, N., Triggs, B.: Histograms of oriented gradients for human detection. In:

CVPR. (2005) 10

36. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML. (2015) 11

37. Yu, A., Grauman, K.: Fine-grained visual comparisons with local learning.

In:

CVPR. (2014) 11

38. Yu, F., Zhang, Y., Song, S., Seﬀ, A., Xiao, J.: Construction of a large-scale
arXiv preprint

image dataset using deep learning with humans in the loop.
arXiv:1506.03365 (2015) 11

39. Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., Oliva, A.: Learning deep features

for scene recognition using places database. In: NIPS. (2014) 487–495 11

40. Seitz, S.M., Dyer, C.R.: View morphing. In: SIGGRAPH, New York (1996) 21–30

12

41. Sun, X., Wang, C., Xu, C., Zhang, L.: Indexing billions of images for sketch-based

retrieval. In: ACM MM. (2013) 12

42. Zhu, J.Y., Lee, Y.J., Efros, A.A.: Averageexplorer: Interactive exploration and

alignment of visual data collections. SIGGRAPH 33(4) (2014) 12

43. Risser, E., Han, C., Dahyot, R., Grinspun, E.: Synthesizing structured image

hybrids. SIGGRAPH 29(4) (2010) 85:1–85:6 12

