## Task5(AIWriter):

- Task: Build an AI Writer that can pass turing-test

### collect data （papers）

collect as many as possible papers about computer science. given that many research papers are just normal pdf, extract every section such as abstract and introduction out is necessary. 

collect data:

- Google scholar
- paper collection repo
- paper reference

```
- **Learning and transferring mid-Level image representations using convolutional neural networks** (2014), M. Oquab et al. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf)
- **Decaf: A deep convolutional activation feature for generic visual recognition** (2014), J. Donahue et al. [[pdf]](http://arxiv.org/pdf/1310.1531)
```

```
#fetch_papers.py
'\*\*(.*?)\*\*.*?\[\[pdf\]\]\((.*?)\)'
regular expression to search paper name and URL
requests library to get url content
```



pre-process:

- pdfminer: a python third-party library to handle pdf files
- special sentences and words: handle the hyphens(for example a word is broken at the end of a line,the dash sign should be processed.)
- nltk punkt: do tokenizing work.

```
#processPDF.py
os.walk papers directory,parse pdfs,get abstract and introduction,split paragraph to sentences. 
```



### sentenct to vector

mapping a sentence with arbitrary length to vector space

implementation of the Paragraph Vector in Quoc Le and Tomas Mikolov's paper:《Distributed Representations of Sentences and Documents》,which applies Word2vec to Paragraph Vector(sentence vector, doc vector), based on gensim.

![fig1](G:\senior\first semester\nlp\nlp20projects\2017202032\fig1.jpg)

![fig2](G:\senior\first semester\nlp\nlp20projects\2017202032\fig2.jpg)

 the only change is the additional paragraph token that is mapped to a vector via matrix D.  In this model, the concatenation or average of this vector  with a context  of  three words  is used to predict  the fourth word.  The paragraph vector represents the missing information from the current context and can act as a memory of the topic of the paragraph.

```
utils.py: contains various general utility functions.
matutils.py: contains math helper functions.
word2vec.py

input_file = 'abstract.txt'
model = Word2Vec(LineSentence(input_file), size=100, window=5, sg=0, min_count=5, workers=8)
model.save(input_file + '.model')

sent_file = 'sentence.txt'
model = Sent2Vec(LineSentence(sent_file), model_file=input_file + '.model')
model.save_sent2vec_format(sent_file + '.vec')

e.g.
#sentence.txt
A distributed storage system to efficiently manage structured data with little cost.
The network handles input sentences of varying length and induces a feature graph over the sentence.

#sentence.txt.vec
2 100
sent_0 -0.009554 -0.003785 -0.004823 0.011703 -0.000153 -0.016041 -0.002839 -0.003843 -0.001643 0.000473 -0.004278 0.000679 0.005522 0.009386 -0.006970 0.001791 -0.008354 -0.012418 0.008810 -0.005545 0.010257 0.003929 0.001259 0.006590 -0.013060 -0.009657 0.002526 -0.019487 -0.002501 0.022083 0.001487 -0.017755 0.004400 -0.011840 0.007120 -0.001989 0.002166 -0.000547 -0.012981 0.006109 -0.003605 -0.005348 -0.007583 -0.002290 0.009460 0.006202 0.008483 0.000667 0.000200 -0.011974 -0.017397 0.016691 -0.007178 -0.007728 -0.011110 -0.001281 -0.004483 0.008614 -0.010878 0.008870 0.009068 -0.002888 -0.000445 -0.000822 -0.009500 -0.008783 0.010807 0.000885 0.002982 0.012150 -0.003579 -0.004494 -0.004649 0.002643 0.001792 -0.006157 -0.004674 -0.017109 0.006467 0.013846 0.011680 0.009841 0.004645 -0.006866 0.009034 0.004638 -0.003010 -0.007648 0.006532 0.009357 -0.002438 0.004003 0.001810 0.004572 0.003141 0.001949 -0.002038 -0.004512 -0.000004 0.005462
sent_1 0.001712 0.013351 0.017967 0.006466 0.007485 0.011689 0.005327 0.008678 -0.027895 0.012858 -0.000825 -0.010725 0.004618 0.013022 -0.023331 0.009364 0.017440 0.035987 0.022993 -0.007518 -0.024009 -0.008675 -0.020077 -0.005244 -0.012269 0.014769 -0.001264 -0.026308 -0.009585 0.004598 -0.007442 0.009185 0.007322 -0.028389 0.034586 -0.010839 -0.003063 -0.008862 -0.007180 0.029103 -0.019670 0.003352 -0.002243 0.009126 -0.026995 0.002298 -0.001262 -0.012612 -0.000749 -0.005303 -0.040008 0.016077 -0.028692 -0.018357 -0.009279 0.008369 -0.000165 -0.012123 0.009525 -0.011600 -0.003792 0.026223 0.019918 -0.015456 0.008158 -0.027038 -0.011645 -0.020822 0.017681 -0.024978 -0.020037 0.017640 -0.015770 -0.009248 0.013114 -0.023810 -0.014884 -0.012257 -0.021918 0.004253 0.008031 -0.012381 0.012354 -0.006265 0.008896 0.004505 0.008882 0.022418 0.008001 0.027500 0.013872 0.001764 0.023158 -0.017609 0.032249 0.011017 -0.005587 -0.002738 0.001257 0.014512
```



### construct sentence warehouse

```
nltk.tokenize.sent_tokenize: split paragraph to sentences
based on the above model,construct sentence vector
```



### experiment

```
#sen_dis.py
```

abstract_test.txt
We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable.



test_result.txt

We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. RPNs are trained end-to-end to generate high quality region proposals, which are used by Fast R-CNN for detection. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. For each update, one set of weights is used to determine the greedy policy and the other to determine its value. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classiﬁcation and Twitter sentiment prediction by distant supervision.



### analysis

- dataset: the amount of paper is small (several hundreds only)
- model: a little simple, just sentence to vector, choose the most several related sentences, replace synonymous word and add some conjunctive words. sometimes it's not smooth.