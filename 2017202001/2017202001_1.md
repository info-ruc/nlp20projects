# 第一次实验报告
### 所选任务：Task7(HumanEye)
- 任务内容：从视觉信息中生成文本信息
- 所选数据集：COCO，训练集大小：13G，验证集大小：6G，总共19G
### 第一阶段已完成部分
- 论文调研：将该任务视为image caption任务，调研了该领域的相关论文，决定从最基础的show and tell模型开始实现，在第二第三阶段不断对模型进行优化改进。
- 数据预处理：该数据集的caption是json格式，预处理部分将有效信息抽取出来，剔除冗余信息，生成一个字典，并建立一个单词与数字索引的双向字典(实际实现方式为两个字典)，并将出现次数小于5的次数忽略，加入关键字< ukn >，用于编码和解码阶段，最后再将图片名与字幕组合起来生成一个tuple，方便data loader的构建。
- 构建dataset类和data loader：继承torch.utils.data.Dataset类，重新写 __ init __ ，__ getitem __ ， __ len __ 三个函数。__ init __ 函数用于初始化数据集的各种参数，__ getitem __ 函数用于取数据集里的一个元素，__ len __ 函数则是返回数据集的大小。由于数据集过大，无法一次性将图片读入内存，于是在__ getitem __ 函数里面单个读入，最后返回图片，同样的，在__ getitem __ 函数里还进行了对字幕的处理，将原来的单词转化为数字索引，并加入开始标识符和结束标识符。最后，还重写了collate_fn函数。由于字幕长度不一致，所以无法转化为tensor，于是对此进行了补0操作，并转化为tensor类型返回。
