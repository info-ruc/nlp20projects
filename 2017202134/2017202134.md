1.	Task and Motivation

I choose Task7: Human Eye as my project topic, in which I will conduct and implement the algorithm to generate text knowledge from visual contents (images or video). The generator should pass Turing-test.
Human Eye is similar with another task: Image Caption, which is a classic challenge in the area of Natural Language Processing and Computer Vision. The generative model should solve the computer vision challenges of determining which objects are in an image, and it also be capable of capturing and expressing their relationships in a natural language.
By conduct this project, I have a deeper understanding of algorithm related to NLP and CV, especially the principle and implementation of the deep learning-based models.




2.	Related Work and Tools

a.	Reference

  I will read and implement the paper: Show, Attend, and Tell: Neural Image Caption Generation with Visual Attention, which published in 2016. This is by no means the current state-of-the-art, but is still pretty darn amazing. The opensource of author Kelvin Xu’s original implementation can be found on Github: https://github.com/kelvinxu/arctic-captions. 


b.	Dataset
  · Flickr8
  · Include 8,000 images, each with five different caption descriptions that provide clear descriptions of highlighted entities and events. Do not include celebrities or named locations, but manually select multiple scenes and situations.

  · Andrej Karpathy's training, validation, and test splits
  · Split the whole dataset into training(6000), validation(1000) and test(1000) set. Captions save in json format. [image_id, sentence_id, tokens, raw_sentence]


c.	Tools
  I will use VSCode + Pytorch to implement the deep learning-based algorithm.



3.	 Models

  I read the paper and sorted out how it worked. The whole model mainly include four parts: Encoder, Decoder, Attention.
 
 
 a. Encoder
  The Encoder encodes the input image with 3 color channels into a smaller image with more learned channels. This smaller encoded image is a summary representation of all that's useful in the original image. The author uses Convolutional Neural Networks (CNNs) as the encoder. There are some pretrained CNNs represent images available in Pytorch such as Resnet, which can capture the essence of an image very well and have good performance and image classification task. Since the last layer or two of these models are linear layers coupled with softmax activation for classification, they should be stripped away in this task.
  
  
b. Decoder & Attention
  The Decoder is to look at the encoded image and generate a caption word by word. Since it's generating a sequence, it would need to be a Recurrent Neural Network (RNN) and the author use an LSTM. In a setting with Attention, instead of using the simple average of all pixels, the author uses the weighted average across all pixels, with the weights of the important pixels being greater. This weighted representation of the image can be concatenated with the previously generated word at each step to generate the next word.
Attention considers the sequence generated thus far, and attends to the part of the image that needs describing next. The Attention Network finish the process of computing the probability that a pixel is the place to look to generate the next word. 

  The combined network works as this. Once the Encoder generates the encoded image, the encoding is transformed to create the initial hidden state h (and cell state C) for the LSTM Decoder. At each decode step, the encoded image and the previous hidden state are used to generate weights for each pixel in the Attention network. the previously generated word and the weighted average of the encoding are fed to the LSTM Decoder to generate the next word.
  
  
c. Beam Search
  I use a linear layer to transform the Decoder's output into a score for each word in the vocabulary. The greedy option would be to choose the word with the highest score and use it to predict the next word. But this is not optimal because the rest of the sequence hinges on that first word you choose. Beam Search work in this step. At each decode step, choose the top k words combined with each word generated in the last decode step. After the sequences terminate, choose the sequence with the best overall score.



4. Implementation

a. Input
· Three inputs: Images, Captions, Caption lengths
  (1) Images
  - Apply pretrained Resnet-101 from torchvision in CNN part
  - Images should be resized to 3*256*256, and pixel values must be in the range [0,1] for uniformity.
Images fed to the model must be a Float tensor of dimension N, 3, 256, 256, and must be normalized by the mean (= [0.485, 0.456, 0.406]) and std (= [0.229, 0.224, 0.225]). N is the batch size.
  (2) Captions
  - A zeroth word, <start>, to generate the first word. A last word <end> to know when to stop
  - <pad> tokens, pad the captions in various length and pass the captions around as fixed size Tensors
  - e.g. <start> a man holds a football <end> <pad> <pad> <pad><end>
  - A word_map which is an index mapping for each word in the corpus, including the <start>,<end>, and <pad> tokens. To look up embeddings for them or to identify their place in the predicted word scores.
  - Therefore, captions fed to the model must be an Int tensor of dimension N, L where L is the padded length.
  
  
b. Encoder
· Use a pretrained ResNet-101 already available in PyTorch's torchvision module. Discard the last two layers (pooling and linear layers).

· Add a fine_tune() method to convolutional blocks 2 through 4 in the ResNet to fine-tune the Encoder CNN. Because the first convolutional block would have usually learned something very fundamental to image processing, such as detecting lines, edges, curves, etc. 


c. Attention
· Composed of linear layers and a couple of activations.

· Separate linear layers transform both the encoded image (flattened to N, 14 * 14, 2048) and the hidden state (output) from the Decoder to the same dimension. They are then added and ReLU activated. A third linear layer transforms this result to a dimension of 1, and then apply the softmax to generate the weights.


d. Decoder
·The iteration is performed manually in a for loop (length of input caption_lengths) with a PyTorch LSTMCell instead of iterating automatically without a loop with a PyTorch LSTM. This is because I need to execute the Attention mechanism between each decode step. An LSTMCell is a single timestep operation, whereas an LSTM would iterate over multiple timesteps continously and provide all outputs at once.



5. Result

The BLEU-4 score on the test set is 0.2829 out of 1.
(BLEU-4: similarity between reference and hypothesis text)

The process of caption generation is visualized and can be found in src/result. 



