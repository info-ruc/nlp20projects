1.	Task and Motivation
  I choose Task7: Human Eye as my project topic, in which I will conduct and implement the algorithm to generate text knowledge from visual contents (images or video). The generator should pass Turing-test.
  Human Eye is similar with another task: Image Caption, which is a classic challenge in the area of Natural Language Processing and Computer Vision. The generative model should solve the computer vision challenges of determining which objects are in an image, and it also be capable of capturing and expressing their relationships in a natural language.
By conduct this project, I have a deeper understanding of algorithm related to NLP and CV, especially the principle and implementation of the deep learning-based models.


2.	Related Work and Tools

a.	Reference
I will read and implement the paper: Show, Attend, and Tell: Neural Image Caption Generation with Visual Attention, which published in 2016. This is by no means the current state-of-the-art, but is still pretty darn amazing. The opensource of author Kelvin Xu’s original implementation can be found on Github: https://github.com/kelvinxu/arctic-captions. 

b.	Dataset
The dataset I used are MSCOCO’14 and AI Challenger’17. Both datasets contain millions of images and caption text. MSCOCO’ 14 is the dataset used in the Show Attend and Tell paper and I will use it as baseline. AIChallenger’17 is a Chinese Image Caption dataset. I will extend the model to this dataset and implement a Chinese Language generator. Because the memory of my own computer is limited, I may use part of the dataset to train the model.

c.	Tools
I will use Anaconda + Pytorch to implement the deep learning-based algorithm.


3.	Solutions and Completed Work 
a.	Dataset Preparation
I download the complete dataset of MSCOCO’14 and AI Challenger’17, which can meet the requirement in task tips: Collecting as much as possible visual contents (images or videos)

b.	Preprocessing
I read in the images of two dataset and save the 3*height*widths array of images in json format. I also segment Chinse caption sentence into words by JIEBA and save the result.

c.	Paper reading
I read the paper and sorted out how it worked. The whole model mainly include four parts: Encoder, Decoder, Attention and Beam Search.
The Encoder encodes the input image with 3 color channels into a smaller image with more learned channels. This smaller encoded image is a summary representation of all that's useful in the original image. The author uses Convolutional Neural Networks (CNNs) as the encoder. There are some pretrained CNNs represent images available in Pytorch such as Resnet, which can capture the essence of an image very well and have good performance and image classification task. Since the last layer or two of these models are linear layers coupled with softmax activation for classification, they should be stripped away in this task.
The Decoder is to look at the encoded image and generate a caption word by word. Since it's generating a sequence, it would need to be a Recurrent Neural Network (RNN) and the author use an LSTM. In a setting with Attention, instead of using the simple average of all pixels, the author uses the weighted average across all pixels, with the weights of the important pixels being greater. This weighted representation of the image can be concatenated with the previously generated word at each step to generate the next word.
Attention considers the sequence generated thus far, and attends to the part of the image that needs describing next. The Attention Network finish the process of computing the probability that a pixel is the place to look to generate the next word. 
The combined network works as this. Once the Encoder generates the encoded image, the encoding is transformed to create the initial hidden state h (and cell state C) for the LSTM Decoder. At each decode step, the encoded image and the previous hidden state are used to generate weights for each pixel in the Attention network. the previously generated word and the weighted average of the encoding are fed to the LSTM Decoder to generate the next word.
Then use a linear layer to transform the Decoder's output into a score for each word in the vocabulary. The greedy option would be to choose the word with the highest score and use it to predict the next word. But this is not optimal because the rest of the sequence hinges on that first word you choose. Beam Search work in this step. At each decode step, choose the top k words combined with each word generated in the last decode step. After the sequences terminate, choose the sequence with the best overall score.

d.	Start to implement the CNN part
