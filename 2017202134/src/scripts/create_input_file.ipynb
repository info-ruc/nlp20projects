{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import torch\n",
    "from skimage import io, transform\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from random import seed, choice, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_files(dataset, karpathy_json_path, image_folder, captions_per_image, min_word_freq, output_folder,\n",
    "                       max_len=100):\n",
    "    \"\"\"\n",
    "    Creates input files for training, validation, and test data.\n",
    "    :param dataset: name of dataset, one of 'coco', 'flickr8k', 'flickr30k'\n",
    "    :param karpathy_json_path: path of Karpathy JSON file with splits and captions\n",
    "    :param image_folder: folder with downloaded images\n",
    "    :param captions_per_image: number of captions to sample per image\n",
    "    :param min_word_freq: words occuring less frequently than this threshold are binned as <unk>s\n",
    "    :param output_folder: folder to save files\n",
    "    :param max_len: don't sample captions longer than this length\n",
    "    \"\"\"\n",
    "\n",
    "    assert dataset in {'coco', 'flickr8k', 'flickr30k'}\n",
    "\n",
    "    # Read Karpathy JSON\n",
    "    with open(karpathy_json_path, 'r') as j:\n",
    "        data = json.load(j)\n",
    "\n",
    "    # Read image paths and captions for each image\n",
    "    train_image_paths = []\n",
    "    train_image_captions = []\n",
    "    val_image_paths = []\n",
    "    val_image_captions = []\n",
    "    test_image_paths = []\n",
    "    test_image_captions = []\n",
    "    word_freq = Counter()\n",
    "\n",
    "    for img in data['images']:\n",
    "        captions = []\n",
    "        for c in img['sentences']:\n",
    "            # Update word frequency\n",
    "            word_freq.update(c['tokens'])\n",
    "            if len(c['tokens']) <= max_len:\n",
    "                captions.append(c['tokens'])\n",
    "\n",
    "        if len(captions) == 0:\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(image_folder, img['filepath'], img['filename']) if dataset == 'coco' else os.path.join(\n",
    "            image_folder, img['filename'])\n",
    "\n",
    "        if img['split'] in {'train', 'restval'}:\n",
    "            train_image_paths.append(path)\n",
    "            train_image_captions.append(captions)\n",
    "        elif img['split'] in {'val'}:\n",
    "            val_image_paths.append(path)\n",
    "            val_image_captions.append(captions)\n",
    "        elif img['split'] in {'test'}:\n",
    "            test_image_paths.append(path)\n",
    "            test_image_captions.append(captions)\n",
    "\n",
    "    # Sanity check\n",
    "    assert len(train_image_paths) == len(train_image_captions)\n",
    "    assert len(val_image_paths) == len(val_image_captions)\n",
    "    assert len(test_image_paths) == len(test_image_captions)\n",
    "\n",
    "    # Create word map\n",
    "    words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
    "    word_map = {k: v + 1 for v, k in enumerate(words)}\n",
    "    word_map['<unk>'] = len(word_map) + 1\n",
    "    word_map['<start>'] = len(word_map) + 1\n",
    "    word_map['<end>'] = len(word_map) + 1\n",
    "    word_map['<pad>'] = 0\n",
    "\n",
    "    # Create a base/root name for all output files\n",
    "    base_filename = dataset + '_' + str(captions_per_image) + '_cap_per_img_' + str(min_word_freq) + '_min_word_freq'\n",
    "\n",
    "    # Save word map to a JSON\n",
    "    with open(os.path.join(output_folder, 'WORDMAP_' + base_filename + '.json'), 'w') as j:\n",
    "        json.dump(word_map, j)\n",
    "\n",
    "    # Sample captions for each image, save images to HDF5 file, and captions and their lengths to JSON files\n",
    "    seed(123)\n",
    "    for impaths, imcaps, split in [(train_image_paths, train_image_captions, 'TRAIN'),\n",
    "                                   (val_image_paths, val_image_captions, 'VAL'),\n",
    "                                   (test_image_paths, test_image_captions, 'TEST')]:\n",
    "\n",
    "        with h5py.File(os.path.join(output_folder, split + '_IMAGES_' + base_filename + '.hdf5'), 'a') as h:\n",
    "            # Make a note of the number of captions we are sampling per image\n",
    "            h.attrs['captions_per_image'] = captions_per_image\n",
    "\n",
    "            # Create dataset inside HDF5 file to store images\n",
    "            images = h.create_dataset('images', (len(impaths), 3, 256, 256), dtype='uint8')\n",
    "\n",
    "            print(\"\\nReading %s images and captions, storing to file...\\n\" % split)\n",
    "\n",
    "            enc_captions = []\n",
    "            caplens = []\n",
    "\n",
    "            for i, path in enumerate(tqdm(impaths)):\n",
    "\n",
    "                # Sample captions\n",
    "                if len(imcaps[i]) < captions_per_image:\n",
    "                    captions = imcaps[i] + [choice(imcaps[i]) for _ in range(captions_per_image - len(imcaps[i]))]\n",
    "                else:\n",
    "                    captions = sample(imcaps[i], k=captions_per_image)\n",
    "\n",
    "                # Sanity check\n",
    "                assert len(captions) == captions_per_image\n",
    "\n",
    "                # Read images\n",
    "                img = io.imread(impaths[i])\n",
    "                if len(img.shape) == 2:\n",
    "                    img = img[:, :, np.newaxis]\n",
    "                    img = np.concatenate([img, img, img], axis=2)\n",
    "                img = transform.resize(img, (256, 256),preserve_range=True)\n",
    "                img = img.transpose(2, 0, 1)\n",
    "                assert img.shape == (3, 256, 256)\n",
    "                assert np.max(img) <= 255\n",
    "\n",
    "                # Save image to HDF5 file\n",
    "                images[i] = img\n",
    "\n",
    "                for j, c in enumerate(captions):\n",
    "                    # Encode captions\n",
    "                    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in c] + [\n",
    "                        word_map['<end>']] + [word_map['<pad>']] * (max_len - len(c))\n",
    "\n",
    "                    # Find caption lengths\n",
    "                    c_len = len(c) + 2\n",
    "\n",
    "                    enc_captions.append(enc_c)\n",
    "                    caplens.append(c_len)\n",
    "\n",
    "            # Sanity check\n",
    "            assert images.shape[0] * captions_per_image == len(enc_captions) == len(caplens)\n",
    "\n",
    "            # Save encoded captions and their lengths to JSON files\n",
    "            with open(os.path.join(output_folder, split + '_CAPTIONS_' + base_filename + '.json'), 'w') as j:\n",
    "                json.dump(enc_captions, j)\n",
    "\n",
    "            with open(os.path.join(output_folder, split + '_CAPLENS_' + base_filename + '.json'), 'w') as j:\n",
    "                json.dump(caplens, j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading TRAIN images and captions, storing to file...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6000/6000 [14:58<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading VAL images and captions, storing to file...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [02:32<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading TEST images and captions, storing to file...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [02:34<00:00,  6.48it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Create input files (along with word map)\n",
    "    create_input_files(dataset='flickr8k',\n",
    "                       karpathy_json_path='D://大四//nlp//caption_datasets//dataset_flickr8k.json',\n",
    "                       image_folder='D://大四//nlp//Flicker8k_Dataset//',\n",
    "                       captions_per_image=5,\n",
    "                       min_word_freq=5,\n",
    "                       output_folder='D://大四//nlp//create_input',\n",
    "                       max_len=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
